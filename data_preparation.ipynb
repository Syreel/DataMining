{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1590ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1704c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = 'dataset/artists.csv'\n",
    "tracks = 'dataset/tracks.csv'\n",
    "\n",
    "index_col = 0\n",
    "df_artists = pd.read_csv(artists, sep=';', index_col=index_col)\n",
    "df_tracks = pd.read_csv(tracks, index_col=index_col)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca4ffe",
   "metadata": {},
   "source": [
    "## üíæ Optimizing Data Types for Efficiency\n",
    "\n",
    "Before we proceed with cleaning and analysis, it's essential to ensure our DataFrames use the most **memory-efficient and appropriate data types**. Converting low-cardinality string columns (like `gender` and `nationality`) to the **`category`** dtype significantly reduces memory usage.\n",
    "\n",
    "We'll also ensure all date columns are correctly parsed as **`datetime`** objects, and descriptive text fields are designated as the modern **`string`** dtype. For integer columns that contain `NaN` values, we use the nullable integer type **`Int64`**.\n",
    "\n",
    "This step makes subsequent operations faster and more memory-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ab7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists['gender'] = df_artists['gender'].astype('category')\n",
    "df_artists['nationality'] = df_artists['nationality'].astype('category')\n",
    "df_artists['country'] = df_artists['country'].astype('category')\n",
    "df_artists['region'] = df_artists['region'].astype('category')\n",
    "df_artists['province'] = df_artists['province'].astype('category')\n",
    "df_artists['birth_place'] = df_artists['birth_place'].astype('category')\n",
    "df_artists['birth_date'] = pd.to_datetime(df_artists['birth_date'], errors='coerce')\n",
    "df_artists['active_start'] = pd.to_datetime(df_artists['active_start'], errors='coerce')\n",
    "df_artists['description'] = df_artists['description'].astype('string')\n",
    "df_artists['name'] = df_artists['name'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae007ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks['id_artist'] = df_tracks['id_artist'].astype('category')\n",
    "df_tracks['id_album'] = df_tracks['id_album'].astype('category')\n",
    "df_tracks['language'] = df_tracks['language'].astype('category')\n",
    "df_tracks['album_type'] = df_tracks['album_type'].astype('category')\n",
    "df_tracks['stats_pageviews'] = df_tracks['stats_pageviews'].astype('Int64')\n",
    "df_tracks['year'] = pd.to_numeric(df_tracks['year'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['month'] = pd.to_numeric(df_tracks['month'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['day'] = pd.to_numeric(df_tracks['day'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['popularity'] = pd.to_numeric(df_tracks['popularity'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['disc_number'] = df_tracks['disc_number'].astype('Int64')\n",
    "df_tracks['track_number'] = df_tracks['track_number'].astype('Int64')\n",
    "df_tracks['explicit'] = df_tracks['explicit'].astype('bool')\n",
    "df_tracks['modified_popularity'] = df_tracks['modified_popularity'].astype('bool')\n",
    "df_tracks['album_release_date'] = pd.to_datetime(df_tracks['album_release_date'], errors='coerce')\n",
    "df_tracks['name_artist'] = df_tracks['name_artist'].astype('string')\n",
    "df_tracks['full_title'] = df_tracks['full_title'].astype('string')\n",
    "df_tracks['title'] = df_tracks['title'].astype('string')\n",
    "df_tracks['featured_artists'] = df_tracks['featured_artists'].astype('string')\n",
    "df_tracks['primary_artist'] = df_tracks['primary_artist'].astype('string')\n",
    "df_tracks['album_name'] = df_tracks['album_name'].astype('string')\n",
    "df_tracks['album'] = df_tracks['album'].astype('string')\n",
    "df_tracks['album_image'] = df_tracks['album_image'].astype('string')\n",
    "df_tracks['lyrics'] = df_tracks['lyrics'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482f10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # Import the Abstract Syntax Tree module for safe evaluation\n",
    "\n",
    "# Assuming your DataFrame is df_tracks and it's already loaded\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"\n",
    "    Safely converts a string representation of a list into a Python list.\n",
    "    Handles NaN/missing values by returning an empty list or pd.NA.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value in (None, 'NaN', ''):\n",
    "        # Return an empty list for missing values if you plan to iterate over it\n",
    "        return []\n",
    "    try:\n",
    "        # Use ast.literal_eval for safe conversion of string-to-list\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Handle cases where the string is malformed or not a list structure\n",
    "        print(f\"Warning: Could not convert value: {value}\")\n",
    "        return [] # Default to empty list on failure\n",
    "\n",
    "# Apply the conversion to both columns\n",
    "df_tracks['swear_IT_words'] = df_tracks['swear_IT_words'].apply(safe_literal_eval)\n",
    "df_tracks['swear_EN_words'] = df_tracks['swear_EN_words'].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab69d5",
   "metadata": {},
   "source": [
    "## üïµÔ∏è Data Validation: Checking and Correcting Primary Key Duplicates\n",
    "\n",
    "For the data preparation phase, we start by performing a crucial check of the primary IDs for rows in both our DataFrames to check for potential duplicates. Ensuring unique identifiers is **foundational** for reliable joins and accurate analysis later on. \n",
    "\n",
    "A formal review of the primary ID columns yielded the following observations:\n",
    "\n",
    "* **`df_tracks`**: Inspection of the track ID column revealed **73 instances of duplicated identifiers**. To guarantee that each record is uniquely identifiable and to maintain the principle of one-to-one entity mapping, these duplicated rows will be managed immediately. IDs are of the format $\\text{TR\\#\\#\\#\\#\\#\\#}$, so we generate new IDs compliant with this format to replace duplicated ones.\n",
    "* **`df_artists`**: The artist ID column was found to be **entirely sound**, presenting no instances of duplicate IDs. Consequently, no corrective action is required for this DataFrame regarding its primary keys.\n",
    "\n",
    "The code below first validates the counts, displays a sample of the duplicates, and then executes the custom logic to **generate unique, non-colliding IDs** to replace the duplicated indices in `df_tracks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776a97e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate index for tracks: 73\n",
      "number of duplicate index for artists: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Creare un set di tutti gli ID esistenti per un controllo rapido\n",
    "existing_tracks_ids = set(df_tracks.index)\n",
    "existing_artists_ids = set(df_artists.index)\n",
    "\n",
    "# 2. Identificare le posizioni (indice booleano) degli indici duplicati.\n",
    "#    Usiamo keep='first' per segnare solo la seconda, terza, ecc. occorrenza.\n",
    "duplicate_mask_tracks = df_tracks.index.duplicated()\n",
    "duplicate_mask_artists = df_artists.index.duplicated()\n",
    "num_duplicates_tracks = duplicate_mask_tracks.sum()\n",
    "num_duplicates_artists = duplicate_mask_artists.sum()\n",
    "print(\"number of duplicate index for tracks:\", num_duplicates_tracks)\n",
    "print(\"number of duplicate index for artists:\", num_duplicates_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ee225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostro tutte le righe che hanno un indice duplicato, ordinate per ID:\n",
      "            id_artist      name_artist  \\\n",
      "id                                       \n",
      "TR108862  ART56320683    Bassi Maestro   \n",
      "TR108862  ART07127070      Noyz Narcos   \n",
      "TR135764  ART73965015           Ghemon   \n",
      "TR135764  ART86549066       Emis Killa   \n",
      "TR190585  ART78209349             Coez   \n",
      "TR190585  ART66932389           Piotta   \n",
      "TR192351  ART81071062        Club Dogo   \n",
      "TR192351  ART88792008    Jake La Furia   \n",
      "TR205970  ART80977821  Jack The Smoker   \n",
      "TR205970  ART08456301          Rancore   \n",
      "\n",
      "                                                 full_title  \\\n",
      "id                                                            \n",
      "TR108862                         Sushi Bar by¬†Bassi¬†Maestro   \n",
      "TR108862                    SPINE by¬†Noyz¬†Narcos (Ft.¬†Coez)   \n",
      "TR135764                   Nessuno vale quanto te by¬†Ghemon   \n",
      "TR135764          Giovani eroi by¬†Emis¬†Killa (Ft.¬†Not¬†Good)   \n",
      "TR190585     Nei treni la notte by¬†Coez (Ft.¬†Frah¬†Quintale)   \n",
      "TR190585                Serpico by¬†Piotta (Ft.¬†Tiromancino)   \n",
      "TR192351        Torner√≤ Da Re - Redrum Version by¬†Club¬†Dogo   \n",
      "TR192351                Musica Commerciale by¬†Jake¬†La Furia   \n",
      "TR205970  24.7 by¬†Jack¬†The Smoker (Ft.¬†Bassi¬†Maestro & Gu√®)   \n",
      "TR205970               Sigla Catteland by¬†Rancore & DJ¬†Myke   \n",
      "\n",
      "                                   title    featured_artists   primary_artist  \\\n",
      "id                                                                              \n",
      "TR108862                       Sushi Bar                <NA>    Bassi Maestro   \n",
      "TR108862                           SPINE                Coez      Noyz Narcos   \n",
      "TR135764          Nessuno vale quanto te                <NA>           Ghemon   \n",
      "TR135764                    Giovani eroi            Not Good       Emis Killa   \n",
      "TR190585              Nei treni la notte       Frah Quintale             Coez   \n",
      "TR190585                         Serpico         Tiromancino           Piotta   \n",
      "TR192351  Torner√≤ Da Re - Redrum Version                <NA>        Club Dogo   \n",
      "TR192351              Musica Commerciale                <NA>    Jake La Furia   \n",
      "TR205970                            24.7  Bassi Maestro, Gu√®  Jack The Smoker   \n",
      "TR205970                 Sigla Catteland                <NA>          Rancore   \n",
      "\n",
      "         language                              album  stats_pageviews  \\\n",
      "id                                                                      \n",
      "TR108862       en                           Sushi EP             <NA>   \n",
      "TR108862       cs                     VIRUS (Deluxe)            19050   \n",
      "TR135764       it                           ORCHIdee             7182   \n",
      "TR135764       it                 Keta Music, Vol. 3            13155   \n",
      "TR190585       it                 From The Rooftop 2             <NA>   \n",
      "TR190585       pl                   ‚Äôna notte infame             <NA>   \n",
      "TR192351       it     Vile Denaro - 10th Anniversary             <NA>   \n",
      "TR192351       it  Musica Commerciale Deluxe Edition            10684   \n",
      "TR205970       it                              V.Ita             <NA>   \n",
      "TR205970       it                               <NA>             <NA>   \n",
      "\n",
      "          swear_IT  swear_EN                                 swear_IT_words  \\\n",
      "id                                                                            \n",
      "TR108862         8         1          [bastardo, cesso, culo, fesso, merda]   \n",
      "TR108862         1         0                                      [scopare]   \n",
      "TR135764         0         0                                             []   \n",
      "TR135764         5         1                [bastardo, cazzo, merda, troia]   \n",
      "TR190585         0         0                                             []   \n",
      "TR190585         0         0                                             []   \n",
      "TR192351         1         0                                         [culo]   \n",
      "TR192351         8         0  [cagare, cazzo, culo, figo, incazzare, merda]   \n",
      "TR205970         4         0                                  [figa, merda]   \n",
      "TR205970         0         0                                             []   \n",
      "\n",
      "         swear_EN_words  year  month   day  n_sentences  n_tokens  \\\n",
      "id                                                                  \n",
      "TR108862     [bastardo]  2007     11    13         67.0     610.0   \n",
      "TR108862             []  1932      1    14         36.0     306.0   \n",
      "TR135764             []  2068      5    27         72.0     466.0   \n",
      "TR135764     [bastardo]  2021      7    23         52.0     584.0   \n",
      "TR190585             []  2022     10    14         44.0     295.0   \n",
      "TR190585             []  2024      3     1         69.0     488.0   \n",
      "TR192351             []  2017      5    19         86.0     683.0   \n",
      "TR192351             []  2013     10    29         36.0     534.0   \n",
      "TR205970             []  1910     10    10         48.0     490.0   \n",
      "TR205970             []  2013   <NA>  <NA>         20.0     148.0   \n",
      "\n",
      "          tokens_per_sent  char_per_tok  lexical_density  \\\n",
      "id                                                         \n",
      "TR108862         9.104478      3.707980         0.490662   \n",
      "TR108862         8.500000      3.806691         0.542751   \n",
      "TR135764         6.472222      4.305687         0.473934   \n",
      "TR135764        11.230769      3.889558         0.487952   \n",
      "TR190585         6.704545      4.256140         0.498246   \n",
      "TR190585         7.072464      4.042254         0.530516   \n",
      "TR192351         7.941860      3.748103         0.449165   \n",
      "TR192351        14.833333      3.880503         0.496855   \n",
      "TR205970        10.208333      3.733766         0.465368   \n",
      "TR205970         7.400000      4.574627         0.537313   \n",
      "\n",
      "          avg_token_per_clause     bpm  centroid    rolloff    flux     rms  \\\n",
      "id                                                                            \n",
      "TR108862              5.922330   89.98    0.1302  1262.0061  1.4183  0.1970   \n",
      "TR108862              6.120000   84.97    0.1069  1307.9852  1.1196  0.2700   \n",
      "TR135764              6.383562   92.05    0.1355  1222.7497  1.1038  0.2144   \n",
      "TR135764              7.121951  165.58    0.1434  1286.8483  1.3029  0.3015   \n",
      "TR190585              6.704545  135.20    0.1077  1099.6117  0.9851  0.1700   \n",
      "TR190585              7.176471   91.96    0.1360  1091.7072  1.1547  0.2306   \n",
      "TR192351              5.598361   83.97    0.2028  1803.5711  1.3480  0.2932   \n",
      "TR192351              5.621053  128.19    0.1353  1436.6329  1.3272  0.2598   \n",
      "TR205970              6.282051   86.06    0.1573  1764.7418  1.2966  0.2984   \n",
      "TR205970              6.727273   98.03    0.1513  1809.8772  1.1953  0.2594   \n",
      "\n",
      "             zcr  flatness  spectral_complexity      pitch  loudness  \\\n",
      "id                                                                     \n",
      "TR108862  0.0527    0.9319              22.1130  2356.4160   20.0195   \n",
      "TR108862  0.0524    0.9056              29.6680  2132.3250   30.8700   \n",
      "TR135764  0.0568    0.9251              28.2788  1896.4159   22.2053   \n",
      "TR135764  0.0564    0.9067              35.7936  2132.3474   34.1819   \n",
      "TR190585  0.0447    0.8918              17.3859  1709.2517   16.3989   \n",
      "TR190585  0.0508    0.9327              23.5366  2503.3810   24.9774   \n",
      "TR192351  0.0721    0.8205              24.8264  2876.5173   33.8417   \n",
      "TR192351  0.0574    0.8673              30.2980  2236.3089   29.5758   \n",
      "TR205970  0.0697    0.9016              40.1808  2170.3874   33.8672   \n",
      "TR205970  0.0718    0.8230              42.1591  2191.6551   28.7624   \n",
      "\n",
      "                            album_name album_release_date album_type  \\\n",
      "id                                                                     \n",
      "TR108862                    Sushi - EP         2007-11-13     single   \n",
      "TR108862                         VIRUS         2022-01-14      album   \n",
      "TR135764                      ORCHIdee         2014-05-27      album   \n",
      "TR135764            Keta Music, Vol. 3         2021-07-23      album   \n",
      "TR190585            From The Rooftop 2         2022-10-14      album   \n",
      "TR190585              'na notte infame         2024-03-01      album   \n",
      "TR192351  Vile Denaro 10th Anniversary         2007-05-17      album   \n",
      "TR192351            Musica Commerciale         2013-01-01      album   \n",
      "TR205970                         V.Ita         2009-10-10      album   \n",
      "TR205970            Musica per bambini         2018-06-01      album   \n",
      "\n",
      "          disc_number  track_number  duration_ms  explicit  popularity  \\\n",
      "id                                                                       \n",
      "TR108862            1             3     215461.0      True           6   \n",
      "TR108862            1             6     155294.0      True          44   \n",
      "TR135764            1             8     238773.0     False          26   \n",
      "TR135764            1            10     152733.0      True          35   \n",
      "TR190585            1             3     181789.0     False          43   \n",
      "TR190585            1             2     192994.0     False          21   \n",
      "TR192351            2             4     244226.0     False           9   \n",
      "TR192351            1             1     163517.0     False          39   \n",
      "TR205970            1             6     240053.0      True          13   \n",
      "TR205970            1             8     279213.0     False          42   \n",
      "\n",
      "                                                album_image   id_album  \\\n",
      "id                                                                       \n",
      "TR108862  https://i.scdn.co/image/ab67616d0000b2734311be...  ALB697589   \n",
      "TR108862  https://i.scdn.co/image/ab67616d0000b273cad459...  ALB525038   \n",
      "TR135764  https://i.scdn.co/image/ab67616d0000b273f7338f...  ALB346809   \n",
      "TR135764  https://i.scdn.co/image/ab67616d0000b27361a8db...  ALB168242   \n",
      "TR190585  https://i.scdn.co/image/ab67616d0000b273f4c5be...  ALB760031   \n",
      "TR190585  https://i.scdn.co/image/ab67616d0000b2735e9652...  ALB996374   \n",
      "TR192351  https://i.scdn.co/image/ab67616d0000b27357cfe6...  ALB390480   \n",
      "TR192351  https://i.scdn.co/image/ab67616d0000b273d49501...  ALB145179   \n",
      "TR205970  https://i.scdn.co/image/ab67616d0000b273418414...  ALB704296   \n",
      "TR205970  https://i.scdn.co/image/ab67616d0000b2736545b2...  ALB599065   \n",
      "\n",
      "                                                     lyrics  \\\n",
      "id                                                            \n",
      "TR108862  Questo mondo resta freddo anche se vivi da sta...   \n",
      "TR108862  Sei al centro del mio cuore come 'na spina\n",
      "Sen...   \n",
      "TR135764  Dicono che da un posto piccolo non pu√≤ venire ...   \n",
      "TR135764  Oh, la city √® silenziosa, in zona solo un clac...   \n",
      "TR190585  Ho fatto un giro in questa citt√†\n",
      "Ed √® come far...   \n",
      "TR190585  Se c'avessi diciott'anni\n",
      "Co quer fuoco che c'h...   \n",
      "TR192351  Quando ritorner√≤ da te\n",
      "Io ci ritorner√≤ da re\n",
      "P...   \n",
      "TR192351  Permettete una parola che √® da un po' che ho n...   \n",
      "TR205970  Vivo questa roba, scrivo della merda che ti sv...   \n",
      "TR205970  Stai su Deejay, su Radio Deejay\n",
      "Questo √® un av...   \n",
      "\n",
      "          modified_popularity  \n",
      "id                             \n",
      "TR108862                False  \n",
      "TR108862                False  \n",
      "TR135764                False  \n",
      "TR135764                False  \n",
      "TR190585                False  \n",
      "TR190585                False  \n",
      "TR192351                False  \n",
      "TR192351                False  \n",
      "TR205970                False  \n",
      "TR205970                False  \n"
     ]
    }
   ],
   "source": [
    "# 1. Creare una maschera per identificare TUTTE le righe (inclusa la prima)\n",
    "#    che hanno un indice duplicato.\n",
    "all_duplicates_mask = df_tracks.index.duplicated(keep=False)\n",
    "\n",
    "# 2. Filtrare il DataFrame per ottenere solo queste righe\n",
    "df_duplicate_groups = df_tracks[all_duplicates_mask]\n",
    "\n",
    "# 3. Ordinare per indice. Questo √® fondamentale per vedere\n",
    "#    le righe con lo stesso indice una accanto all'altra.\n",
    "df_duplicate_groups_sorted = df_duplicate_groups.sort_index()\n",
    "\n",
    "# 4. Stampare i gruppi di duplicati\n",
    "if not df_duplicate_groups_sorted.empty:\n",
    "    print(\"Mostro tutte le righe che hanno un indice duplicato, ordinate per ID:\")\n",
    "    # Stampiamo le prime 30 (o modifica il numero se vuoi vederne di pi√π)\n",
    "    print(df_duplicate_groups_sorted.head(10))\n",
    "else:\n",
    "    # Questo scenario si verifica se num_duplicates (dal tuo codice) era 0\n",
    "    print(\"Nessuna riga con indice duplicato trovata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6970c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "Generating 73 random unique IDs...\n",
      "Finished generating unique IDs.\n",
      "\n",
      "Generated 73 new unique IDs.\n",
      "Example new ID: TR795908\n",
      "Check for duplicates after replacement: False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# --- 1. Identify Duplicated Rows and Count ---\n",
    "# Find the boolean mask for rows where the ID (index) is duplicated,\n",
    "# keeping only the duplicates *after* the first occurrence.\n",
    "duplicated_mask = df_tracks.index.duplicated(keep='first')\n",
    "num_duplicates_to_replace = duplicated_mask.sum() # Should be 73\n",
    "print(num_duplicates_to_replace)\n",
    "\n",
    "# --- 2. Define ID Generation Helper ---\n",
    "def format_track_id(number, prefix='TR', padding=6):\n",
    "    \"\"\"Formats a number into a TRXXXXXX string.\"\"\"\n",
    "    # Uses f-string formatting to zero-pad the number to 6 digits\n",
    "    return f\"{prefix}{number:0{padding}d}\"\n",
    "\n",
    "# --- 3. Generate New Unique IDs with Collision Check ---\n",
    "\n",
    "# Convert the existing index to a set for O(1) average time complexity lookups\n",
    "existing_ids = set(df_tracks.index)\n",
    "new_track_ids = []\n",
    "\n",
    "# Range for 6-digit numbers (000000 to 999999)\n",
    "MIN_ID = 0\n",
    "MAX_ID = 999999 \n",
    "\n",
    "print(f\"Generating {num_duplicates_to_replace} random unique IDs...\")\n",
    "\n",
    "while len(new_track_ids) < num_duplicates_to_replace:\n",
    "    # Generate a random 6-digit number\n",
    "    random_num = random.randint(MIN_ID, MAX_ID)\n",
    "    \n",
    "    # Format it to the \"TRXXXXXX\" string\n",
    "    new_id = format_track_id(random_num)\n",
    "    \n",
    "    # Check for collision against all existing IDs\n",
    "    if new_id not in existing_ids:\n",
    "        new_track_ids.append(new_id)\n",
    "        # Immediately add the new ID to the existing_ids set to prevent\n",
    "        # generating the same random ID twice during this loop\n",
    "        existing_ids.add(new_id)\n",
    "\n",
    "print(\"Finished generating unique IDs.\")\n",
    "\n",
    "# --- 4. Replace Duplicated IDs in the DataFrame Index ---\n",
    "\n",
    "# Get the actual index values that need to be replaced (the index values of the duplicated rows)\n",
    "indices_to_replace = df_tracks.index[duplicated_mask]\n",
    "\n",
    "# Create a Series of the new IDs, matching the indices (positions) of the duplicated rows\n",
    "new_ids_series = pd.Series(\n",
    "    new_track_ids,\n",
    "    index=indices_to_replace\n",
    ")\n",
    "\n",
    "# Replace the duplicated index values in-place\n",
    "df_tracks.index.values[duplicated_mask] = new_ids_series.values\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"\\nGenerated {len(new_track_ids)} new unique IDs.\")\n",
    "print(f\"Example new ID: {new_track_ids[0]}\")\n",
    "print(f\"Check for duplicates after replacement: {df_tracks.index.duplicated().any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2032d",
   "metadata": {},
   "source": [
    "## üßπ Removing Redundant Artist Columns\n",
    "\n",
    "We discovered that **`name_artist`**, **`name`**, and **`primary_artist`** all highlight the same information, creating unnecessary redundancy in our dataset. To determine which columns to keep, we performed a thorough **normalization and comparison analysis**.\n",
    "\n",
    "After joining the tracks and artists DataFrames, we implemented a **helper function** to normalize all artist-related string columns. This normalization process includes:\n",
    "- Converting to lowercase\n",
    "- Removing accents (e.g., '√®' ‚Üí 'e')  \n",
    "- Stripping special characters\n",
    "- Trimming whitespace\n",
    "\n",
    "We applied this normalization to **`name`**, **`primary_artist`**, **`name_artist`**, and **`featured_artists`** to ensure a fair comparison. Our analysis revealed that **`primary_artist`** and **`name_artist`** are *identical* after normalization, while **`name`** contains the same unique values but with a slightly altered version for some artists, hence still being redundant.\n",
    "\n",
    "We also checked for **self-titled tracks** (where the track name matches the artist name) and examined edge cases like featured artists. Based on these findings, we confidently **dropped** the redundant **`name`** and **`primary_artist`** columns, retaining only **`name_artist`** as this column (equivalent to **`primary_artist`**) matches the same version of how the artist name is written in **`featured_artists`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09fbdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tracks.join(df_artists, on='id_artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a7e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione helper per la normalizzazione\n",
    "def normalize_series(series):\n",
    "    # 1. Minuscolo\n",
    "    s = series.str.lower()\n",
    "    \n",
    "    # 2. Rimuove accenti (es. '√®' -> 'e')\n",
    "    # NFKD normalizza i caratteri, 'ascii' rimuove ci√≤ che non √® ascii (accenti)\n",
    "    s = s.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    \n",
    "    # 3. Rimuove caratteri speciali (tutto tranne lettere, numeri, spazi)\n",
    "    # [^\\w\\s] significa \"tutto ci√≤ che NON √® un carattere di parola (\\w) o uno spazio (\\s)\"\n",
    "    s = s.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    \n",
    "    # 4. Rimuove spazi extra all'inizio/fine\n",
    "    s = s.str.strip()\n",
    "    \n",
    "    # (Opzionale) Sostituisce spazi multipli con uno singolo\n",
    "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5f1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applichiamo la normalizzazione alle tre colonne\n",
    "df['name'] = normalize_series(df['name'])\n",
    "df['primary_artist'] = normalize_series(df['primary_artist'])\n",
    "df['name_artist'] = normalize_series(df['name_artist'])\n",
    "df['featured_artists'] = normalize_series(df['featured_artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03edf8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi: 'primary_artist' e 'name_artist' sono sempre identici dopo la normalizzazione.\n"
     ]
    }
   ],
   "source": [
    "# Controlla se le due colonne sono SEMPRE identiche\n",
    "are_artists_identical = (df['primary_artist'] == df['name_artist']).all()\n",
    "\n",
    "if are_artists_identical:\n",
    "    print(\"Analisi: 'primary_artist' e 'name_artist' sono sempre identici dopo la normalizzazione.\")\n",
    "else:\n",
    "    print(\"Analisi: 'primary_artist' e 'name_artist' NON sono sempre identici.\")\n",
    "    \n",
    "    diff_df = df[df['primary_artist'] != df['name_artist']]\n",
    "    print(diff_df[['primary_artist', 'name_artist', 'primary_artist', 'name_artist']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd05ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name primary_artist\n",
      "id                                   \n",
      "TR317207   gue pequeno            gue\n",
      "TR446826   gue pequeno            gue\n",
      "TR228275   gue pequeno            gue\n",
      "TR697556   gue pequeno            gue\n",
      "TR391415   gue pequeno            gue\n",
      "...                ...            ...\n",
      "TR794750  samuel heron   samuel costa\n",
      "TR102539  samuel heron   samuel costa\n",
      "TR178809   joey funboy       joey ita\n",
      "TR589443   joey funboy       joey ita\n",
      "TR735987   joey funboy       joey ita\n",
      "\n",
      "[870 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cerca tracce omonime (dove il nome della traccia √® uguale al nome dell'artista)\n",
    "self_titled_tracks = df[df['name'] != df['primary_artist']]\n",
    "print(self_titled_tracks[['name', 'primary_artist']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf3beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['name', 'primary_artist'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fbd38",
   "metadata": {},
   "source": [
    "Active_end column is completely empty so we can drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2707cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['active_end'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f54aac",
   "metadata": {},
   "source": [
    "## üîç Analyzing `full_title`\n",
    "\n",
    "### `full_title` vs `title` Redundancy\n",
    "\n",
    "The **`full_title`** and **`title`** attributes should theoretically correspond, as both identify the track's name. However, **`full_title`** contains additional information by appending the performer with **\"by (artist_name)\"** and featuring artists with **\"Ft. (featured_artists)\"**.\n",
    "\n",
    "This explains why **`full_title`** has more unique values compared to **`title`**. However, by examining the actual title name contained in the first portion of **`full_title`**, we notice that the two columns do in fact correspond to the same underlying track name.\n",
    "\n",
    "We performed a **regex-based extraction and normalization** to verify this relationship holds across *all* records. The process involved:\n",
    "\n",
    "- **Extracting** the title portion from **`full_title`** by splitting at the last occurrence of `\" by\"`\n",
    "- **Normalizing smart quotes and apostrophes** (e.g., `'` ‚Üí `'`, `\"` ‚Üí `\"`) to handle encoding differences\n",
    "- **Standardizing whitespace** by stripping leading/trailing spaces and collapsing multiple spaces into one\n",
    "\n",
    "After these comprehensive normalization steps, we confirmed that the extracted title from **`full_title`** is *identical* to **`title`** across all rows. This allows us to confidently **discard** one of the two columns, eliminating redundancy while preserving complete information.\n",
    "\n",
    "This verification ensures data integrity and simplifies our schema for future analysis. ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66840002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(series):\n",
    "    \"\"\"\n",
    "    Normalize a pandas Series containing titles by:\n",
    "    - Replacing smart quotes/apostrophes with straight ones\n",
    "    - Stripping leading/trailing whitespace\n",
    "    - Collapsing multiple spaces into single spaces\n",
    "    \"\"\"\n",
    "    # Normalize smart apostrophes\n",
    "    s = series.str.replace('‚Äô', \"'\", regex=False)\n",
    "    s = s.str.replace('‚Äò', \"'\", regex=False)\n",
    "\n",
    "    # Normalize smart double quotes\n",
    "    s = s.str.replace('‚Äú', '\"', regex=False)\n",
    "    s = s.str.replace('‚Äù', '\"', regex=False)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    s = s.str.strip()\n",
    "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2af45c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the columns equal after normalization? True\n",
      "Number of rows still unequal: 0\n"
     ]
    }
   ],
   "source": [
    "df_title = df_tracks[['full_title', 'title']].copy()\n",
    "\n",
    "# Extract title portion from full_title\n",
    "split_series = df_title['full_title'].str.rsplit(' by', n=1)\n",
    "df_title['cleaned_attribute'] = split_series.str[0]\n",
    "\n",
    "# Apply normalization to both columns\n",
    "df_title['cleaned_attribute'] = normalize_title(df_title['cleaned_attribute'])\n",
    "df_title['title'] = normalize_title(df_title['title'])\n",
    "\n",
    "# Compare results\n",
    "are_columns_equal_final = (df_title['cleaned_attribute'] == df_title['title']).all()\n",
    "print(f\"Are the columns equal after normalization? {are_columns_equal_final}\")\n",
    "\n",
    "# Check the remaining mismatched rows (should now be 0)\n",
    "final_mismatched_rows = df_title[df_title['cleaned_attribute'] != df_title['title']]\n",
    "print(f\"Number of rows still unequal: {len(final_mismatched_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ec2c7",
   "metadata": {},
   "source": [
    "## üéµ Validating Artist and Featured Artists from `full_title`\n",
    "\n",
    "Having established that **`full_title`** contains redundant information about track titles, we now investigate whether the **artist** and **featured artists** information embedded in **`full_title`** matches the dedicated columns **`name_artist`** and **`featured_artists`**.\n",
    "\n",
    "The **`full_title`** follows the pattern: `\"Track Name by Artist (Ft. Featured Artists)\"` or `\"Track Name by Artist, Featured Artist 1, ... & Featured Artist N\"`. We perform a **multi-step extraction and normalization** process to validate this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6cb61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the artist names equal? True\n",
      "Number of rows still unequal: 0\n"
     ]
    }
   ],
   "source": [
    "df_artist_and_feat = df_tracks[['full_title', 'name_artist', 'featured_artists']].copy()\n",
    "\n",
    "df_artist_and_feat['name_artist'] = normalize_title(df_artist_and_feat['name_artist'])\n",
    "df_artist_and_feat['featured_artists'] = normalize_title(df_artist_and_feat['featured_artists'])\n",
    "df_artist_and_feat['full_title'] = normalize_title(df_artist_and_feat['full_title'])\n",
    "\n",
    "# --- Step 1: Extract 'artist_and_feat' (Artist + Features) ---\n",
    "split_series_1 = df_artist_and_feat['full_title'].str.rsplit(' by', n=1)\n",
    "df_artist_and_feat['artist_and_feat'] = split_series_1.str[1]\n",
    "df_artist_and_feat.drop(columns=['full_title'], inplace=True)\n",
    "\n",
    "# --- Step 2: Separate 'cleaned_artist' from 'cleaned_feat' ---\n",
    "split_series_2 = df_artist_and_feat['artist_and_feat'].str.rsplit('(Ft.', n=1)\n",
    "df_artist_and_feat['cleaned_artist'] = split_series_2.str[0]\n",
    "df_artist_and_feat['cleaned_feat'] = split_series_2.str[1].str.replace(r'\\)$', '', regex=True)\n",
    "\n",
    "# --- NEW: Convert & to , BEFORE extracting the artist ---\n",
    "df_artist_and_feat['cleaned_artist'] = df_artist_and_feat['cleaned_artist'].str.replace('&', ',', regex=False)\n",
    "\n",
    "# --- Extract primary artist by splitting at the FIRST comma ---\n",
    "split_series_3 = df_artist_and_feat['cleaned_artist'].str.split(',', n=1)\n",
    "df_artist_and_feat['cleaned_artist'] = split_series_3.str[0]\n",
    "\n",
    "# The remaining artists after the first comma become features\n",
    "remaining_artists = split_series_3.str[1]\n",
    "\n",
    "# --- Move remaining artists to 'cleaned_feat' if '(Ft....)' was empty ---\n",
    "mask_empty_feat = df_artist_and_feat['cleaned_feat'].isna() | (df_artist_and_feat['cleaned_feat'].str.strip() == '')\n",
    "\n",
    "df_artist_and_feat['cleaned_feat'] = df_artist_and_feat['cleaned_feat'].mask(\n",
    "    mask_empty_feat,\n",
    "    remaining_artists.fillna('').str.strip()\n",
    ")\n",
    "\n",
    "# --- Strip whitespace from cleaned_artist ---\n",
    "df_artist_and_feat['cleaned_artist'] = df_artist_and_feat['cleaned_artist'].str.strip()\n",
    "\n",
    "# --- Final Comparison ---\n",
    "are_names_equal_final = (df_artist_and_feat['cleaned_artist'] == df_artist_and_feat['name_artist']).all()\n",
    "print(f\"Are the artist names equal? {are_names_equal_final}\")\n",
    "\n",
    "# Identify and print the remaining mismatched rows\n",
    "final_mismatched_rows = df_artist_and_feat[df_artist_and_feat['cleaned_artist'] != df_artist_and_feat['name_artist']]\n",
    "print(f\"Number of rows still unequal: {len(final_mismatched_rows)}\")\n",
    "\n",
    "# if len(final_mismatched_rows) > 0:\n",
    "#     print(\"\\nSample of remaining mismatched rows:\")\n",
    "#     rows_to_display = final_mismatched_rows.head(10)\n",
    "#     print(rows_to_display[['name_artist', 'cleaned_artist']])\n",
    "    \n",
    "#     print(\"\\nFirst Mismatched Row Details:\")\n",
    "#     first_id = rows_to_display.index[0]\n",
    "#     print(f\"name_artist: '{df_artist_and_feat['name_artist'].loc[first_id]}'\")\n",
    "#     print(f\"cleaned_artist: '{df_artist_and_feat['cleaned_artist'].loc[first_id]}'\")\n",
    "\n",
    "# df_artist_and_feat.drop(columns=['artist_and_feat'], inplace=True)\n",
    "# print(df_artist_and_feat[['name_artist', 'cleaned_artist']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee95500",
   "metadata": {},
   "source": [
    "After applying comprehensive normalization to compare **`cleaned_feat`** (extracted from **`full_title`**) with the original **`featured_artists`** column, we identified **413 mismatched rows**.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "The mismatches reveal a systematic pattern: in many cases, the original **`featured_artists`** column is *empty* while **`cleaned_feat`** contains valid artist names extracted from **`full_title`**. This indicates that **`full_title`** actually contains *more complete* information about featured artists than the dedicated **`featured_artists`** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51dfbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_artists(series):\n",
    "    \"\"\"\n",
    "    Normalize and sort a pandas Series containing artist names by:\n",
    "    - Replacing '&' with ',' for consistent delimiter\n",
    "    - Splitting by comma into individual artists\n",
    "    - Stripping whitespace from each artist name\n",
    "    - Sorting artists alphabetically\n",
    "    - Rejoining into a single comma-separated string\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        A pandas Series containing artist names (can be comma or ampersand-separated)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        A normalized Series with alphabetically sorted, comma-separated artist names\n",
    "    \"\"\"\n",
    "    # Replace & with , for consistent delimiter\n",
    "    s = series.str.replace('&', ',', regex=False)\n",
    "    \n",
    "    # Split by comma, strip whitespace, and filter out empty strings\n",
    "    list_artists = s.str.split(',').apply(\n",
    "        lambda x: [item.strip() for item in x if item.strip()] if isinstance(x, list) else []\n",
    "    )\n",
    "    \n",
    "    # Sort alphabetically\n",
    "    sorted_artists = list_artists.apply(lambda x: sorted(x))\n",
    "    \n",
    "    # Rejoin into comma-separated string\n",
    "    normalized_series = sorted_artists.apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    return normalized_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdcf3554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows still unequal: 413\n",
      "\n",
      "Sample of remaining mismatched rows:\n",
      "         featured_artists                                       cleaned_feat\n",
      "id                                                                          \n",
      "TR266736                   Friman (ITA), Mehdi (ITA), Mothz, Spender, The...\n",
      "TR281032                                                           Manu Chao\n",
      "TR811171                                                         Mara Sattei\n",
      "TR822203                                                         Mara Sattei\n",
      "TR397308                                                       Tiziano Ferro\n",
      "TR212338                                                         Mara Sattei\n",
      "TR372774                                                         Mara Sattei\n",
      "TR993112                                                         Mara Sattei\n",
      "TR444969                                                         Mara Sattei\n",
      "TR479694                                                         Mara Sattei\n",
      "\n",
      "Cleaned Series for First Mismatched Row (After Aggressive Strip):\n",
      "featured_artists (normalized): ''\n",
      "cleaned_feat (normalized): 'Friman (ITA), Mehdi (ITA), Mothz, Spender, Thelonious B., Zyrtck'\n"
     ]
    }
   ],
   "source": [
    "df_artist_and_feat['featured_artists'] = sort_artists(df_artist_and_feat['featured_artists'])\n",
    "df_artist_and_feat['cleaned_feat'] = sort_artists(df_artist_and_feat['cleaned_feat'])\n",
    "\n",
    "# Identify and print the remaining mismatched rows using the normalized series\n",
    "final_mismatched_rows = df_artist_and_feat[df_artist_and_feat['cleaned_feat'] != df_artist_and_feat['featured_artists']]\n",
    "print(f\"Number of rows still unequal: {len(final_mismatched_rows)}\")\n",
    "\n",
    "# Print the remaining mismatched rows for inspection\n",
    "if len(final_mismatched_rows) > 0:\n",
    "    print(\"\\nSample of remaining mismatched rows:\")\n",
    "    # We display the original columns and the two normalized versions for true inspection\n",
    "    rows_to_display = final_mismatched_rows.head(10)\n",
    "    print(rows_to_display[['featured_artists', 'cleaned_feat']])\n",
    "\n",
    "    print(\"\\nCleaned Series for First Mismatched Row (After Aggressive Strip):\")\n",
    "    first_id = rows_to_display.index[0]\n",
    "    # Use the normalized series for the clearest inspection\n",
    "    print(f\"featured_artists (normalized): '{df_artist_and_feat['featured_artists'].loc[first_id]}'\")\n",
    "    print(f\"cleaned_feat (normalized): '{df_artist_and_feat['cleaned_feat'].loc[first_id]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49f2ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11166 entries, TR934808 to TR552777\n",
      "Data columns (total 54 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  object        \n",
      " 2   full_title            11166 non-null  string        \n",
      " 3   title                 11166 non-null  string        \n",
      " 4   featured_artists      3517 non-null   object        \n",
      " 5   language              11061 non-null  category      \n",
      " 6   album                 9652 non-null   string        \n",
      " 7   stats_pageviews       4642 non-null   Int64         \n",
      " 8   swear_IT              11166 non-null  int64         \n",
      " 9   swear_EN              11166 non-null  int64         \n",
      " 10  swear_IT_words        11166 non-null  object        \n",
      " 11  swear_EN_words        11166 non-null  object        \n",
      " 12  year                  10728 non-null  Int64         \n",
      " 13  month                 9969 non-null   Int64         \n",
      " 14  day                   9843 non-null   Int64         \n",
      " 15  n_sentences           11090 non-null  float64       \n",
      " 16  n_tokens              11090 non-null  float64       \n",
      " 17  tokens_per_sent       11090 non-null  float64       \n",
      " 18  char_per_tok          11090 non-null  float64       \n",
      " 19  lexical_density       11090 non-null  float64       \n",
      " 20  avg_token_per_clause  11090 non-null  float64       \n",
      " 21  bpm                   11102 non-null  float64       \n",
      " 22  centroid              11102 non-null  float64       \n",
      " 23  rolloff               11102 non-null  float64       \n",
      " 24  flux                  11102 non-null  float64       \n",
      " 25  rms                   11102 non-null  float64       \n",
      " 26  zcr                   11102 non-null  float64       \n",
      " 27  flatness              11102 non-null  float64       \n",
      " 28  spectral_complexity   11102 non-null  float64       \n",
      " 29  pitch                 11102 non-null  float64       \n",
      " 30  loudness              11102 non-null  float64       \n",
      " 31  album_name            11088 non-null  string        \n",
      " 32  album_release_date    10827 non-null  datetime64[ns]\n",
      " 33  album_type            11088 non-null  category      \n",
      " 34  disc_number           11088 non-null  Int64         \n",
      " 35  track_number          11088 non-null  Int64         \n",
      " 36  duration_ms           11088 non-null  float64       \n",
      " 37  explicit              11166 non-null  bool          \n",
      " 38  popularity            11137 non-null  Int64         \n",
      " 39  album_image           11088 non-null  string        \n",
      " 40  id_album              11088 non-null  category      \n",
      " 41  lyrics                11163 non-null  string        \n",
      " 42  modified_popularity   11166 non-null  bool          \n",
      " 43  gender                11166 non-null  category      \n",
      " 44  birth_date            8588 non-null   datetime64[ns]\n",
      " 45  birth_place           8588 non-null   category      \n",
      " 46  nationality           8557 non-null   category      \n",
      " 47  description           10028 non-null  string        \n",
      " 48  active_start          6565 non-null   datetime64[ns]\n",
      " 49  province              8467 non-null   category      \n",
      " 50  region                8024 non-null   category      \n",
      " 51  country               8467 non-null   category      \n",
      " 52  latitude              8588 non-null   float64       \n",
      " 53  longitude             8588 non-null   float64       \n",
      "dtypes: Int64(7), bool(2), category(9), datetime64[ns](3), float64(19), int64(2), object(5), string(7)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55dd5e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "TR934808             [ernia, gue]\n",
      "TR760029           [thelonious b]\n",
      "TR916821    [mambolosco, radical]\n",
      "TR480968                 [taxi b]\n",
      "TR585039                  [rkomi]\n",
      "Name: featured_artists, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['featured_artists'] = df_artist_and_feat['cleaned_feat']\n",
    "\n",
    "df['featured_artists'] = df['featured_artists'].apply(\n",
    "    lambda x: [] if pd.isna(x) else [s.strip() for s in x.split(',')]\n",
    ")\n",
    "\n",
    "df['featured_artists'] = df['featured_artists'].apply(\n",
    "    lambda lst: normalize_series(pd.Series(lst)).tolist() if lst else []\n",
    ")\n",
    "\n",
    "print(df['featured_artists'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422c9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for featured_artist in df['featured_artists']\n",
    "# df['featured_artists'] = normalize_series(df_artist_and_feat['cleaned_feat'])\n",
    "# print(df['featured_artists'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccbd6a",
   "metadata": {},
   "source": [
    "Now full title column is redundant: the featured artist has been extracted and the title column is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ffdbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['full_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9baa21a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "TR934808             [ernia, gue]\n",
      "TR760029           [thelonious b]\n",
      "TR916821    [mambolosco, radical]\n",
      "TR480968                 [taxi b]\n",
      "TR585039                  [rkomi]\n",
      "TR550335                       []\n",
      "TR170793                       []\n",
      "TR627195              [dani faiv]\n",
      "TR628871                       []\n",
      "TR700756                       []\n",
      "Name: featured_artists, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['featured_artists'].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f6ea7",
   "metadata": {},
   "source": [
    "## Language attribute\n",
    "Most present language for main lyrics are italian. english and polish. We checked most of these languages and they don't seem to respect the main language of the lyrics.\n",
    "\n",
    "So we decided to run a SOTA language model to detect based on the tokens of he lyrics colmn the language of the track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74805d63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m  \n\u001b[0;32m      4\u001b[0m df_language \u001b[38;5;241m=\u001b[39m df_tracks[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import regex as re  \n",
    "\n",
    "df_language = df_tracks[['language', 'lyrics', 'n_sentences']].copy()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    # Normalize smart quotes to straight quotes\n",
    "    text = re.sub(r'[‚Äò‚Äô]', \"'\", str(text))\n",
    "    text = re.sub(r'[‚Äú‚Äù]', '\"', text)\n",
    "    # Aggressively remove characters that might be noise or confuse the model (e.g., emojis, non-standard symbols)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\'\\\"]', '', text, flags=re.UNICODE)\n",
    "    return text\n",
    "\n",
    "df_language['lyrics_normalized'] = df_language['lyrics'].apply(normalize_text)\n",
    "\n",
    "print(df_language['lyrics_normalized'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872851d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         language most_probable_language  confidence  n_sentences\n",
      "id                                                               \n",
      "TR934808       pl                   __it    0.964671        102.0\n",
      "TR760029       en                   __it    0.923832         56.0\n",
      "TR916821       en                   __it    0.953723         88.0\n",
      "TR480968       it                   __it    0.963273         37.0\n",
      "TR585039       en                   __it    0.983940         48.0\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'lid.176.bin'\n",
    "\n",
    "model = fasttext.load_model(MODEL_PATH)\n",
    "\n",
    "def detect_language_safe(text, model):\n",
    "    \"\"\"\n",
    "    Safely detects the language and confidence using FastText.\n",
    "    Fortified to handle DataFrame edge cases (NaN, None, short strings).\n",
    "    Returns a tuple (language_code, confidence_score) or (None, 0.0).\n",
    "    \"\"\"\n",
    "    # 1. Explicitly check for NaN/None and ensure string conversion\n",
    "    if pd.isna(text):\n",
    "        return None, 0.0\n",
    "    \n",
    "    # Ensure it's a string and strip whitespace\n",
    "    text_str = str(text).strip()\n",
    "    \n",
    "    # FIX: Remove newline and carriage return characters, as FastText requires a single line\n",
    "    text_str = text_str.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # FastText needs a minimum amount of text (let's keep the minimum length check)\n",
    "    if len(text_str) < 20: \n",
    "        # Optionally log which records were too short\n",
    "        # print(f\"Skipping record due to short length: {text_str[:10]}...\")\n",
    "        return None, 0.0\n",
    "    \n",
    "    try:\n",
    "        # k=1 asks for the single best prediction\n",
    "        predictions = model.predict(text_str, k=1) \n",
    "        \n",
    "        # predictions[0] is the label list: ['label__it']\n",
    "        # predictions[1] is the probability list: [0.99]\n",
    "        label = predictions[0][0].replace('__label', '')\n",
    "        confidence = predictions[1][0]\n",
    "        \n",
    "        return label, confidence\n",
    "    except Exception as e:\n",
    "        # If an exception is still caught, print a detailed message \n",
    "        # to help diagnose the specific content causing the crash.\n",
    "        print(f\"FastText Prediction failed for input starting: '{text_str[:50]}...'\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return None, 0.0\n",
    "\n",
    "results = df_language['lyrics_normalized'].apply(\n",
    "    lambda x: detect_language_safe(x, model)\n",
    ")\n",
    "\n",
    "# Unpack the Series of tuples into the two new columns\n",
    "\n",
    "# The first element of the tuple is the language code\n",
    "df_language['most_probable_language'] = results.apply(lambda x: x[0])\n",
    "\n",
    "# The second element of the tuple is the confidence score\n",
    "df_language['confidence'] = results.apply(lambda x: x[1])\n",
    "\n",
    "# Displaying the new columns (optional)\n",
    "print(df_language[['language', 'most_probable_language', 'confidence', 'n_sentences']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044cc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHHCAYAAACiOWx7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAY9dJREFUeJzt3Xl8TNf/P/DXZJtMIkGQjUhSIsQuPohdE2JrbaVaKkgpgiT2pbbQqiWxVSklllJqqbYoib2I2GIpqb1JRRYlRIRsc35/+OV+jQR3uEkGr+fjkUd7zz1z7mtm7sQ79557RyWEECAiIiKiFzIq7gBEREREbwIWTUREREQysGgiIiIikoFFExEREZEMLJqIiIiIZGDRRERERCQDiyYiIiIiGVg0EREREcnAoomIiIhIBhZNZBCmTp0KlUpVJNtq2bIlWrZsKS0fOHAAKpUKmzdvLpLt9+3bFy4uLkWyrVeVnp6Ozz//HPb29lCpVAgKCiruSPQaVq1aBZVKhX/++Udqe/ZzUNwKyliQovxdQfQsFk2kuLxffnk/5ubmcHR0hK+vLxYuXIgHDx4osp1bt25h6tSpOHPmjCLjKcmQs8nx9ddfY9WqVRg8eDDWrl2Lzz777Ll9XVxcdN7vp38eP36sWKaMjAxMnToVBw4ckNr++eef52772Z+X/WMs18WLFzF16lTFxnuTFPQeEL1LTIo7AL29QkJC4OrqiuzsbCQlJeHAgQMICgpCWFgYfvvtN9SqVUvq++WXX2LcuHF6jX/r1i1MmzYNLi4uqFOnjuzHRURE6LWdV/GibMuXL4dWqy30DK9j3759aNSoEaZMmSKrf506dTBy5Mh87WZmZoplysjIwLRp0wBAOkJSrlw5rF27VqdfaGgobt68iXnz5um0lytXTpEcFy9exLRp09CyZUuDP2L4Iq/yOSjoPSB6l7BookLTrl071K9fX1oeP3489u3bh44dO+LDDz9EbGwsNBoNAMDExAQmJoW7O2ZkZMDCwkLRf8hfhampabFuX46UlBR4eHjI7l++fHn07t27EBMVzNLSMt92N2zYgNTU1GLJo7ScnBxotdpC2WeL+3NA9Cbi6TkqUu+//z4mTZqEuLg4/Pjjj1J7QfMUIiMj0bRpU5QqVQolSpSAu7s7JkyYAODJPKT//e9/AIB+/fpJp2BWrVoF4MlfwTVq1MCpU6fQvHlzWFhYSI993lyO3NxcTJgwAfb29rC0tMSHH36If//9V6ePi4sL+vbtm++xT4/5smwFzWl6+PAhRo4cCScnJ6jVari7u2Pu3LkQQuj0U6lUGDp0KLZt24YaNWpArVajevXq2LVrV8Ev+DNSUlLg7+8POzs7mJubo3bt2li9erW0Pm9+140bN7Bjxw5FTm2Fh4fj/fffh62tLdRqNTw8PLBkyZJ8/U6ePAlfX1+ULVsWGo0Grq6u6N+/P4Anp+HyjhRNmzZNyjV16lRZGTIzMzFlyhRUrlwZarUaTk5OGDNmDDIzM6U+fn5+MDc3R2xsrM5jfX19Ubp0ady6dQurVq1C9+7dAQCtWrWScrzodFXfvn1RokQJXL9+Hb6+vrC0tISjoyNCQkJ03t+8U41z587F/PnzUalSJajValy8eBEA8Pfff+Ojjz6CjY0NzM3NUb9+ffz222/5tnfhwgW8//770Gg0qFChAmbMmFHgkc2CPgePHz/G1KlTUaVKFZibm8PBwQFdu3bFtWvXZL0HSmeUS+4+5uLigo4dO+Lw4cNo0KABzM3N8d5772HNmjX5+p47dw4tWrTQyRgeHp7v8/C8/fDZ3xV3797FqFGjULNmTZQoUQLW1tZo164dzp49m++xcXFx+PDDD2FpaQlbW1sEBwdj9+7dBe5r0dHRaNu2LUqWLAkLCwu0aNECR44c0enz4MEDBAUFwcXFBWq1Gra2tmjdujVOnz794heW8uGRJipyn332GSZMmICIiAgMGDCgwD4XLlxAx44dUatWLYSEhECtVuPq1avSL4Nq1aohJCQEkydPxsCBA9GsWTMAQOPGjaUx7ty5g3bt2qFnz57o3bs37OzsXpjrq6++gkqlwtixY5GSkoL58+fDx8cHZ86ckY6IySEn29OEEPjwww+xf/9++Pv7o06dOti9ezdGjx6NhISEfKeZDh8+jK1bt2LIkCGwsrLCwoUL0a1bN8THx6NMmTLPzfXo0SO0bNkSV69exdChQ+Hq6opNmzahb9++uHfvHgIDA1GtWjWsXbsWwcHBqFChgnTK7WWntrKzs/Hff//ptFlYWMDCwgJLlixB9erV8eGHH8LExAS///47hgwZAq1Wi4CAAABPirk2bdqgXLlyGDduHEqVKoV//vkHW7dulba/ZMkSDB48GF26dEHXrl0BQOcU7/NotVp8+OGHOHz4MAYOHIhq1arh/PnzmDdvHi5fvoxt27YBABYsWIB9+/bBz88PUVFRMDY2xvfff4+IiAisXbsWjo6OaN68OYYPH46FCxdiwoQJqFatGgBI/32e3NxctG3bFo0aNcLs2bOxa9cuTJkyBTk5OQgJCdHpGx4ejsePH2PgwIFQq9WwsbHBhQsX0KRJE5QvXx7jxo2DpaUlfv75Z3Tu3BlbtmxBly5dAABJSUlo1aoVcnJypH7Lli2Ttf/m5uaiY8eO2Lt3L3r27InAwEA8ePAAkZGR+Ouvv+Dj4/PC96AoMj6PnH0sz9WrV/HRRx/B398ffn5+WLlyJfr27QtPT09Ur14dAJCQkCAVxePHj4elpSV++OEHqNXqV854/fp1bNu2Dd27d4erqyuSk5Px/fffo0WLFrh48SIcHR0BPPkD6v3330diYiICAwNhb2+P9evXY//+/fnG3LdvH9q1awdPT09MmTIFRkZGUgH5559/okGDBgCAQYMGYfPmzRg6dCg8PDxw584dHD58GLGxsahXr94rP6d3kiBSWHh4uAAgTpw48dw+JUuWFHXr1pWWp0yZIp7eHefNmycAiNu3bz93jBMnTggAIjw8PN+6Fi1aCABi6dKlBa5r0aKFtLx//34BQJQvX16kpaVJ7T///LMAIBYsWCC1OTs7Cz8/v5eO+aJsfn5+wtnZWVretm2bACBmzJih0++jjz4SKpVKXL16VWoDIMzMzHTazp49KwCIRYsW5dvW0+bPny8AiB9//FFqy8rKEl5eXqJEiRI6z93Z2Vl06NDhheM93RdAvp8pU6YIIYTIyMjI9xhfX1/x3nvvScu//PLLS/eZ27dv64z7PB06dNB5fdeuXSuMjIzEn3/+qdNv6dKlAoA4cuSI1LZ7927pvbh+/booUaKE6Ny5s87jNm3aJACI/fv3vzBHHj8/PwFADBs2TGrTarWiQ4cOwszMTNrHb9y4IQAIa2trkZKSojOGt7e3qFmzpnj8+LHOGI0bNxZubm5SW1BQkAAgoqOjpbaUlBRRsmRJAUDcuHFDan92n125cqUAIMLCwvI9B61WK4R48XtQGBkL8uzvCiHk7WNC/N++eujQIZ1tq9VqMXLkSKlt2LBhQqVSiZiYGKntzp07wsbGJl/G570ez/6uePz4scjNzdXpc+PGDaFWq0VISIjUFhoaKgCIbdu2SW2PHj0SVatW1dnvtFqtcHNzE76+vtL7k/dauLq6itatW0ttJUuWFAEBAfkykv54eo6KRYkSJV54FV2pUqUAAL/++usrH7ZXq9Xo16+f7P59+vSBlZWVtPzRRx/BwcEBO3fufKXty7Vz504YGxtj+PDhOu0jR46EEAJ//PGHTruPjw8qVaokLdeqVQvW1ta4fv36S7djb2+PTz75RGozNTXF8OHDkZ6ejoMHD77yc2jYsCEiIyN1fvr06QMAOkcQ7t+/j//++w8tWrTA9evXcf/+fQD/935v374d2dnZr5yjIJs2bUK1atVQtWpV/Pfff9LP+++/DwA6f8G3adMGX3zxBUJCQtC1a1eYm5vj+++/VyTH0KFDpf/PO82alZWFPXv26PTr1q2bzpG9u3fvYt++fejRowcePHgg5b9z5w58fX1x5coVJCQkAHjyHjdq1Eg6wgA8OUrXq1evl+bbsmULypYti2HDhuVb97JL/Isq4/PI2cfyeHh4SEd/87bt7u6u8/nZtWsXvLy8dC7isLGxea2MarUaRkZP/snNzc3FnTt3pGkHT58m27VrF8qXL48PP/xQajM3N893VP7MmTO4cuUKPv30U9y5c0d6zR8+fAhvb28cOnRI+t1ZqlQpREdH49atW6+cn57g6TkqFunp6bC1tX3u+o8//hg//PADPv/8c4wbNw7e3t7o2rUrPvroI+kXz8uUL19er8mubm5uOssqlQqVK1cu9EvL4+Li4OjoqFOwAf93yicuLk6nvWLFivnGKF26NFJTU1+6HTc3t3yv3/O2o4+yZcvCx8enwHVHjhzBlClTEBUVhYyMDJ119+/fR8mSJdGiRQt069YN06ZNw7x589CyZUt07twZn3766WudEgGAK1euIDY29rmnGFNSUnSW586di19//RVnzpzB+vXrX7ifymVkZIT33ntPp61KlSoAkG//cnV11Vm+evUqhBCYNGkSJk2aVOD4KSkpKF++POLi4tCwYcN8693d3V+a8dq1a3B3d3+lCzKKKuPzyNnH8sj5/MTFxcHLyytfv8qVK79yRq1WiwULFuC7777DjRs3kJubK617+rR6XFwcKlWqlK9QfXbbV65cAfBkLt7z3L9/H6VLl8bs2bPh5+cHJycneHp6on379ujTp0++fZJejkUTFbmbN2/i/v37L/wFpNFocOjQIezfvx87duzArl27sHHjRrz//vuIiIiAsbHxS7fzOnMknud5f3Hn5ubKyqSE521HPDNp3BBcu3YN3t7eqFq1KsLCwuDk5AQzMzPs3LkT8+bNk/4Szru56LFjx/D7779j9+7d6N+/P0JDQ3Hs2DGUKFHilTNotVrUrFkTYWFhBa53cnLSWY6JiZEKqfPnz+scmSsKz+63ea/RqFGj4OvrW+BjXucfcyUUZ0a5+1ieovr8PF0UAU/ufTZp0iT0798f06dPh42NDYyMjBAUFPRKR9PzHjNnzpzn3nIl73PTo0cPNGvWDL/88gsiIiIwZ84czJo1C1u3bkW7du303va7jEUTFbm8++o875drHiMjI3h7e8Pb2xthYWH4+uuvMXHiROzfvx8+Pj6K3xU47y+3PEIIXL16VWeycenSpXHv3r18j42Li9P5q02fbM7OztizZw8ePHigc7Tp77//ltYrwdnZGefOnYNWq9U52qT0dp72+++/IzMzE7/99pvOX/gFTWoFgEaNGqFRo0b46quvsH79evTq1QsbNmzA559//srvd6VKlXD27Fl4e3u/dIyHDx+iX79+8PDwQOPGjTF79mx06dJFuhoS0O+9zaPVanH9+nXp6BIAXL58GQBeeq+nvP3K1NT0uUfz8jg7O+fbjwHg0qVLL81YqVIlREdHIzs7+7m3xXjecy+qjAXRdx+Tw9nZGVevXs3XXlBbQb8TsrKykJiYqNO2efNmtGrVCitWrNBpv3fvHsqWLauz7YsXL0IIofN6P7vtvFP01tbWL33NAcDBwQFDhgzBkCFDkJKSgnr16uGrr75i0aQnzmmiIrVv3z5Mnz4drq6uL5wfcPfu3XxteX9N5V0mbmlpCQAFFjGvYs2aNTrzrDZv3ozExESdXyqVKlXCsWPHkJWVJbVt3749360J9MnWvn175Obm4ttvv9VpnzdvHlQqlWK/1Nq3b4+kpCRs3LhRasvJycGiRYtQokQJtGjRQpHtPC3vr/qn/4q/f/8+wsPDdfqlpqbm+0v/2ffbwsICgP7vd48ePZCQkIDly5fnW/fo0SM8fPhQWh47dizi4+OxevVqhIWFwcXFBX5+fjq3JnjV/e7p91cIgW+//Rampqbw9vZ+4eNsbW3RsmVLfP/99/n+IQaA27dvS//fvn17HDt2DMePH9dZv27dupfm69atG/777798+2FeXuD570FRZSyI3H1MH76+voiKitK5o//du3cLzFipUiUcOnRIp23ZsmX5jjQZGxvn28c3bdokzfV6etsJCQk6t2p4/Phxvv3X09MTlSpVwty5c5Genp4vV95rnpubm29el62tLRwdHXX2a5KHR5qo0Pzxxx/4+++/kZOTg+TkZOzbtw+RkZFwdnbGb7/9BnNz8+c+NiQkBIcOHUKHDh3g7OyMlJQUfPfdd6hQoQKaNm0K4Mkvq1KlSmHp0qWwsrKCpaUlGjZsmG9OiFw2NjZo2rQp+vXrh+TkZMyfPx+VK1fWmYD5+eefY/PmzWjbti169OiBa9eu4ccff9SZmK1vtg8++ACtWrXCxIkT8c8//6B27dqIiIjAr7/+iqCgoHxjv6qBAwfi+++/R9++fXHq1Cm4uLhg8+bNOHLkCObPn59vTpUS2rRpAzMzM3zwwQf44osvkJ6ejuXLl8PW1lbnH9fVq1fju+++Q5cuXVCpUiU8ePAAy5cvh7W1Ndq3bw/gyWkrDw8PbNy4EVWqVIGNjQ1q1KiBGjVqvDDDZ599hp9//hmDBg3C/v370aRJE+Tm5uLvv//Gzz//jN27d6N+/frYt28fvvvuO0yZMkW6DDs8PBwtW7bEpEmTMHv2bABPijljY2PMmjUL9+/fh1qtlu4R9Dzm5ubYtWsX/Pz80LBhQ/zxxx/YsWMHJkyYIOtO5YsXL0bTpk1Rs2ZNDBgwAO+99x6Sk5MRFRWFmzdvSvf6GTNmDNauXYu2bdsiMDBQupw/7yjji/Tp0wdr1qzBiBEjcPz4cTRr1gwPHz7Enj17MGTIEHTq1OmF70FRZCyI3H1MH2PGjMGPP/6I1q1bY9iwYdItBypWrIi7d+/qHAH6/PPPMWjQIHTr1g2tW7fG2bNnsXv3bp2jRwDQsWNHhISEoF+/fmjcuDHOnz+PdevW5ZtX9MUXX+Dbb7/FJ598gsDAQDg4OGDdunXS78u8bRsZGeGHH35Au3btUL16dfTr1w/ly5dHQkIC9u/fD2tra/z+++948OABKlSogI8++gi1a9dGiRIlsGfPHpw4cQKhoaGv9Pq804rlmj16q+XdciDvx8zMTNjb24vWrVuLBQsW6FzanufZy4j37t0rOnXqJBwdHYWZmZlwdHQUn3zyibh8+bLO43799Vfh4eEhTExMdC7xb9GihahevXqB+Z53y4GffvpJjB8/Xtja2gqNRiM6dOgg4uLi8j0+NDRUlC9fXqjVatGkSRNx8uTJfGO+KNuztxwQQogHDx6I4OBg4ejoKExNTYWbm5uYM2eOzqXEQjy5vLmgS4efdyuEZyUnJ4t+/fqJsmXLCjMzM1GzZs0Cb4ug7y0HXtT3t99+E7Vq1RLm5ubCxcVFzJo1S7q8Pe/S7dOnT4tPPvlEVKxYUajVamFrays6duwoTp48qTPW0aNHhaenpzAzM3vupd7P3nJAiCe3Vpg1a5aoXr26UKvVonTp0sLT01NMmzZN3L9/X6SlpQlnZ2dRr149kZ2drfPY4OBgYWRkJKKioqS25cuXi/fee08YGxu/9PYDfn5+wtLSUly7dk20adNGWFhYCDs7OzFlyhSdS9DzbjkwZ86cAse5du2a6NOnj7C3txempqaifPnyomPHjmLz5s06/c6dOydatGghzM3NRfny5cX06dPFihUrXnrLASGeXK4+ceJE4erqKkxNTYW9vb346KOPxLVr16Q+L3oPlM5YkIJuOSBnHxPi+ftqQa9FTEyMaNasmVCr1aJChQpi5syZYuHChQKASEpKkvrl5uaKsWPHirJlywoLCwvh6+srrl69WuAtB0aOHCkcHByERqMRTZo0EVFRUQVu+/r166JDhw5Co9GIcuXKiZEjR4otW7YIAOLYsWP5cnbt2lWUKVNGqNVq4ezsLHr06CH27t0rhBAiMzNTjB49WtSuXVtYWVkJS0tLUbt2bfHdd9+98HWmgqmEMMDZo0REb4m+ffti8+bNBZ5CoTdLUFAQvv/+e6SnpxfZhR955s+fj+DgYNy8eRPly5cv0m3T/+GcJiIiomc8evRIZ/nOnTtYu3YtmjZtWugF07Pbfvz4Mb7//nu4ubmxYCpmnNNERET0DC8vL7Rs2RLVqlVDcnIyVqxYgbS0tOfeh0pJXbt2RcWKFVGnTh3cv38fP/74I/7+++9XnixPymHRRERE9Iz27dtj8+bNWLZsGVQqFerVq4cVK1agefPmhb5tX19f/PDDD1i3bh1yc3Ph4eGBDRs24OOPPy70bdOLcU4TERERkQyc00REREQkA4smIiIiIhk4p0khWq0Wt27dgpWVleJf70FERESFQwiBBw8ewNHR8aVfCM+iSSG3bt3K98WfRERE9Gb4999/UaFChRf2YdGkkLyvoPj3339hbW2t6NjZ2dmIiIhAmzZtnvtFmsWNGZXBjMpgRmUwozKYURmFlTEtLQ1OTk6yvkqKRZNC8k7JWVtbF0rRZGFhAWtra4PemZnx9TGjMphRGcyoDGZURmFnlDO1hhPBiYiIiGRg0UREREQkA4smIiIiIhlYNBERERHJwKKJiIiISAYWTUREREQysGgiIiIikoFFExEREZEMLJqIiIiIZGDRRERERCQDiyYiIiIiGVg0EREREcnAoomIiIhIBhZNRERERDKYFHcAku/s2bMwMlK2zi1btiwqVqyo6JhERERvIxZNb4CbN28CAJo3b45Hjx4pOra5xgKX/o5l4URERPQSLJreAHfu3AEA2LQdhlxrR8XGzb7zL+5sD8V///3HoomIiOglWDS9QUxtysOkbKXijkFERPRO4kRwIiIiIhlYNBERERHJwKKJiIiISAYWTUREREQysGgiIiIikoFFExEREZEMLJqIiIiIZGDRRERERCQDiyYiIiIiGVg0EREREcnAoomIiIhIBhZNRERERDKwaCIiIiKSgUUTERERkQwsmoiIiIhkYNFEREREJAOLJiIiIiIZWDQRERERycCiiYiIiEgGFk1EREREMrBoIiIiIpKhWIumQ4cO4YMPPoCjoyNUKhW2bdums14IgcmTJ8PBwQEajQY+Pj64cuWKTp+7d++iV69esLa2RqlSpeDv74/09HSdPufOnUOzZs1gbm4OJycnzJ49O1+WTZs2oWrVqjA3N0fNmjWxc+dOxZ8vERERvbmKtWh6+PAhateujcWLFxe4fvbs2Vi4cCGWLl2K6OhoWFpawtfXF48fP5b69OrVCxcuXEBkZCS2b9+OQ4cOYeDAgdL6tLQ0tGnTBs7Ozjh16hTmzJmDqVOnYtmyZVKfo0eP4pNPPoG/vz9iYmLQuXNndO7cGX/99VfhPXkiIiJ6o5gU58bbtWuHdu3aFbhOCIH58+fjyy+/RKdOnQAAa9asgZ2dHbZt24aePXsiNjYWu3btwokTJ1C/fn0AwKJFi9C+fXvMnTsXjo6OWLduHbKysrBy5UqYmZmhevXqOHPmDMLCwqTiasGCBWjbti1Gjx4NAJg+fToiIyPx7bffYunSpUXwShAREZGhK9ai6UVu3LiBpKQk+Pj4SG0lS5ZEw4YNERUVhZ49eyIqKgqlSpWSCiYA8PHxgZGREaKjo9GlSxdERUWhefPmMDMzk/r4+vpi1qxZSE1NRenSpREVFYURI0bobN/X1zff6cKnZWZmIjMzU1pOS0sDAGRnZyM7O/t1n74OrVYLAFCbqCCMhWLjqkxU0Gg00Gq1r5057/FKP3clMaMymFEZzKgMZlTGu5xRn/EMtmhKSkoCANjZ2em029nZSeuSkpJga2urs97ExAQ2NjY6fVxdXfONkbeudOnSSEpKeuF2CjJz5kxMmzYtX3tERAQsLCzkPEW9zWpXEUCugiM6Ax/8hISEBCQkJCgyYmRkpCLjFCZmVAYzKoMZlcGMyngXM2ZkZMjua7BFk6EbP368ztGptLQ0ODk5oU2bNrC2tlZ0WzExMUhMTMTYP+Ihyri+/AEyZSVfR/L6cTh06BBq1679WmNlZ2cjMjISrVu3hqmpqUIJlcWMymBGZTCjMphRGe9yxrwzRXIYbNFkb28PAEhOToaDg4PUnpycjDp16kh9UlJSdB6Xk5ODu3fvSo+3t7dHcnKyTp+85Zf1yVtfELVaDbVana/d1NRU8R3OyOjJfP3MHAGRq1Js3MwcgUePHsHIyEixzIXx/JXGjMpgRmUwozKYURnvYkZ9xjLY+zS5urrC3t4ee/fuldrS0tIQHR0NLy8vAICXlxfu3buHU6dOSX327dsHrVaLhg0bSn0OHTqkc84yMjIS7u7uKF26tNTn6e3k9cnbDhEREVGxFk3p6ek4c+YMzpw5A+DJ5O8zZ84gPj4eKpUKQUFBmDFjBn777TecP38effr0gaOjIzp37gwAqFatGtq2bYsBAwbg+PHjOHLkCIYOHYqePXvC0dERAPDpp5/CzMwM/v7+uHDhAjZu3IgFCxbonFoLDAzErl27EBoair///htTp07FyZMnMXTo0KJ+SYiIiMhAFevpuZMnT6JVq1bScl4h4+fnh1WrVmHMmDF4+PAhBg4ciHv37qFp06bYtWsXzM3NpcesW7cOQ4cOhbe3N4yMjNCtWzcsXLhQWl+yZElEREQgICAAnp6eKFu2LCZPnqxzL6fGjRtj/fr1+PLLLzFhwgS4ublh27ZtqFGjRhG8CkRERPQmKNaiqWXLlhDi+ZfQq1QqhISEICQk5Ll9bGxssH79+hdup1atWvjzzz9f2Kd79+7o3r37iwMTERHRO8tg5zQRERERGRIWTUREREQysGgiIiIikoFFExEREZEMLJqIiIiIZGDRRERERCQDiyYiIiIiGVg0EREREcnAoomIiIhIBhZNRERERDKwaCIiIiKSgUUTERERkQwsmoiIiIhkYNFEREREJAOLJiIiIiIZWDQRERERycCiiYiIiEgGFk1EREREMrBoIiIiIpKBRRMRERGRDCyaiIiIiGRg0UREREQkA4smIiIiIhlYNBERERHJYKJP59jYWGzYsAF//vkn4uLikJGRgXLlyqFu3brw9fVFt27doFarCysrERERUbGRdaTp9OnT8PHxQd26dXH48GE0bNgQQUFBmD59Onr37g0hBCZOnAhHR0fMmjULmZmZhZ2biIiIqEjJOtLUrVs3jB49Gps3b0apUqWe2y8qKgoLFixAaGgoJkyYoFRGIiIiomInq2i6fPkyTE1NX9rPy8sLXl5eyM7Ofu1gRERERIZE1um5pwum69ev69WfiIiI6G2g99VzlStXRqtWrfDjjz/i8ePHhZGJiIiIyODoXTSdPn0atWrVwogRI2Bvb48vvvgCx48fL4xsRERERAZD76KpTp06WLBgAW7duoWVK1ciMTERTZs2RY0aNRAWFobbt28XRk4iIiKiYvXKN7c0MTFB165dsWnTJsyaNQtXr17FqFGj4OTkhD59+iAxMVHJnERERETF6pWLppMnT2LIkCFwcHBAWFgYRo0ahWvXriEyMhK3bt1Cp06dlMxJREREVKz0uiM4AISFhSE8PByXLl1C+/btsWbNGrRv3x5GRk/qL1dXV6xatQouLi5KZyUiIiIqNnoXTUuWLEH//v3Rt29fODg4FNjH1tYWK1aseO1wRERERIZC76LpypUrL+1jZmYGPz+/VwpEREREZIj0ntMUHh6OTZs25WvftGkTVq9erUgoIiIiIkOjd9E0c+ZMlC1bNl+7ra0tvv76a0VCERERERkavYum+Ph4uLq65mt3dnZGfHy8IqGIiIiIDI3eRZOtrS3OnTuXr/3s2bMoU6aMIqGIiIiIDI3eRdMnn3yC4cOHY//+/cjNzUVubi727duHwMBA9OzZszAyEhERERU7va+emz59Ov755x94e3vDxOTJw7VaLfr06cM5TURERPTW0rtoMjMzw8aNGzF9+nScPXsWGo0GNWvWhLOzc2HkIyIiIjIIehdNeapUqYIqVaoomYWIiIjIYOldNOXm5mLVqlXYu3cvUlJSoNVqddbv27dPsXBEREREhkLvoikwMBCrVq1Chw4dUKNGDahUqsLIRURERGRQ9C6aNmzYgJ9//hnt27cvjDxEREREBknvWw6YmZmhcuXKhZGFiIiIyGDpXTSNHDkSCxYsgBCiMPIQERERGSS9T88dPnwY+/fvxx9//IHq1avD1NRUZ/3WrVsVC0dERERkKPQumkqVKoUuXboURhYiIiIig6V30RQeHl4YOYiIiIgMmt5zmgAgJycHe/bswffff48HDx4AAG7duoX09HRFwxEREREZCr2Lpri4ONSsWROdOnVCQEAAbt++DQCYNWsWRo0apWi43NxcTJo0Ca6urtBoNKhUqRKmT5+uMwldCIHJkyfDwcEBGo0GPj4+uHLlis44d+/eRa9evWBtbY1SpUrB398/X4F37tw5NGvWDObm5nBycsLs2bMVfS5ERET0ZtO7aAoMDET9+vWRmpoKjUYjtXfp0gV79+5VNNysWbOwZMkSfPvtt4iNjcWsWbMwe/ZsLFq0SOoze/ZsLFy4EEuXLkV0dDQsLS3h6+uLx48fS3169eqFCxcuIDIyEtu3b8ehQ4cwcOBAaX1aWhratGkDZ2dnnDp1CnPmzMHUqVOxbNkyRZ8PERERvbn0ntP0559/4ujRozAzM9Npd3FxQUJCgmLBAODo0aPo1KkTOnToIG3jp59+wvHjxwE8Oco0f/58fPnll+jUqRMAYM2aNbCzs8O2bdvQs2dPxMbGYteuXThx4gTq168PAFi0aBHat2+PuXPnwtHREevWrUNWVhZWrlwJMzMzVK9eHWfOnEFYWJhOcUVERETvLr2LJq1Wi9zc3HztN2/ehJWVlSKh8jRu3BjLli3D5cuXUaVKFZw9exaHDx9GWFgYAODGjRtISkqCj4+P9JiSJUuiYcOGiIqKQs+ePREVFYVSpUpJBRMA+Pj4wMjICNHR0ejSpQuioqLQvHlznULQ19cXs2bNQmpqKkqXLp0vW2ZmJjIzM6XltLQ0AEB2djays7MVfR3yvt9PbaKCMFbu/lgqExU0Gg20Wu1rZ857vNLPXUnMqAxmVAYzKoMZlfEuZ9RnPL2LpjZt2mD+/PnSqSuVSoX09HRMmTJF8a9WGTduHNLS0lC1alUYGxsjNzcXX331FXr16gUASEpKAgDY2dnpPM7Ozk5al5SUBFtbW531JiYmsLGx0enj6uqab4y8dQUVTTNnzsS0adPytUdERMDCwuJVnu5LzWpXEUD+gvXVOQMf/ISEhATFjhJGRkYqMk5hYkZlMKMymFEZzKiMdzFjRkaG7L56F02hoaHw9fWFh4cHHj9+jE8//RRXrlxB2bJl8dNPP+k73Av9/PPPWLduHdavXy+dMgsKCoKjoyP8/PwU3Za+xo8fjxEjRkjLaWlpcHJyQps2bWBtba3otmJiYpCYmIixf8RDlHF9+QNkykq+juT143Do0CHUrl37tcbKzs5GZGQkWrdune+Gp4aCGZXBjMpgRmUwozLe5Yx5Z4rk0LtoqlChAs6ePYsNGzbg3LlzSE9Ph7+/P3r16qUzMVwJo0ePxrhx49CzZ08AQM2aNREXF4eZM2fCz88P9vb2AIDk5GQ4ODhIj0tOTkadOnUAAPb29khJSdEZNycnB3fv3pUeb29vj+TkZJ0+ect5fZ6lVquhVqvztZuamiq+wxkZPZmvn5kjIHJVio2bmSPw6NEjGBkZKZa5MJ6/0phRGcyoDGZUBjMq413MqM9YehdNwJPTW717936Vh+olIyNDKhjyGBsbS3N8XF1dYW9vj71790pFUlpaGqKjozF48GAAgJeXF+7du4dTp07B09MTALBv3z5otVo0bNhQ6jNx4kRkZ2dLL15kZCTc3d0LPDVHRERE7x69i6Y1a9a8cH2fPn1eOcyzPvjgA3z11VeoWLEiqlevjpiYGISFhaF///4AnsynCgoKwowZM+Dm5gZXV1dMmjQJjo6O6Ny5MwCgWrVqaNu2LQYMGIClS5ciOzsbQ4cORc+ePeHo6AgA+PTTTzFt2jT4+/tj7Nix+Ouvv7BgwQLMmzdPsedCREREbza9i6bAwECd5ezsbGRkZMDMzAwWFhaKFk2LFi3CpEmTMGTIEKSkpMDR0RFffPEFJk+eLPUZM2YMHj58iIEDB+LevXto2rQpdu3aBXNzc6nPunXrMHToUHh7e8PIyAjdunXDwoULpfUlS5ZEREQEAgIC4OnpibJly2Ly5Mm83QARERFJ9C6aUlNT87VduXIFgwcPxujRoxUJlcfKygrz58/H/Pnzn9tHpVIhJCQEISEhz+1jY2OD9evXv3BbtWrVwp9//vmqUYmIiOgt90rfPfcsNzc3fPPNN/mOQhERERG9LRQpmoAnk8Nv3bql1HBEREREBkXv03O//fabzrIQAomJifj222/RpEkTxYIRERERGRK9i6a8q9LyqFQqlCtXDu+//z5CQ0OVykVERERkUF7pu+eIiIiI3jWKzWkiIiIiepvpfaTp6e9be5mwsDB9hyciIiIySHoXTTExMYiJiUF2djbc3d0BAJcvX4axsTHq1asn9VOplPuONCIiIqLipnfR9MEHH8DKygqrV6+WvpctNTUV/fr1Q7NmzTBy5EjFQxIREREVN73nNIWGhmLmzJk6X2RbunRpzJgxg1fPERER0VtL76IpLS0Nt2/fztd++/ZtPHjwQJFQRERERIZG76KpS5cu6NevH7Zu3YqbN2/i5s2b2LJlC/z9/dG1a9fCyEhERERU7PSe07R06VKMGjUKn376KbKzs58MYmICf39/zJkzR/GARERERIZA76LJwsIC3333HebMmYNr164BACpVqgRLS0vFwxEREREZile+uWViYiISExPh5uYGS0tLCCGUzEVERERkUPQumu7cuQNvb29UqVIF7du3R2JiIgDA39+ftxsgIiKit5beRVNwcDBMTU0RHx8PCwsLqf3jjz/Grl27FA1HREREZCj0ntMUERGB3bt3o0KFCjrtbm5uiIuLUywYERERkSHR+0jTw4cPdY4w5bl79y7UarUioYiIiIgMjd5FU7NmzbBmzRppWaVSQavVYvbs2WjVqpWi4YiIiIgMhd6n52bPng1vb2+cPHkSWVlZGDNmDC5cuIC7d+/iyJEjhZGRiIiIqNjpfaSpRo0auHz5Mpo2bYpOnTrh4cOH6Nq1K2JiYlCpUqXCyEhERERU7PQ60pSdnY22bdti6dKlmDhxYmFlIiIiIjI4eh1pMjU1xblz5worCxEREZHB0vv0XO/evbFixYrCyEJERERksPSeCJ6Tk4OVK1diz5498PT0zPedc2FhYYqFIyIiIjIUehdNf/31F+rVqwcAuHz5ss46lUqlTCoiIiIiAyO7aLp+/TpcXV2xf//+wsxDREREZJBkz2lyc3PD7du3peWPP/4YycnJhRKKiIiIyNDILpqEEDrLO3fuxMOHDxUPRERERGSI9L56joiIiOhdJLtoUqlU+SZ6c+I3ERERvStkTwQXQqBv375Qq9UAgMePH2PQoEH5bjmwdetWZRMSERERGQDZRZOfn5/Ocu/evRUPQ0RERGSoZBdN4eHhhZmDiIiIyKBxIjgRERGRDLKKpkGDBuHmzZuyBty4cSPWrVv3WqGIiIiIDI2s03PlypVD9erV0aRJE3zwwQeoX78+HB0dYW5ujtTUVFy8eBGHDx/Ghg0b4OjoiGXLlhV2biIiIqIiJatomj59OoYOHYoffvgB3333HS5evKiz3srKCj4+Pli2bBnatm1bKEGJiIiIipPsieB2dnaYOHEiJk6ciNTUVMTHx+PRo0coW7YsKlWqxHs2ERER0VtNdtH0tNKlS6N06dJKZyEiIiIyWLx6joiIiEgGFk1EREREMrBoIiIiIpKBRRMRERGRDHoXTY8ePUJGRoa0HBcXh/nz5yMiIkLRYERERESGRO+iqVOnTlizZg0A4N69e2jYsCFCQ0PRqVMnLFmyRPGARERERIZA76Lp9OnTaNasGQBg8+bNsLOzQ1xcHNasWYOFCxcqHpCIiIjIEOhdNGVkZMDKygoAEBERga5du8LIyAiNGjVCXFyc4gGJiIiIDIHeRVPlypWxbds2/Pvvv9i9ezfatGkDAEhJSYG1tbXiAYmIiIgMgd5F0+TJkzFq1Ci4uLigYcOG8PLyAvDkqFPdunUVD0hERERkCPT+GpWPPvoITZs2RWJiImrXri21e3t7o0uXLoqGIyIiIjIUr/Tdc/b29rC3t9dpa9CggSKBiIiIiAyRrKKpa9eusgfcunXrK4cpSEJCAsaOHYs//vgDGRkZqFy5MsLDw1G/fn0AgBACU6ZMwfLly3Hv3j00adIES5YsgZubmzTG3bt3MWzYMPz+++8wMjJCt27dsGDBApQoUULqc+7cOQQEBODEiRMoV64chg0bhjFjxij6XIiIiOjNJWtOU8mSJaUfa2tr7N27FydPnpTWnzp1Cnv37kXJkiUVDZeamoomTZrA1NQUf/zxBy5evIjQ0FCULl1a6jN79mwsXLgQS5cuRXR0NCwtLeHr64vHjx9LfXr16oULFy4gMjIS27dvx6FDhzBw4EBpfVpaGtq0aQNnZ2ecOnUKc+bMwdSpU7Fs2TJFnw8RERG9uWQdaQoPD5f+f+zYsejRoweWLl0KY2NjAEBubi6GDBmi+NVzs2bNgpOTk872XV1dpf8XQmD+/Pn48ssv0alTJwDAmjVrYGdnh23btqFnz56IjY3Frl27cOLECeno1KJFi9C+fXvMnTsXjo6OWLduHbKysrBy5UqYmZmhevXqOHPmDMLCwnSKKyIiInp36T2naeXKlTh8+LBUMAGAsbExRowYgcaNG2POnDmKhfvtt9/g6+uL7t274+DBgyhfvjyGDBmCAQMGAABu3LiBpKQk+Pj4SI8pWbIkGjZsiKioKPTs2RNRUVEoVaqUVDABgI+PD4yMjBAdHY0uXbogKioKzZs3h5mZmdTH19cXs2bNQmpqqs6RrTyZmZnIzMyUltPS0gAA2dnZyM7OVuw1AACtVgsAUJuoIIyFYuOqTFTQaDTQarWvnTnv8Uo/dyUxozKYURnMqAxmVMa7nFGf8fQumnJycvD333/D3d1dp/3vv/+W/nFXyvXr17FkyRKMGDECEyZMwIkTJzB8+HCYmZnBz88PSUlJAAA7Ozudx9nZ2UnrkpKSYGtrq7PexMQENjY2On2ePoL19JhJSUkFFk0zZ87EtGnT8rVHRETAwsLiFZ/xi81qVxFAroIjOgMf/ISEhAQkJCQoMmJkZKQi4xQmZlQGMyqDGZXBjMp4FzM+/X26L6N30dSvXz/4+/vj2rVr0hVz0dHR+Oabb9CvXz99h3shrVaL+vXr4+uvvwYA1K1bF3/99ReWLl0KPz8/Rbelr/Hjx2PEiBHSclpaGpycnNCmTRvFT1PGxMQgMTERY/+Ihyjj+vIHyJSVfB3J68fh0KFDOrePeBXZ2dmIjIxE69atYWpqqlBCZTGjMphRGcyoDGZUxrucMe9MkRx6F01z586Fvb09QkNDkZiYCABwcHDA6NGjMXLkSH2HeyEHBwd4eHjotFWrVg1btmwBAOm2B8nJyXBwcJD6JCcno06dOlKflJQUnTFycnJw9+5d6fH29vZITk7W6ZO3/OytFfKo1Wqo1ep87aamporvcEZGT+brZ+YIiFyVYuNm5gg8evQIRkZGimUujOevNGZUBjMqgxmVwYzKeBcz6jOWXncEz8nJwY8//gg/Pz8kJCTg3r17uHfvHhISEjBmzBideU5KaNKkCS5duqTTdvnyZTg7OwN4Minc3t4ee/fuldanpaUhOjpaulO5l5cX7t27h1OnTkl99u3bB61Wi4YNG0p9Dh06pHNeMzIyEu7u7gWemiMiIqJ3j15Fk4mJCQYNGiRdzm9tbV2o3zcXHByMY8eO4euvv8bVq1exfv16LFu2DAEBAQAAlUqFoKAgzJgxA7/99hvOnz+PPn36wNHREZ07dwbw5MhU27ZtMWDAABw/fhxHjhzB0KFD0bNnTzg6OgIAPv30U5iZmcHf3x8XLlzAxo0bsWDBAp3Tb0RERPRu0/v0XIMGDRATEyMd7SlM//vf//DLL79g/PjxCAkJgaurK+bPn49evXpJfcaMGYOHDx9i4MCBuHfvHpo2bYpdu3bB3Nxc6rNu3ToMHToU3t7e0s0tFy5cKK0vWbIkIiIiEBAQAE9PT5QtWxaTJ0/m7QaIiIhIonfRNGTIEIwcORI3b96Ep6cnLC0tddbXqlVLsXAA0LFjR3Ts2PG561UqFUJCQhASEvLcPjY2Nli/fv0Lt1OrVi38+eefr5yTiIiI3m56F009e/YEAAwfPlxqU6lUEEJApVIhN1fJS+KJiIiIDIPeRdONGzcKIwcRERGRQdO7aCqKuUxEREREhkbvogkArl27hvnz5yM2NhYA4OHhgcDAQFSqVEnRcERERESGQq9bDgDA7t274eHhgePHj6NWrVqoVasWoqOjUb169Tfi9utEREREr0LvI03jxo1DcHAwvvnmm3ztY8eORevWrRULR0RERGQo9D7SFBsbC39//3zt/fv3x8WLFxUJRURERGRo9C6aypUrhzNnzuRrP3PmDGxtbZXIRERERGRw9D49N2DAAAwcOBDXr19H48aNAQBHjhzBrFmz+LUjRERE9NbSu2iaNGkSrKysEBoaivHjxwMAHB0dMXXqVJ0bXhIRERG9TfQumlQqFYKDgxEcHIwHDx4AAKysrBQPRkRERGRIXumO4Dk5OXBzc9Mplq5cuQJTU1O4uLgomY+IiIjIIOg9Ebxv3744evRovvbo6Gj07dtXiUxEREREBkfvoikmJgZNmjTJ196oUaMCr6ojIiIiehvoXTSpVCppLtPT7t+/j9zcXEVCERERERkavYum5s2bY+bMmToFUm5uLmbOnImmTZsqGo6IiIjIUOg9EXzWrFlo3rw53N3d0axZMwDAn3/+ibS0NOzbt0/xgERERESGQO8jTR4eHjh37hx69OiBlJQUPHjwAH369MHff/+NGjVqFEZGIiIiomKn95Em4MnNLL/++mulsxAREREZLL2PNAFPTsf17t0bjRs3RkJCAgBg7dq1OHz4sKLhiIiIiAyF3kXTli1b4OvrC41Gg9OnTyMzMxPAk6vnePSJiIiI3lZ6F00zZszA0qVLsXz5cpiamkrtTZo0wenTpxUNR0RERGQo9C6aLl26hObNm+drL1myJO7du6dEJiIiIiKDo3fRZG9vj6tXr+ZrP3z4MN577z1FQhEREREZGr2LpgEDBiAwMBDR0dFQqVS4desW1q1bh1GjRmHw4MGFkZGIiIio2Ol9y4Fx48ZBq9XC29sbGRkZaN68OdRqNUaNGoVhw4YVRkYiIiKiYqd30aRSqTBx4kSMHj0aV69eRXp6Ojw8PFCiRAk8evQIGo2mMHISERERFatXuk8TAJiZmcHDwwMNGjSAqakpwsLC4OrqqmQ2IiIiIoMhu2jKzMzE+PHjUb9+fTRu3Bjbtm0DAISHh8PV1RXz5s1DcHBwYeUkIiIiKlayT89NnjwZ33//PXx8fHD06FF0794d/fr1w7FjxxAWFobu3bvD2Ni4MLMSERERFRvZRdOmTZuwZs0afPjhh/jrr79Qq1Yt5OTk4OzZs1CpVIWZkYiIiKjYyT49d/PmTXh6egIAatSoAbVajeDgYBZMRERE9E6QXTTl5ubCzMxMWjYxMUGJEiUKJRQRERGRoZF9ek4Igb59+0KtVgMAHj9+jEGDBsHS0lKn39atW5VNSERERGQAZBdNfn5+Osu9e/dWPAwRERGRoZJdNIWHhxdmDiIiIiKD9so3tyQiIiJ6l7BoIiIiIpKBRRMRERGRDCyaiIiIiGSQVTTVq1cPqampAICQkBBkZGQUaigiIiIiQyOraIqNjcXDhw8BANOmTUN6enqhhiIiIiIyNLJuOVCnTh3069cPTZs2hRACc+fOfe7dwCdPnqxoQCIiIiJDIKtoWrVqFaZMmYLt27dDpVLhjz/+gIlJ/oeqVCoWTURERPRWklU0ubu7Y8OGDQAAIyMj7N27F7a2toUajIiIiMiQyL4jeB6tVlsYOYiIiIgMmt5FEwBcu3YN8+fPR2xsLADAw8MDgYGBqFSpkqLhiIiIiAyF3vdp2r17Nzw8PHD8+HHUqlULtWrVQnR0NKpXr47IyMjCyEhERERU7PQ+0jRu3DgEBwfjm2++ydc+duxYtG7dWrFwRERERIZC7yNNsbGx8Pf3z9fev39/XLx4UZFQRERERIZG76KpXLlyOHPmTL72M2fO8Io6IiIiemvpfXpuwIABGDhwIK5fv47GjRsDAI4cOYJZs2ZhxIgRigckIiIiMgR6F02TJk2ClZUVQkNDMX78eACAo6Mjpk6diuHDhysekIiIiMgQ6F00qVQqBAcHIzg4GA8ePAAAWFlZKR6MiIiIyJDoPafpaVZWVkVaMH3zzTdQqVQICgqS2h4/foyAgACUKVMGJUqUQLdu3ZCcnKzzuPj4eHTo0AEWFhawtbXF6NGjkZOTo9PnwIEDqFevHtRqNSpXroxVq1YVwTMiIiKiN8VrFU1F6cSJE/j+++9Rq1Ytnfbg4GD8/vvv2LRpEw4ePIhbt26ha9eu0vrc3Fx06NABWVlZOHr0KFavXo1Vq1bpfEfejRs30KFDB7Rq1QpnzpxBUFAQPv/8c+zevbvInh8REREZtjeiaEpPT0evXr2wfPlylC5dWmq/f/8+VqxYgbCwMLz//vvw9PREeHg4jh49imPHjgEAIiIicPHiRfz444+oU6cO2rVrh+nTp2Px4sXIysoCACxduhSurq4IDQ1FtWrVMHToUHz00UeYN29esTxfIiIiMjyv9DUqRS0gIAAdOnSAj48PZsyYIbWfOnUK2dnZ8PHxkdqqVq2KihUrIioqCo0aNUJUVBRq1qwJOzs7qY+vry8GDx6MCxcuoG7duoiKitIZI6/P06cBn5WZmYnMzExpOS0tDQCQnZ2N7Ozs133KOvK+709tooIwFoqNqzJRQaPRQKvVvnbmvMcr/dyVxIzKYEZlMKMymFEZ73JGfcbTq2jKzs5G27ZtsXTpUri5uekd7FVs2LABp0+fxokTJ/KtS0pKgpmZGUqVKqXTbmdnh6SkJKnP0wVT3vq8dS/qk5aWhkePHkGj0eTb9syZMzFt2rR87REREbCwsJD/BPUwq11FALkKjugMfPATEhISkJCQoMiIb8JX6TCjMphRGcyoDGZUxruYMSMjQ3ZfvYomU1NTnDt3Tu9Ar+rff/9FYGAgIiMjYW5uXmTblWP8+PE696VKS0uDk5MT2rRpA2tra0W3FRMTg8TERIz9Ix6ijKti42YlX0fy+nE4dOgQateu/VpjZWdnIzIyEq1bt4apqalCCZXFjMpgRmUwozKYURnvcsa8M0Vy6H16rnfv3lixYkW+754rDKdOnUJKSgrq1asnteXm5uLQoUP49ttvsXv3bmRlZeHevXs6R5uSk5Nhb28PALC3t8fx48d1xs27uu7pPs9ecZecnAxra+sCjzIBgFqthlqtztduamqq+A5nZPRk6llmjoDIVSk2bmaOwKNHj2BkZKRY5sJ4/kpjRmUwozKYURnMqIx3MaM+Y+ldNOXk5GDlypXYs2cPPD09YWlpqbM+LCxM3yGfy9vbG+fPn9dp69evH6pWrYqxY8fCyckJpqam2Lt3L7p16wYAuHTpEuLj4+Hl5QUA8PLywldffYWUlBTpa14iIyNhbW0NDw8Pqc/OnTt1thMZGSmNQURERKR30fTXX39JR34uX76ss06lUu4oCPDkPlA1atTQabO0tESZMmWkdn9/f4wYMQI2NjawtrbGsGHD4OXlhUaNGgEA2rRpAw8PD3z22WeYPXs2kpKS8OWXXyIgIEA6UjRo0CB8++23GDNmDPr37499+/bh559/xo4dOxR9PkRERPTm0rto2r9/f2HkeGXz5s2DkZERunXrhszMTPj6+uK7776T1hsbG2P79u0YPHgwvLy8YGlpCT8/P4SEhEh9XF1dsWPHDgQHB2PBggWoUKECfvjhB/j6+hbHUyIiIiID9Mq3HLh69SquXbuG5s2bQ6PRQAih+JGmghw4cEBn2dzcHIsXL8bixYuf+xhnZ+d8p9+e1bJlS8TExCgRkYiIiN5Cet/c8s6dO/D29kaVKlXQvn17JCYmAnhymmzkyJGKByQiIiIyBHoXTcHBwTA1NUV8fLzO/Yg+/vhj7Nq1S9FwRERERIZC79NzERER2L17NypUqKDT7ubmhri4OMWCERERERkSvY80PXz4sMA7Xt+9e7fA+xYRERERvQ30LpqaNWuGNWvWSMsqlQparRazZ89Gq1atFA1HREREZCj0Pj03e/ZseHt74+TJk8jKysKYMWNw4cIF3L17F0eOHCmMjERERETFTu8jTTVq1MDly5fRtGlTdOrUCQ8fPkTXrl0RExODSpUqFUZGIiIiomL3SvdpKlmyJCZOnKh0FiIiIiKD9UpFU2pqKlasWIHY2FgAgIeHB/r16wcbGxtFwxEREREZCr1Pzx06dAguLi5YuHAhUlNTkZqaioULF8LV1RWHDh0qjIxERERExU7vI00BAQH4+OOPsWTJEhgbGwMAcnNzMWTIEAQEBOD8+fOKhyQiIiIqbnofabp69SpGjhwpFUzAky/FHTFiBK5evapoOCIiIiJDoXfRVK9ePWku09NiY2NRu3ZtRUIRERERGRpZp+fOnTsn/f/w4cMRGBiIq1evolGjRgCAY8eOYfHixfjmm28KJyURERFRMZNVNNWpUwcqlQpCCKltzJgx+fp9+umn+Pjjj5VLR0RERGQgZBVNN27cKOwcRERERAZNVtHk7Oxc2DmIiIiIDNor3dzy1q1bOHz4MFJSUqDVanXWDR8+XJFgRERERIZE76Jp1apV+OKLL2BmZoYyZcpApVJJ61QqFYsmIiIieivpXTRNmjQJkydPxvjx42FkpPcdC4iIiIjeSHpXPRkZGejZsycLJiIiInqn6F35+Pv7Y9OmTYWRhYiIiMhg6X16bubMmejYsSN27dqFmjVrwtTUVGd9WFiYYuGIiIiIDMUrFU27d++Gu7s7AOSbCE5ERET0NtK7aAoNDcXKlSvRt2/fQohDREREZJj0ntOkVqvRpEmTwshCREREZLD0LpoCAwOxaNGiwshCREREZLD0Pj13/Phx7Nu3D9u3b0f16tXzTQTfunWrYuGIiIiIDIXeRVOpUqXQtWvXwshCREREZLD0LprCw8MLIwcRERGRQeNtvYmIiIhk0PtIk6ur6wvvx3T9+vXXCkRERERkiPQumoKCgnSWs7OzERMTg127dmH06NFK5SIiIiIyKHoXTYGBgQW2L168GCdPnnztQERERESGSLE5Te3atcOWLVuUGo6IiIjIoChWNG3evBk2NjZKDUdERERkUPQ+PVe3bl2dieBCCCQlJeH27dv47rvvFA1HREREZCj0Lpo6d+6ss2xkZIRy5cqhZcuWqFq1qlK5iIiIiAyK3kXTlClTCiMHERERkUHjzS2JiIiIZJB9pMnIyOiFN7UEAJVKhZycnNcORURERGRoZBdNv/zyy3PXRUVFYeHChdBqtYqEIiIiIjI0soumTp065Wu7dOkSxo0bh99//x29evVCSEiIouGIiIiIDMUrzWm6desWBgwYgJo1ayInJwdnzpzB6tWr4ezsrHQ+IiIiIoOgV9F0//59jB07FpUrV8aFCxewd+9e/P7776hRo0Zh5SMiIiIyCLJPz82ePRuzZs2Cvb09fvrppwJP1xERERG9rWQXTePGjYNGo0HlypWxevVqrF69usB+W7duVSwcERERkaGQXTT16dPnpbccICIiInpbyS6aVq1aVYgxiIiIiAwb7whOREREJAOLJiIiIiIZWDQRERERycCiiYiIiEgGFk1EREREMhh00TRz5kz873//g5WVFWxtbdG5c2dcunRJp8/jx48REBCAMmXKoESJEujWrRuSk5N1+sTHx6NDhw6wsLCAra0tRo8ejZycHJ0+Bw4cQL169aBWq1G5cmVeLUhEREQ6DLpoOnjwIAICAnDs2DFERkYiOzsbbdq0wcOHD6U+wcHB+P3337Fp0yYcPHgQt27dQteuXaX1ubm56NChA7KysnD06FGsXr0aq1atwuTJk6U+N27cQIcOHdCqVSucOXMGQUFB+Pzzz7F79+4ifb5ERERkuGTfp6k47Nq1S2d51apVsLW1xalTp9C8eXPcv38fK1aswPr16/H+++8DAMLDw1GtWjUcO3YMjRo1QkREBC5evIg9e/bAzs4OderUwfTp0zF27FhMnToVZmZmWLp0KVxdXREaGgoAqFatGg4fPox58+bB19e3yJ83ERERGR6DLpqedf/+fQCAjY0NAODUqVPIzs6Gj4+P1Kdq1aqoWLEioqKi0KhRI0RFRaFmzZqws7OT+vj6+mLw4MG4cOEC6tati6ioKJ0x8voEBQU9N0tmZiYyMzOl5bS0NABAdnY2srOzX/u5Pk2r1QIA1CYqCGOh2LgqExU0Gg20Wu1rZ857vNLPXUnMqAxmVAYzKoMZlfEuZ9RnvDemaNJqtQgKCkKTJk1Qo0YNAEBSUhLMzMxQqlQpnb52dnZISkqS+jxdMOWtz1v3oj5paWl49OgRNBpNvjwzZ87EtGnT8rVHRETAwsLi1Z7kS8xqVxFAroIjOgMf/ISEhAQkJCQoMmJkZKQi4xQmZlQGMyqDGZXBjMp4FzNmZGTI7vvGFE0BAQH466+/cPjw4eKOAgAYP348RowYIS2npaXByckJbdq0gbW1taLbiomJQWJiIsb+EQ9RxlWxcbOSryN5/TgcOnQItWvXfq2xsrOzERkZidatW8PU1FShhMpiRmUwozKYURnMqIx3OWPemSI53oiiaejQodi+fTsOHTqEChUqSO329vbIysrCvXv3dI42JScnw97eXupz/PhxnfHyrq57us+zV9wlJyfD2tq6wKNMAKBWq6FWq/O1m5qaKr7DGRk9ma+fmSMgcpX70uTMHIFHjx7ByMhIscyF8fyVxozKYEZlMKMymFEZ72JGfcYy6KvnhBAYOnQofvnlF+zbtw+urrpHWTw9PWFqaoq9e/dKbZcuXUJ8fDy8vLwAAF5eXjh//jxSUlKkPpGRkbC2toaHh4fU5+kx8vrkjUFERERk0EeaAgICsH79evz666+wsrKS5iCVLFkSGo0GJUuWhL+/P0aMGAEbGxtYW1tj2LBh8PLyQqNGjQAAbdq0gYeHBz777DPMnj0bSUlJ+PLLLxEQECAdKRo0aBC+/fZbjBkzBv3798e+ffvw888/Y8eOHcX23ImIiMiwGPSRpiVLluD+/fto2bIlHBwcpJ+NGzdKfebNm4eOHTuiW7duaN68Oezt7bF161ZpvbGxMbZv3w5jY2N4eXmhd+/e6NOnD0JCQqQ+rq6u2LFjByIjI1G7dm2Ehobihx9+4O0GiIiISGLQR5qEePnl9ebm5li8eDEWL1783D7Ozs7YuXPnC8dp2bIlYmJi9M5IRERE7waDPtJEREREZChYNBERERHJwKKJiIiISAYWTUREREQysGgiIiIikoFFExEREZEMLJqIiIiIZGDRRERERCQDiyYiIiIiGVg0EREREcnAoomIiIhIBhZNRERERDKwaCIiIiKSgUUTERERkQwsmoiIiIhkYNFEREREJAOLJiIiIiIZWDQRERERycCiiYiIiEgGFk1EREREMrBoIiIiIpKBRRMRERGRDCyaiIiIiGRg0UREREQkA4smIiIiIhlYNBERERHJwKKJiIiISAYWTUREREQysGgiIiIikoFFExEREZEMLJqIiIiIZGDRRERERCQDiyYiIiIiGVg0EREREcnAoomIiIhIBhZNRERERDKwaCIiIiKSgUUTERERkQwsmoiIiIhkYNFEREREJAOLJiIiIiIZWDQRERERycCiiYiIiEgGFk1EREREMrBoIiIiIpKBRRMRERGRDCyaiIiIiGRg0UREREQkA4smIiIiIhlYNBERERHJYFLcAaj4xcbGvvYYWq0WAHD27FkYGT2pxcuWLYuKFSu+9thERESGgEXTOyw3PRVQqdC7d+/XHkuj0eCnn35C8+bN8ejRIwCAucYCl/6OZeFERERvBRZN7zBtZjogBMp0HAnTMk6vNZa5iQoAYPfpN3icI5B951/c2R6K//77j0UTERG9FVg0PWPx4sWYM2cOkpKSULt2bSxatAgNGjQo7liFyrSME9T2lV9rDDNjASAXZnbvQeSqlAlGRERkQDgR/CkbN27EiBEjMGXKFJw+fRq1a9eGr68vUlJSijsaERERFTMeaXpKWFgYBgwYgH79+gEAli5dih07dmDlypUYN25cMaejN1l8fDz++++/l/YraEL9y3DCPRFR0WDR9P9lZWXh1KlTGD9+vNRmZGQEHx8fREVFFWMyepbcAuRZcgqSwihA4uPj4V61Gh4/ynhp34Im1L9MYU64L+i1fpXC7lmFWejFx8dLR4dfJ+OzWJwWDX0/3/rsj3wP6XWxaPr//vvvP+Tm5sLOzk6n3c7ODn///Xe+/pmZmcjMzJSW79+/DwC4e/cusrOzFc2WlpaGjIwMqO7GQZv1WLFxjR4kwtzcHKo7NyC0mS9/wAtoTYCMDCdoE/+FyAFUqbdgbm6OU6dOIS0tTaHEQEpKCgZ+MQiZj+UVFE/TaDRYvHgx2rRp89yCRG2uwbLvl8LW1vZ1o0quXLkCCC3KNekBY6syL+xrbmqMjIwM2Pn443F27kvHzn1wBw9O/Ybdu3fDzc1NqcgAnv9ay3kdX6YwXmfg/zIbqfDaGZ+ldGatVouMjAz8+eefMDExkf7xV5KRkdFrjft0xmcLktcduyCv8vnWZ38srP3uZa/Fi17H1x37dTw99utkfNG4SsrLeOfOHZiamio27oMHDwAAQoiXdxYkhBAiISFBABBHjx7VaR89erRo0KBBvv5TpkwRAPjDH/7whz/84c9b8PPvv/++tFbgkab/r2zZsjA2NkZycrJOe3JyMuzt7fP1Hz9+PEaMGCEta7Va3L17F2XKlIFKpezVY2lpaXBycsK///4La2trRcdWCjMqgxmVwYzKYEZlMKMyCiujEAIPHjyAo6PjS/uyaPr/zMzM4Onpib1796Jz584AnhRCe/fuxdChQ/P1V6vVUKvVOm2lSpUq1IzW1tYGuzPnYUZlMKMymFEZzKgMZlRGYWQsWbKkrH4smp4yYsQI+Pn5oX79+mjQoAHmz5+Phw8fSlfTERER0buLRdNTPv74Y9y+fRuTJ09GUlIS6tSpg127duWbHE5ERETvHhZNzxg6dGiBp+OKk1qtxpQpU/KdDjQkzKgMZlQGMyqDGZXBjMowhIwqIeRcY0dERET0buPXqBARERHJwKKJiIiISAYWTUREREQysGh6A7Vs2RJBQUHFHeONJoTAwIEDYWNjA5VKhTNnzhR3pDca98m3W9++faX719HbiZ9heVg0vYG2bt2K6dOnS8suLi6YP39+8QV6A+3atQurVq3C9u3bkZiYiBo1ahR3JCoGb9Jn58CBA1CpVLh3716Rb3vBggVYtWpVkW+XyNDwlgNvIBsbm+KO8Ma7du0aHBwc0Lhx4+f2ycrKgpmZWRGmIjJMcu+WbEgM/fNr6PneJtnZ2Yp9wS+PNL2Bnj6M2rJlS8TFxSE4OBgqlUrx772TS6vVYubMmXB1dYVGo0Ht2rWxefNmaX3eX8l79+5F/fr1YWFhgcaNG+PSpUtFnrVv374YNmwY4uPjoVKp4OLiAuDJazl06FAEBQWhbNmy8PX1LfJswMtfy9TUVPTq1QvlypWDRqOBm5sbwsPDiyzfw4cP0adPH5QoUQIODg4IDQ3N1yczMxOjRo1C+fLlYWlpiYYNG+LAgQNFlhH4v/dz6NChKFmyJMqWLYtJkyZJ32RenJ+dli1bYtiwYQgKCkLp0qVhZ2eH5cuXS99AYGVlhcqVK+OPP/4AAPzzzz9o1aoVAKB06dJQqVTo27dvkeV99vTcrl270LRpU5QqVQplypRBx44dce3atSLLU5DnfX4PHjyIBg0aQK1Ww8HBAePGjUNOTo5B5Pvnn3/yTQ+4d+8eVCpVkX9eACAnJ+e5nxcASExMRIcOHaDRaODq6or169cX+dHal+17ea/pxo0b0aJFC5ibm2PdunWKbZ9F0xtu69atqFChAkJCQpCYmIjExMRiyTFz5kysWbMGS5cuxYULFxAcHIzevXvj4MGDOv0mTpyI0NBQnDx5EiYmJujfv3+RZ12wYAFCQkJQoUIFJCYm4sSJE9K61atXw8zMDEeOHMHSpUuLPBvw8tdy0qRJuHjxIv744w/ExsZiyZIlKFu2bJHlGz16NA4ePIhff/0VEREROHDgAE6fPq3TZ+jQoYiKisKGDRtw7tw5dO/eHW3btsWVK1eKLCfw5P00MTHB8ePHsWDBAoSFheGHH34AUPyfndWrV6Ns2bI4fvw4hg0bhsGDB6N79+5o3LgxTp8+jTZt2uCzzz5DRkYGnJycsGXLFgDApUuXkJiYiAULFhRp3qc9fPgQI0aMwMmTJ7F3714YGRmhS5cu0Gq1xZYJyP/5TUhIQPv27fG///0PZ8+exZIlS7BixQrMmDHDIPIZmhd9XgCgT58+uHXrFg4cOIAtW7Zg2bJlSElJKdKMcve9cePGITAwELGxscr+ASzojdOiRQsRGBgoLTs7O4t58+YVW57Hjx8LCwsLcfToUZ12f39/8cknnwghhNi/f78AIPbs2SOt37FjhwAgHj16VKR5hRBi3rx5wtnZWaetRYsWom7dukWe5WlyXssPPvhA9OvXrzjiiQcPHggzMzPx888/S2137twRGo1G2ifj4uKEsbGxSEhI0Hmst7e3GD9+fJFlbdGihahWrZrQarVS29ixY0W1atWk5eL67LRo0UI0bdpUWs7JyRGWlpbis88+k9oSExMFABEVFSWE+L/PUGpqalHHFX5+fqJTp07PXX/79m0BQJw/f77oQj2joM/vhAkThLu7u84+sHjxYlGiRAmRm5tb7Plu3LghAIiYmBipLTU1VQAQ+/fvL/J8L/q8xMbGCgDixIkT0vorV64IAMX678+z+17eazp//vxC2R6PNNFru3r1KjIyMtC6dWuUKFFC+lmzZk2+Q/a1atWS/t/BwQEAivwvlRfx9PQs1u3LeS0HDx6MDRs2oE6dOhgzZgyOHj1aZPmuXbuGrKwsNGzYUGqzsbGBu7u7tHz+/Hnk5uaiSpUqOs/h4MGDRX4Kp1GjRjqn3by8vHDlyhXk5uYWaY6CPP1ZMDY2RpkyZVCzZk2pLe87Lw3p85HnypUr+OSTT/Dee+/B2tpaOsUdHx9frLme/fzGxsbCy8tLZx9o0qQJ0tPTcfPmzaKOV+y/X17mRZ+XS5cuwcTEBPXq1ZPWV65cGaVLly7SjHL3vfr16xfK9jkRnF5beno6AGDHjh0oX768zrpnvyPo6cl4eR/O4j6k/zRLS8ti3b6c17Jdu3aIi4vDzp07ERkZCW9vbwQEBGDu3LlFnrcg6enpMDY2xqlTp2BsbKyzrkSJEsWUyvA8OzFVpVIZ/OcjzwcffABnZ2csX74cjo6O0Gq1qFGjBrKysoo1V3F/fl/m2XxGRk+OW4in5g1lZ2cXaaY3jdx9r7D2BRZNbwEzM7Ni/cvZw8MDarUa8fHxaNGiRbHleBvIfS3LlSsHPz8/+Pn5oVmzZhg9enSRFE2VKlWCqakpoqOjUbFiRQBPJqZfvnxZylu3bl3k5uYiJSUFzZo1K/RMLxIdHa2zfOzYMbi5uUnFXHF/dvSRd6VVcee9c+cOLl26hOXLl0vv7+HDh4s10/NUq1YNW7ZsgRBCKkKPHDkCKysrVKhQoZjTPfkcA08mWNetWxcAivWecS/6vLi7uyMnJwcxMTHSEbOrV68iNTW1yPIZwr7Houkt4OLigkOHDqFnz55Qq9VFOikYAKysrDBq1CgEBwdDq9WiadOmuH//Po4cOQJra2v4+fkVaZ43mZzXcvLkyfD09ET16tWRmZmJ7du3o1q1akWSr0SJEvD398fo0aNRpkwZ2NraYuLEidJfzABQpUoV9OrVC3369EFoaCjq1q2L27dvY+/evahVqxY6dOhQJFmBJ4fsR4wYgS+++AKnT5/GokWLdK72K+7Pjj6cnZ2hUqmwfft2tG/fHhqNpliO3JUuXRplypTBsmXL4ODggPj4eIwbN67Ic8gxZMgQzJ8/H8OGDcPQoUNx6dIlTJkyBSNGjNDZZ4uLRqNBo0aN8M0338DV1RUpKSn48ssviy3Piz4vVatWhY+PDwYOHIglS5bA1NQUI0eOhEajKbIrTw1h32PR9BYICQnBF198gUqVKiEzM1PnUG9RmT59OsqVK4eZM2fi+vXrKFWqFOrVq4cJEyYUeZY33cteSzMzM4wfPx7//PMPNBoNmjVrhg0bNhRZvjlz5iA9PR0ffPABrKysMHLkSNy/f1+nT3h4OGbMmIGRI0ciISEBZcuWRaNGjdCxY8ciywk8udrn0aNHaNCgAYyNjREYGIiBAwdK6w3hsyNX+fLlMW3aNIwbNw79+vVDnz59iuWGk0ZGRtiwYQOGDx+OGjVqwN3dHQsXLkTLli2LPMvLlC9fHjt37sTo0aNRu3Zt2NjYwN/fv1gLk2etXLkS/v7+8PT0hLu7O2bPno02bdoUS5aXfV7WrFkDf39/NG/eHPb29pg5cyYuXLgAc3PzIslnCPueShjybwkiolfUsmVL1KlT542547ch++STT2BsbIwff/yxuKOQAbl58yacnJywZ88eeHt7F3ecIsEjTUREVKCcnBxcvnwZUVFR+OKLL4o7DhWzffv2IT09HTVr1kRiYiLGjBkDFxcXNG/evLijFRkWTcUkPj4eHh4e+dozMjJgYWEh/fdZFy9elCbgvguZXsSQ8xpytjchX57XyWnI2d6kz0tGRgbatWuHQYMGGWxGfm6KJl92djYmTJiA69evw8rKCo0bN8a6dev0/ooSQ38dX4Sn54pJTk4O/vnnH70f5+LiAhOTwql1DTHTixhyXkPOBhh+vjyGnNOQsxXkTchr6BmZTxlvSs6CsGgiIiIikqH4r7kkIiIiegOwaCIiIiKSgUUTERERkQwsmoio2PXt2xedO3eWllu2bImgoKAiz3HgwAGoVCrcu3evyLdNRIaPRRMRFahv375QqVRQqVQwMzND5cqVERISgpycnELf9tatWzF9+nRZfYu60HFxceENM4neUbxPExE9V9u2bREeHo7MzEzs3LkTAQEBMDU1xfjx4/P1zcrKkr5U9nXZ2NgoMg4RkZJ4pImInkutVsPe3h7Ozs4YPHgwfHx88NtvvwH4v1NqX331FRwdHeHu7g4A+Pfff9GjRw+UKlUKNjY26NSpk849WXJzczFixAiUKlUKZcqUwZgxY/J959uzp+cyMzMxduxYODk5Qa1Wo3LlylixYgX++ecftGrVCsCTL/NUqVTo27cvAECr1WLmzJlwdXWFRqNB7dq1sXnzZp3t7Ny5E1WqVIFGo0GrVq1e6d4xT8vNzYW/v7+0TXd3dyxYsECnT97rNnfuXDg4OKBMmTIICAhAdna21CcxMREdOnSARqOBq6sr1q9fr3OE659//oFKpcKZM2ekx9y7dw8qlQoHDhyQnSUnJwfDhw+X3ouxY8fCz89P51SpnNeR6F3BI01EJJtGo8GdO3ek5b1798La2hqRkZEAntwx2NfXF15eXvjzzz9hYmKCGTNmoG3btjh37hzMzMwQGhqKVatWYeXKlahWrRpCQ0Pxyy+/4P3333/udvv06YOoqCgsXLgQtWvXxo0bN/Dff//ByckJW7ZsQbdu3XDp0iVYW1tDo9EAAGbOnIkff/wRS5cuhZubGw4dOoTevXujXLlyaNGiBf7991907doVAQEBGDhwIE6ePImRI0e+1uuj1WpRoUIFbNq0CWXKlMHRo0cxcOBAODg4oEePHlK//fv3w8HBAfv378fVq1fx8ccfo06dOhgwYID0fP/77z8cOHAApqamGDFiBFJSUhTPMmvWLKxbtw7h4eGoVq0aFixYgG3btkmFqJzXkeidIoiICuDn5yc6deokhBBCq9WKyMhIoVarxahRo6T1dnZ2IjMzU3rM2rVrhbu7u9BqtVJbZmam0Gg0Yvfu3UIIIRwcHMTs2bOl9dnZ2aJChQrStoQQokWLFiIwMFAIIcSlS5cEABEZGVlgzv379wsAIjU1VWp7/PixsLCwEEePHtXp6+/vLz755BMhhBDjx48XHh4eOuvHjh2bb6xnOTs7i3nz5j13/bMCAgJEt27dpGU/Pz/h7OwscnJypLbu3buLjz/+WAghRGxsrAAgTpw4Ia2/cuWKACBt98aNGwKAiImJkfqkpqYKAGL//v2ys9jZ2Yk5c+ZIyzk5OaJixYrSeyHndSR6l/BIExE91/bt21GiRAlkZ2dDq9Xi008/xdSpU6X1NWvW1JnHdPbsWVy9ehVWVlY64zx+/BjXrl3D/fv3kZiYiIYNG0rrTExMUL9+/Xyn6PKcOXMGxsbGeh3VuHr1KjIyMtC6dWud9qysLNStWxcAEBsbq5MDALy8vGRv43kWL16MlStXIj4+Ho8ePUJWVhbq1Kmj06d69eowNjaWlh0cHHD+/HkAwKVLl2BiYoJ69epJ6ytXrozSpUsrmuX+/ftITk5GgwYNpP7Gxsbw9PSEVqsFIO91JHqXsGgioudq1aoVlixZAjMzMzg6Oub73idLS0ud5fT0dHh6emLdunX5xipXrtwrZcg73aaP9PR0AMCOHTtQvnx5nXVqtfqVcsixYcMGjBo1CqGhofDy8oKVlRXmzJmD6OhonX7PfsGpSqWSChU5jIyeTEd9utB8ek6UPllepLheRyJDxaKJiJ7L0tISlStXlt2/Xr162LhxI2xtbWFtbV1gHwcHB0RHR6N58+YAnkxGPnXqlM6RlafVrFkTWq0WBw8ehI+PT771eUe6cnNzpTYPDw+o1WrEx8c/9whVtWrVpEnteY4dO/byJ/kCR44cQePGjTFkyBCp7dq1a3qN4e7ujpycHMTExMDT0xPAkyM+qampUp+8AjQxMVE64vP0pHA5WUqWLAk7OzucOHFCei9yc3Nx+vRp6WiUnNeR6F3CoomIFNOrVy/MmTMHnTp1QkhICCpUqIC4uDhs3boVY8aMQYUKFRAYGIhvvvkGbm5uqFq1KsLCwl54jyUXFxf4+fmhf//+0kTwuLg4pKSkoEePHnB2doZKpcL27dvRvn17aDQaWFlZYdSoUQgODoZWq0XTpk1x//59HDlyBNbW1vDz88OgQYMQGhqK0aNH4/PPP8epU6ewatUqWc8zISEhX5Hi7OwMNzc3rFmzBrt374arqyvWrl2LEydOwNXVVfZrWLVqVfj4+GDgwIFYsmQJTE1NMXLkSGg0GqhUKgBPjr41atQI33zzDVxdXZGSkoIvv/xSZxw5WYYNG4aZM2eicuXKqFq1KhYtWoTU1FRpO3JeR6J3SnFPqiIiw/T0RHB91icmJoo+ffqIsmXLCrVaLd577z0xYMAAcf/+fSHEk4nfgYGBwtraWpQqVUqMGDFC9OnT57kTwYUQ4tGjRyI4OFg4ODgIMzMzUblyZbFy5UppfUhIiLC3txcqlUr4+fkJIZ5MXp8/f75wd3cXpqamoly5csLX11ccPHhQetzvv/8uKleuLNRqtWjWrJlYuXKlrIngAPL9rF27Vjx+/Fj07dtXlCxZUpQqVUoMHjxYjBs3TtSuXfuFr1tgYKBo0aKFtHzr1i3Rrl07oVarhbOzs1i/fr2wtbUVS5culfpcvHhReHl5CY1GI+rUqSMiIiJ0JoLLyZKdnS2GDh0qrK2tRenSpcXYsWNF9+7dRc+ePaU+cl5HoneFSojnzL4kIiKDcPPmTTg5OWHPnj3w9vYutO1otVpUq1YNPXr0kH1HdqJ3CU/PEREZmH379iE9PR01a9ZEYmIixowZAxcXF2nukVLi4uIQERGBFi1aIDMzE99++y1u3LiBTz/9VNHtEL0tWDQRERmY7OxsTJgwAdevX4eVlRUaN26MdevW5bvq7nUZGRlh1apVGDVqFIQQqFGjBvbs2YNq1aopuh2itwVPzxERERHJwO+eIyIiIpKBRRMRERGRDCyaiIiIiGRg0UREREQkA4smIiIiIhlYNBERERHJwKKJiIiISAYWTUREREQysGgiIiIikuH/AeZXYt0AcwZ5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "__it    10876\n",
      "__en      206\n",
      "__es       52\n",
      "__fr       11\n",
      "__de        5\n",
      "__pt        3\n",
      "__ro        3\n",
      "__mt        1\n",
      "__ja        1\n",
      "__ru        1\n",
      "__bg        1\n",
      "__ar        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming df_language is your DataFrame with the 'confidence' column\n",
    "\n",
    "# Import the necessary plotting library for display\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "collapse_map = {\n",
    "    '__it': '__it',\n",
    "    '__en': '__en',\n",
    "    '__es': '__es',\n",
    "    '__fr': '__fr',\n",
    "    '__de': '__de',\n",
    "    '__pt': '__pt',\n",
    "    '__ro': '__ro',\n",
    "    '__ru': '__ru',\n",
    "    '__bg': '__bg',\n",
    "    '__ar': '__ar',\n",
    "    '__ja': '__ja',\n",
    "    '__mt': '__mt',\n",
    "\n",
    "    # Map regional languages to parent languages:\n",
    "    '__nap': '__it',   # Neapolitan ‚Üí Italian\n",
    "    '__lmo': '__it',   # Lombard ‚Üí Italian\n",
    "    '__ca': '__es',    # Catalan ‚Üí Spanish (if you prefer otherwise, tell me)\n",
    "    '__li': '__de',    # Limburgish ‚Üí German family (closest parent)\n",
    "}\n",
    "\n",
    "df_language['language_main'] = df_language['most_probable_language'].map(collapse_map)\n",
    "df['language'] = df_language['language_main']\n",
    "\n",
    "df['language'].hist(\n",
    "    bins=20, # Number of bins (intervals) for the histogram\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Distribution of FastText predicted languages')\n",
    "plt.xlabel('Predicted Language')\n",
    "plt.ylabel('Number of Records (Frequency)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(df['language'].value_counts())\n",
    "# df['language'] = df_language['most_probable_language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows/values that contain 1 or fewer sentences is: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65069/1835569498.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df_1_sentence, linguistic_columns] = pd.NA\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the boolean mask\n",
    "df_1_sentence = df['n_sentences'] <= 1\n",
    "\n",
    "# 2. Count the rows being affected (Correct use of sum())\n",
    "rows_to_nullify = df_1_sentence.sum()\n",
    "print(f\"The number of rows/values that contain 1 or fewer sentences is: {rows_to_nullify}\")\n",
    "\n",
    "# 3. Define the columns to nullify\n",
    "linguistic_columns = ['language', 'swear_IT', 'swear_EN', \n",
    "                      'swear IT words', 'swear_EN_words', 'n_sentences', \n",
    "                      'n_tokens', 'tokens_per_sent', 'avg_token_per_clause', \n",
    "                      'explicit', 'lyrics']\n",
    "\n",
    "# 4. Apply the mask to the DataFrame df (This is the correct operation)\n",
    "df.loc[df_1_sentence, linguistic_columns] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dacee0b",
   "metadata": {},
   "source": [
    "## album, album_name, album id\n",
    "\n",
    "While the column album seems more reasonable and coherent, it contains multiple null values.\n",
    "Some album in \"album_name\" appear truncated and incomplete.\n",
    "\n",
    "We decided to keep the normalization for better readability and to have normalized occurrences.\n",
    "\n",
    "To create a new correct version of the column showing the album relative to every tracks we decided to do 3 major choices:\n",
    "\n",
    "    #Choice 1 (for null 'album'): Use 'album_name_norm',\n",
    "    \n",
    "    #Choice 2 (for Mismatch): Use 'album_norm',\n",
    "    \n",
    "    #Choice 3 (for Match): mantain 'album_norm' (the same with 'album_name_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "610103db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creazione di 'correct_album' ---\n",
      "Colonna 'correct_album' creata con successo.\n",
      "\n",
      "--- [FASE 3]: Assegnazione di 'id_album_final' (Algoritmo 1-a-1) ---\n",
      "Inizio processamento di 1884 album per assegnazione ID...\n",
      "Processamento ID terminato. Mappa 1-a-1 creata.\n",
      "\n",
      "--- VERIFICA FINALE ---\n",
      "Album con pi√π di 1 ID: 0\n",
      "ID con pi√π di 1 Album: 0\n",
      "\n",
      "--- Esempio di 10 righe pulite: ---\n",
      "                     correct_album   id_album id_album_final\n",
      "id                                                          \n",
      "TR109530                mia maesta  ALB874268      ALB178352\n",
      "TR983017                 classe 73  ALB100033      ALB100033\n",
      "TR939046                 petrichor  ALB864377      ALB864377\n",
      "TR961627                  adversus  ALB699010      ALB699010\n",
      "TR344645       effetto notte lalba  ALB303773      ALB886654\n",
      "TR919302              tutti a casa  ALB241207      ALB241207\n",
      "TR695912                mia maesta  ALB178352      ALB178352\n",
      "TR865197              trvppin boyz  ALB615760      ALB193343\n",
      "TR708095  allucinazione collettiva  ALB691734      ALB691734\n",
      "TR219103                    quorum  ALB504051      ALB504051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "print(\"--- Creazione di 'correct_album' ---\")\n",
    "\n",
    "# Applica la normalizzazione alle due colonne originali\n",
    "df['album_norm'] = normalize_series(df['album'])\n",
    "df['album_name_norm'] = normalize_series(df['album_name'])\n",
    "\n",
    "# Definisci le condizioni per la colonna 'correct_album'\n",
    "conditions = [\n",
    "    (df['album'].isnull()), # Priorit√† 1: Se 'album' √® nullo...\n",
    "    (df['album_norm'] != df['album_name_norm']), # Priorit√† 2: Se c'√® mismatch...\n",
    "    (df['album_norm'] == df['album_name_norm'])  # Priorit√† 3: Se c'√® match...\n",
    "]\n",
    "\n",
    "# Definisci le scelte corrispondenti\n",
    "choices = [\n",
    "    df['album_name_norm'], # ...usa 'album_name_norm'\n",
    "    df['album_norm'],      # ...usa 'album_norm'\n",
    "    df['album_norm']       # ...usa 'album_norm'\n",
    "]\n",
    "\n",
    "# Crea la colonna 'correct_album'\n",
    "df['correct_album'] = np.select(conditions, choices, default=np.nan)\n",
    "print(\"Colonna 'correct_album' creata con successo.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- [FASE 3]: Assegnazione di 'id_album_final' (Algoritmo 1-a-1) ---\")\n",
    "\n",
    "# --- 3.1: Preparazione ---\n",
    "\n",
    "# Ordina gli album per frequenza (d√† priorit√† agli album pi√π grandi)\n",
    "album_order = df['correct_album'].value_counts().index\n",
    "\n",
    "# Filtra i dati per creare la mappa dei candidati\n",
    "df_candidates = df.dropna(subset=['correct_album', 'id_album'])\n",
    "track_counts = df_candidates.groupby(['correct_album', 'id_album']).size().to_frame('count')\n",
    "\n",
    "# Ordina i candidati per album e poi per frequenza\n",
    "track_counts = track_counts.sort_values(['correct_album', 'count'], ascending=[True, False])\n",
    "\n",
    "# Crea un dizionario di liste di candidati: {'Album': ['id_pi√π_freq', 'id_secondo_pi√π_freq']}\n",
    "all_id_candidates = track_counts.reset_index().groupby('correct_album')['id_album'].apply(list).to_dict()\n",
    "\n",
    "# --- 3.2: Esecuzione del Loop ---\n",
    "\n",
    "used_ids = set() # Set per gli ID gi√† \"presi\"\n",
    "final_album_to_id_map = {} # La nostra mappa pulita finale\n",
    "\n",
    "def generate_new_id():\n",
    "    new_id = f\"ALB{random.randint(100000, 999999)}\"\n",
    "    while new_id in used_ids:\n",
    "        new_id = f\"ALB{random.randint(100000, 999999)}\"\n",
    "    return new_id\n",
    "\n",
    "print(f\"Inizio processamento di {len(album_order)} album per assegnazione ID...\")\n",
    "\n",
    "# Itera sugli album in ordine di priorit√†\n",
    "for album_name in album_order:\n",
    "    \n",
    "    candidate_ids = all_id_candidates.get(album_name, []) # Lista di ID candidati\n",
    "    assigned_id = None # Flag\n",
    "\n",
    "    # Cerca il primo ID valido (non gi√† usato)\n",
    "    for potential_id in candidate_ids:\n",
    "        if potential_id not in used_ids:\n",
    "            assigned_id = potential_id\n",
    "            used_ids.add(assigned_id) # \"Prenota\" l'ID\n",
    "            final_album_to_id_map[album_name] = assigned_id\n",
    "            break # Passa all'album successivo\n",
    "    \n",
    "    # Se non √® stato trovato nessun ID valido (o non c'erano candidati)\n",
    "    if assigned_id is None:\n",
    "        new_id = generate_new_id()\n",
    "        used_ids.add(new_id)\n",
    "        final_album_to_id_map[album_name] = new_id\n",
    "\n",
    "print(\"Processamento ID terminato. Mappa 1-a-1 creata.\")\n",
    "\n",
    "# --- 3.3: Applicazione Finale ---\n",
    "\n",
    "# Applica la mappa pulita al DataFrame\n",
    "df['id_album_final'] = df['correct_album'].map(final_album_to_id_map)\n",
    "\n",
    "print(\"\\n--- VERIFICA FINALE ---\")\n",
    "\n",
    "# Controlla la relazione 1-a-1\n",
    "check_ids_per_album = df.groupby('correct_album')['id_album_final'].nunique()\n",
    "check_albums_per_id = df.groupby('id_album_final')['correct_album'].nunique()\n",
    "\n",
    "print(f\"Album con pi√π di 1 ID: {(check_ids_per_album > 1).sum()}\")\n",
    "print(f\"ID con pi√π di 1 Album: {(check_albums_per_id > 1).sum()}\")\n",
    "\n",
    "print(\"\\n--- Esempio di 10 righe pulite: ---\")\n",
    "print(df[['correct_album', 'id_album', 'id_album_final']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "444161c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['album'] = df['correct_album']\n",
    "df.drop(columns=['album_name', 'album_norm', 'album_name_norm', 'correct_album'], inplace=True)\n",
    "df['id_album'] = df['id_album_final']\n",
    "df.drop(columns=['id_album_final'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0256faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11166 entries, TR934808 to TR552777\n",
      "Data columns (total 52 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  object        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11061 non-null  category      \n",
      " 5   album                 11161 non-null  object        \n",
      " 6   stats_pageviews       4642 non-null   Int64         \n",
      " 7   swear_IT              11166 non-null  int64         \n",
      " 8   swear_EN              11166 non-null  int64         \n",
      " 9   swear_IT_words        11166 non-null  object        \n",
      " 10  swear_EN_words        11166 non-null  object        \n",
      " 11  year                  10728 non-null  Int64         \n",
      " 12  month                 9969 non-null   Int64         \n",
      " 13  day                   9843 non-null   Int64         \n",
      " 14  n_sentences           11090 non-null  float64       \n",
      " 15  n_tokens              11090 non-null  float64       \n",
      " 16  tokens_per_sent       11090 non-null  float64       \n",
      " 17  char_per_tok          11090 non-null  float64       \n",
      " 18  lexical_density       11090 non-null  float64       \n",
      " 19  avg_token_per_clause  11090 non-null  float64       \n",
      " 20  bpm                   11102 non-null  float64       \n",
      " 21  centroid              11102 non-null  float64       \n",
      " 22  rolloff               11102 non-null  float64       \n",
      " 23  flux                  11102 non-null  float64       \n",
      " 24  rms                   11102 non-null  float64       \n",
      " 25  zcr                   11102 non-null  float64       \n",
      " 26  flatness              11102 non-null  float64       \n",
      " 27  spectral_complexity   11102 non-null  float64       \n",
      " 28  pitch                 11102 non-null  float64       \n",
      " 29  loudness              11102 non-null  float64       \n",
      " 30  album_release_date    10827 non-null  datetime64[ns]\n",
      " 31  album_type            11088 non-null  category      \n",
      " 32  disc_number           11088 non-null  Int64         \n",
      " 33  track_number          11088 non-null  Int64         \n",
      " 34  duration_ms           11088 non-null  float64       \n",
      " 35  explicit              11166 non-null  bool          \n",
      " 36  popularity            11137 non-null  Int64         \n",
      " 37  album_image           11088 non-null  string        \n",
      " 38  id_album              11161 non-null  object        \n",
      " 39  lyrics                11163 non-null  string        \n",
      " 40  modified_popularity   11166 non-null  bool          \n",
      " 41  gender                11166 non-null  category      \n",
      " 42  birth_date            8588 non-null   datetime64[ns]\n",
      " 43  birth_place           8588 non-null   category      \n",
      " 44  nationality           8557 non-null   category      \n",
      " 45  description           10028 non-null  string        \n",
      " 46  active_start          6565 non-null   datetime64[ns]\n",
      " 47  province              8467 non-null   category      \n",
      " 48  region                8024 non-null   category      \n",
      " 49  country               8467 non-null   category      \n",
      " 50  latitude              8588 non-null   float64       \n",
      " 51  longitude             8588 non-null   float64       \n",
      "dtypes: Int64(7), bool(2), category(8), datetime64[ns](3), float64(19), int64(2), object(7), string(4)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf2848",
   "metadata": {},
   "source": [
    "## Stats page views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d9fbf",
   "metadata": {},
   "source": [
    "As considered in data understanding phase, almost 60% of records is missing (Nan) so we decided to drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eef90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['stats_pageviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2706b",
   "metadata": {},
   "source": [
    "## Year, Month, Day and Album Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd2e79",
   "metadata": {},
   "source": [
    "there are multiple nan occurrences in month and day column so we decided to set such records at 01 to create a proper date based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "638b9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_components = {\n",
    "    # CRITICAL CHANGE: Fill NaN in 'year' with 0. \n",
    "    # This prevents the ValueError by allowing astype('int64') to work.\n",
    "    # The 'errors='coerce'' parameter will ensure year 0 converts to NaT.\n",
    "    'year': df['year'].fillna(0).astype('int64'),\n",
    "    'month': df['month'].fillna(1).astype('int64'),\n",
    "    'day': df['day'].fillna(1).astype('int64')\n",
    "}\n",
    "\n",
    "df['date'] = pd.to_datetime(date_components, errors='coerce') \n",
    "df.drop(columns=['year', 'month', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4283c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_release=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc8c3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Creazione di un dataset \"Lungo\" con tutte le date e le info anagrafiche ---\n",
    "# Manteniamo 'name_artist' e 'birth_date' nell'indice, poi impiliamo le date\n",
    "df_all_dates = (\n",
    "    df_release.set_index(['name_artist', 'birth_date'])[['date', 'album_release_date']]\n",
    "    .stack()                        # Impila le colonne date in un'unica serie\n",
    "    .reset_index(name='data_evento') # Trasforma in DataFrame, la colonna delle date si chiama 'data_evento'\n",
    "    .drop(columns=['level_2'])      # Rimuove i nomi vecchi delle colonne ('date', 'album_release_date') che non servono\n",
    ")\n",
    "\n",
    "# --- 2. Creazione della Soglia di Controllo (+15 anni) ---\n",
    "df_all_dates['soglia_adulto'] = df_all_dates['birth_date'] + pd.DateOffset(years=15)\n",
    "\n",
    "# --- 3. Applicazione del Filtro Intelligente ---\n",
    "# Vogliamo tenere la data SOLO se:\n",
    "# A. La data √® maggiore del 1960 (filtro rumore base)\n",
    "#    AND\n",
    "# B. (Manca la data di nascita) OR (C'√® la nascita e la data evento √® > 15 anni)\n",
    "# 1988 is the minimum registered active start in dataset\n",
    "condizione_validita = (df_all_dates['data_evento'] > '1988-01-01') & \\\n",
    "                      (\n",
    "                          (df_all_dates['soglia_adulto'].isna()) | \\\n",
    "                          (df_all_dates['data_evento'] >= df_all_dates['soglia_adulto'])\n",
    "                      )\n",
    "\n",
    "# Applichiamo il filtro: questo rimuove tutte le tracce \"infantili\"\n",
    "date_valide_plausibili = df_all_dates[condizione_validita]\n",
    "\n",
    "# --- 4. Calcolo della Minima Storica (quella \"dopo\") ---\n",
    "# Ora che abbiamo buttato le date impossibili, la .min() prender√† automaticamente\n",
    "# la successiva pi√π piccola valida.\n",
    "min_storico_artista = date_valide_plausibili.groupby('name_artist')['data_evento'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e41cfea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Integrazione nel calcolo finale (Come prima) ---\n",
    "stima_storica = df_release['name_artist'].map(min_storico_artista)\n",
    "\n",
    "soglia_master = df_release['active_start'].combine_first(\n",
    "    stima_storica \n",
    ")\n",
    "\n",
    "# Applicazione correzione\n",
    "condizione = (df_release['date'] > pd.Timestamp.now()) | \\\n",
    "             (df_release['date'].isna()) | \\\n",
    "             (df_release['date'] < soglia_master)\n",
    "\n",
    "df_release.loc[condizione, 'date'] = df_release.loc[condizione, 'album_release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "454076fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione = (df_release['album_release_date'] > pd.Timestamp.now()) | \\\n",
    "             (df_release['album_release_date'].isna()) | \\\n",
    "             (df_release['album_release_date'] < soglia_master)\n",
    "\n",
    "df_release.loc[condizione, 'album_release_date'] = df_release.loc[condizione, 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94ec15",
   "metadata": {},
   "source": [
    "After cleaning and filtering the dataset, the columns of album_release_date and date are redundant and both provide the same information on when did the track released.\n",
    "\n",
    "We decided to use both column to correct the missing and non valid data and selected the best mode of the tracks in the same album (not the tracks with album_type=single) between the date mode and album_release_date mode.\n",
    "\n",
    "Then we create the column release_date that has the same data for all the tracks in the same album (selected with the previous control) and for singles we decided to use the column date because of more accuracy in the \"mode test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b453a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo della moda migliore per ciascun album...\n",
      "\n",
      "--- Conteggio finale delle fonti scelte ---\n",
      "best_mode_source\n",
      "date                  1387\n",
      "album_release_date     494\n",
      "none                     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fatto! Colonne 'correct_release_date' e 'best_mode_source' aggiunte.\n",
      "\n",
      "--- Esempio di 10 righe casuali ---\n",
      "                                 album       date album_release_date  \\\n",
      "5190              il mio lato peggiore 2025-04-23         2025-05-16   \n",
      "5653     p secondigliano 2020 remaster 2018-07-20         2018-07-20   \n",
      "9673              hellvisback platinum 2016-02-05         2016-02-05   \n",
      "2571   basso profilo the mixtape mmxvi 2022-06-24         2022-06-24   \n",
      "1193             sentimental season ep 2021-12-03         2021-12-03   \n",
      "10212                   nato nellacqua 2012-02-17         2012-02-17   \n",
      "420       real talk cypher vol 1 mixed 2016-07-25         2019-09-06   \n",
      "7225     quello che vi consiglio vol 1 2009-10-16         2023-12-14   \n",
      "5512            di vizi di forma virtu 2008-06-13         2008-06-13   \n",
      "7825                       non dormire 2005-07-22         2005-01-01   \n",
      "\n",
      "      correct_release_date  \n",
      "5190            2025-05-16  \n",
      "5653            2018-07-20  \n",
      "9673            2016-02-05  \n",
      "2571            2016-12-25  \n",
      "1193            2021-12-03  \n",
      "10212           2012-02-17  \n",
      "420             2016-07-25  \n",
      "7225            2009-10-16  \n",
      "5512            2008-06-13  \n",
      "7825            2005-07-22  \n"
     ]
    }
   ],
   "source": [
    "group_keys = ['album'] \n",
    "def get_best_mode_info(group):\n",
    "    \"\"\"\n",
    "    Calcola la moda e la frequenza per 'date' e 'album_release_date'\n",
    "    e restituisce SIA LA FONTE ('date'/'album_release_date') \n",
    "    SIA IL VALORE (la data).\n",
    "    \"\"\"\n",
    "    # Calcola mode e frequenze per 'date'\n",
    "    date_counts = group['date'].value_counts()\n",
    "    date_mode_freq = date_counts.iloc[0] if not date_counts.empty else 0\n",
    "    date_mode_value = date_counts.index[0] if not date_counts.empty else pd.NaT\n",
    "\n",
    "    # Calcola mode e frequenze per 'album_release_date'\n",
    "    release_counts = group['album_release_date'].value_counts()\n",
    "    release_mode_freq = release_counts.iloc[0] if not release_counts.empty else 0\n",
    "    release_mode_value = release_counts.index[0] if not release_counts.empty else pd.NaT\n",
    "\n",
    "    # Confronta le frequenze e definisci fonte e valore\n",
    "    if date_mode_freq > release_mode_freq:\n",
    "        source = 'date'\n",
    "        value = date_mode_value\n",
    "    elif release_mode_freq > date_mode_freq:\n",
    "        source = 'album_release_date'\n",
    "        value = release_mode_value\n",
    "    elif date_mode_freq == 0: # Nessun dato valido\n",
    "        source = 'none'\n",
    "        value = pd.NaT\n",
    "    else: \n",
    "        # Pareggio (date_mode_freq == release_mode_freq > 0)\n",
    "        source = 'date' # Scegliamo 'date' come preferenza\n",
    "        value = date_mode_value\n",
    "    \n",
    "    # Restituisci una Serie con entrambe le info\n",
    "    return pd.Series({\n",
    "        'best_mode_source': source, \n",
    "        'correct_release_date': value\n",
    "    })\n",
    "\n",
    "# 4. Calcola le info migliori per ogni gruppo (album)\n",
    "print(\"Calcolo della moda migliore per ciascun album...\")\n",
    "# best_album_info ora sar√† un DataFrame con colonne 'best_mode_source' e 'correct_release_date'\n",
    "best_album_info = df_release.groupby(group_keys).apply(get_best_mode_info)\n",
    "\n",
    "print(\"\\n--- Conteggio finale delle fonti scelte ---\")\n",
    "final_counts = best_album_info['best_mode_source'].value_counts()\n",
    "print(final_counts)\n",
    "\n",
    "# Unisci (merge) questo DataFrame al DataFrame originale\n",
    "# Pandas unir√† 'best_mode_source' e 'correct_release_date'\n",
    "df_release = df_release.merge(best_album_info, on=group_keys, how='left')\n",
    "\n",
    "print(\"\\nFatto! Colonne 'correct_release_date' e 'best_mode_source' aggiunte.\")\n",
    "\n",
    "# Controlla il risultato (mostrando tutte le colonne rilevanti)\n",
    "print(\"\\n--- Esempio di 10 righe casuali ---\")\n",
    "print(df_release.sample(n=10)[['album', 'date', 'album_release_date', 'correct_release_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e89a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      album       date album_release_date  \\\n",
      "5995                      champagne e spine 2007-01-01         2007-01-01   \n",
      "7583                           bieber fever 2023-05-02         2024-05-30   \n",
      "2616  pmc vs club dogo the official mixtape 2004-04-30         2017-11-17   \n",
      "8567                     taxi driver deluxe 2021-04-30         2021-11-30   \n",
      "5483                 di vizi di forma virtu 2008-06-13         2008-06-13   \n",
      "8013                       faccio un casino 2017-05-05         2017-05-05   \n",
      "1469                     santana money gang 2023-04-24         2025-04-10   \n",
      "2237                         casus belli ep 2012-10-30         2005-01-01   \n",
      "4153                              classe 73 2003-01-01         2003-01-01   \n",
      "4256                                   hate 2005-09-01         2005-09-01   \n",
      "\n",
      "     correct_release_date  \n",
      "5995           2010-10-11  \n",
      "7583           2023-05-02  \n",
      "2616           2004-04-30  \n",
      "8567           2021-09-23  \n",
      "5483           2008-06-13  \n",
      "8013           2017-05-05  \n",
      "1469           2025-04-10  \n",
      "2237           2012-10-30  \n",
      "4153           2003-01-01  \n",
      "4256           2005-09-01  \n"
     ]
    }
   ],
   "source": [
    "print(df_release.sample(n=10)[['album', 'date', 'album_release_date', 'correct_release_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7becc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione = (df_release['album_type'] == 'single')\n",
    "df_release.loc[condizione, 'correct_release_date'] = df_release.loc[condizione, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79b95959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame 'df' cleaned!\n",
      "         id_artist  name_artist  \\\n",
      "3666   ART52272796        neffa   \n",
      "10946  ART07024718        fedez   \n",
      "7861   ART07127070  noyz narcos   \n",
      "4702   ART15560128        vacca   \n",
      "4714   ART15560128        vacca   \n",
      "5878   ART86549066   emis killa   \n",
      "1212   ART70825116   capo plaza   \n",
      "7090   ART85046033     gemitaiz   \n",
      "3513   ART75741740  articolo 31   \n",
      "3408   ART75741740  articolo 31   \n",
      "\n",
      "                                                  title  \\\n",
      "3666                                              Intro   \n",
      "10946                                        Caro Amico   \n",
      "7861                                              INTRO   \n",
      "4702                                           Corvetto   \n",
      "4714                                           Grateful   \n",
      "5878                                 Martin Luther King   \n",
      "1212                                                 20   \n",
      "7090                                            A Me Mi   \n",
      "3513   √à Natale (Ma Io Non Ci Sto Dentro) (Strumentale)   \n",
      "3408                                       La rinascita   \n",
      "\n",
      "                 featured_artists language  \\\n",
      "3666                           []       it   \n",
      "10946  [cili man, dari mc, sbend]       it   \n",
      "7861                           []       en   \n",
      "4702                      [lelfo]       it   \n",
      "4714                           []       en   \n",
      "5878                      [luche]       it   \n",
      "1212                           []       it   \n",
      "7090              [achille lauro]       it   \n",
      "3513                           []       co   \n",
      "3408                           []       it   \n",
      "\n",
      "                                                  album  swear_IT  swear_EN  \\\n",
      "3666   i molteplici mondi di giovanni il cantante neffa         0         0   \n",
      "10946                      tutto il contrario remixtape         3         0   \n",
      "7861                                       virus deluxe         0         1   \n",
      "4702                                 don vacca corleone         7         0   \n",
      "4714                                                don         0         0   \n",
      "5878                                keta music volume 2         1         1   \n",
      "1212                                  20 deluxe edition         3         0   \n",
      "7090                      quello che vi consiglio vol 7         2         0   \n",
      "3513                   e natale ma io non ci sto dentro         0         0   \n",
      "3408                                            nessuno         4         0   \n",
      "\n",
      "                             swear_IT_words swear_EN_words  n_sentences  \\\n",
      "3666                                     []             []          1.0   \n",
      "10946                         [cazzo, culo]             []         99.0   \n",
      "7861                                     []         [shit]         11.0   \n",
      "4702           [cazzi, cazzo, merda, palle]             []         77.0   \n",
      "4714                                     []             []         71.0   \n",
      "5878                             [pisciare]        [bitch]         75.0   \n",
      "1212                  [cazzo, palle, troia]             []         60.0   \n",
      "7090                                [merda]             []         42.0   \n",
      "3513                                     []             []         12.0   \n",
      "3408   [cazzo, coglioni, merda, vaffanculo]             []         97.0   \n",
      "\n",
      "       n_tokens  tokens_per_sent  char_per_tok  lexical_density  \\\n",
      "3666        3.0         3.000000      8.000000         0.666667   \n",
      "10946     877.0         8.858586      4.097448         0.480278   \n",
      "7861       94.0         8.545455      3.678161         0.321839   \n",
      "4702      839.0        10.896104      4.451087         0.572011   \n",
      "4714      512.0         7.211268      4.060465         0.225581   \n",
      "5878      545.0         7.266667      4.022965         0.574113   \n",
      "1212      570.0         9.500000      3.814961         0.492126   \n",
      "7090      528.0        12.571429      3.593301         0.497608   \n",
      "3513      168.0        14.000000      3.235294         0.500000   \n",
      "3408      970.0        10.000000      4.282366         0.523438   \n",
      "\n",
      "       avg_token_per_clause     bpm  centroid    rolloff    flux     rms  \\\n",
      "3666               0.000000   92.86    0.1191  1438.0479  1.4375  0.2039   \n",
      "10946              5.965986  156.11    0.1267  1940.4243  1.0959  0.2278   \n",
      "7861              94.000000   95.00    0.1154   874.4430  1.1980  0.2808   \n",
      "4702               9.426966  129.52    0.1090  1241.6156  1.3716  0.2320   \n",
      "4714              64.000000  153.86    0.1674  2306.6749  1.2706  0.2723   \n",
      "5878               8.790323  139.87    0.1513  2034.9864  1.1218  0.2404   \n",
      "1212               5.181818  150.10    0.1184  1347.4906  1.2210  0.2301   \n",
      "7090               7.436620  134.88    0.1436  1580.6706  1.2847  0.1717   \n",
      "3513              14.000000   99.90    0.1527  1650.6726  1.1681  0.1859   \n",
      "3408               7.080292  178.21    0.1199  1222.0837  1.3413  0.1347   \n",
      "\n",
      "          zcr  flatness  spectral_complexity      pitch  loudness  \\\n",
      "3666   0.0544    0.9076              21.5547  2418.2060   21.1407   \n",
      "10946  0.0753    0.9570              41.8383  2203.7709   23.6712   \n",
      "7861   0.0465    0.9026              25.2485  2168.5256   32.0923   \n",
      "4702   0.0470    0.9436              23.6239  2241.1467   24.4674   \n",
      "4714   0.0837    0.7726              39.3957  2459.4751   30.8413   \n",
      "5878   0.0787    0.8454              34.8047  2123.3794   26.6078   \n",
      "1212   0.0520    0.8911              26.3010  2086.8692   24.9767   \n",
      "7090   0.0616    0.8354              20.2327  2013.1721   17.1210   \n",
      "3513   0.0712    0.9415              29.8075  2444.3543   17.8328   \n",
      "3408   0.0516    0.8911              21.2237  2327.6136   11.8504   \n",
      "\n",
      "      album_release_date album_type  disc_number  track_number  duration_ms  \\\n",
      "3666          2003-01-01      album            1             1      26026.0   \n",
      "10946         2011-05-28      album            1             9     266173.0   \n",
      "7861          2007-01-01      album            1             1     146506.0   \n",
      "4702          2019-01-11      album            1            10     226432.0   \n",
      "4714          2018-01-19      album            1            11     273227.0   \n",
      "5878          2015-06-18      album            1            10     253186.0   \n",
      "1212          2018-04-20      album            1             1     241800.0   \n",
      "7090          2025-05-16      album            1             7     223834.0   \n",
      "3513          1993-12-25     single            1             5     224066.0   \n",
      "3408          1998-05-07      album            1             2     334160.0   \n",
      "\n",
      "       explicit  popularity  \\\n",
      "3666      False           0   \n",
      "10946     False          59   \n",
      "7861      False          36   \n",
      "4702      False          28   \n",
      "4714      False          11   \n",
      "5878      False          23   \n",
      "1212      False          51   \n",
      "7090       True          79   \n",
      "3513      False           0   \n",
      "3408      False          22   \n",
      "\n",
      "                                             album_image   id_album  \\\n",
      "3666   https://i.scdn.co/image/ab67616d0000b273905c3d...  ALB946793   \n",
      "10946  https://i.scdn.co/image/ab67616d0000b273302045...  ALB346288   \n",
      "7861   https://i.scdn.co/image/ab67616d0000b273bfc8c7...  ALB525038   \n",
      "4702   https://i.scdn.co/image/ab67616d0000b273eca8bb...  ALB622549   \n",
      "4714   https://i.scdn.co/image/ab67616d0000b27362b7ea...  ALB651093   \n",
      "5878   https://i.scdn.co/image/ab67616d0000b2739d9a7d...  ALB360767   \n",
      "1212   https://i.scdn.co/image/ab67616d0000b273fe950b...  ALB234424   \n",
      "7090   https://i.scdn.co/image/ab67616d0000b2730bd9d3...  ALB957480   \n",
      "3513   https://i.scdn.co/image/ab67616d0000b27346fddb...  ALB193064   \n",
      "3408   https://i.scdn.co/image/ab67616d0000b2739b63fe...  ALB906997   \n",
      "\n",
      "                                                  lyrics  modified_popularity  \\\n",
      "3666                          2 ContributorsIntro Lyrics                False   \n",
      "10946  E l'ho capito finalmente, niente √® per sempre\n",
      "...                False   \n",
      "7861   Yo, check it out y'all, what's the deal\n",
      "It's t...                False   \n",
      "4702   Il brano vanta la collaborazione di L'Elfo.\n",
      "La...                False   \n",
      "4714   Have you ever breathed?\n",
      "(Breathed)\n",
      "Have you ev...                False   \n",
      "5878   Uh, sto a cento all'ora, H√§kkinen\n",
      "Quando arriv...                False   \n",
      "1212   Prima ero solo un bambino, ora ne ho venti\n",
      "Tu ...                False   \n",
      "7090   Pi√π mi guardo intorno, pi√π non so come, pi√π no...                False   \n",
      "3513   √à Natale, √® Natale, ma io non ci sto dentro\n",
      "√à ...                False   \n",
      "3408   Sballa, strippa, salta su (Su, su, su)\n",
      "Spaghet...                False   \n",
      "\n",
      "      gender birth_date birth_place nationality  \\\n",
      "3666       M 1967-10-07     Scafati      Italia   \n",
      "10946      M 1989-10-15      Milano      Italia   \n",
      "7861       M 1979-12-15        Roma      Italia   \n",
      "4702       M 1979-10-21    Cagliari      Italia   \n",
      "4714       M 1979-10-21    Cagliari      Italia   \n",
      "5878       M 1989-11-14   Vimercate      Italia   \n",
      "1212       M 1998-04-20     Salerno      Italia   \n",
      "7090       M 1988-11-04        Roma      Italia   \n",
      "3513       M        NaT         NaN         NaN   \n",
      "3408       M        NaT         NaN         NaN   \n",
      "\n",
      "                                             description active_start  \\\n",
      "3666   cantautore, rapper e produttore discografico i...   1988-01-01   \n",
      "10946  rapper, personaggio televisivo e imprenditore ...   2006-01-01   \n",
      "7861   rapper e produttore discografico italiano (1979-)   1996-01-01   \n",
      "4702                                     rapper italiano   2001-01-01   \n",
      "4714                                     rapper italiano   2001-01-01   \n",
      "5878                             rapper italiano (1989-)   2006-01-01   \n",
      "1212                             rapper italiano (1998-)          NaT   \n",
      "7090    rapper e produttore discografico italiano (1988)   2003-01-01   \n",
      "3513                    gruppo musicale hip hop italiano   1990-01-01   \n",
      "3408                    gruppo musicale hip hop italiano   1990-01-01   \n",
      "\n",
      "                    province     region country   latitude  longitude  \\\n",
      "3666                 Salerno   Campania  Italia  40.749989  14.526949   \n",
      "10946                 Milano  Lombardia  Italia  45.464194   9.189635   \n",
      "7861                    Roma      Lazio  Italia  41.893320  12.482932   \n",
      "4702                Cagliari   Sardegna  Italia  39.217199   9.113311   \n",
      "4714                Cagliari   Sardegna  Italia  39.217199   9.113311   \n",
      "5878   Monza e della Brianza  Lombardia  Italia  45.613963   9.370060   \n",
      "1212                 Salerno   Campania  Italia  40.419442  15.310609   \n",
      "7090                    Roma      Lazio  Italia  41.893320  12.482932   \n",
      "3513                     NaN        NaN     NaN        NaN        NaN   \n",
      "3408                     NaN        NaN     NaN        NaN        NaN   \n",
      "\n",
      "            date    best_mode_source release_date  \n",
      "3666  2003-10-03  album_release_date   2003-01-01  \n",
      "10946 2011-05-28                date   2011-05-28  \n",
      "7861  2022-01-14                date   2022-01-14  \n",
      "4702  2019-01-11  album_release_date   2019-01-11  \n",
      "4714  2018-01-19  album_release_date   2018-01-19  \n",
      "5878  2015-06-18  album_release_date   2015-06-18  \n",
      "1212  2018-04-20  album_release_date   2018-04-20  \n",
      "7090  2016-12-30                date   2016-12-30  \n",
      "3513  1993-12-25  album_release_date   1993-12-25  \n",
      "3408  1998-05-14  album_release_date   1998-05-07  \n"
     ]
    }
   ],
   "source": [
    "group_keys = ['album'] \n",
    "\n",
    "best_album_info_df = df.groupby(group_keys).apply(get_best_mode_info)\n",
    "\n",
    "df = df.merge(best_album_info_df, on=group_keys, how='left')\n",
    "df.rename(columns={'correct_release_date': 'release_date'}, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame 'df' cleaned!\")\n",
    "print(df.sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f988167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['release_date'] > pd.Timestamp.now()) | (df['release_date'].isna()) | (df['release_date']<df['active_start'])\n",
    "dati_filtrati = df.loc[condizione, ['release_date', 'album','active_start','name_artist']]\n",
    "df.loc[condizione, ['release_date']]=pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "274a6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11166 entries, 0 to 11165\n",
      "Data columns (total 51 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  object        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11061 non-null  category      \n",
      " 5   album                 11161 non-null  object        \n",
      " 6   swear_IT              11166 non-null  int64         \n",
      " 7   swear_EN              11166 non-null  int64         \n",
      " 8   swear_IT_words        11166 non-null  object        \n",
      " 9   swear_EN_words        11166 non-null  object        \n",
      " 10  n_sentences           11090 non-null  float64       \n",
      " 11  n_tokens              11090 non-null  float64       \n",
      " 12  tokens_per_sent       11090 non-null  float64       \n",
      " 13  char_per_tok          11090 non-null  float64       \n",
      " 14  lexical_density       11090 non-null  float64       \n",
      " 15  avg_token_per_clause  11090 non-null  float64       \n",
      " 16  bpm                   11102 non-null  float64       \n",
      " 17  centroid              11102 non-null  float64       \n",
      " 18  rolloff               11102 non-null  float64       \n",
      " 19  flux                  11102 non-null  float64       \n",
      " 20  rms                   11102 non-null  float64       \n",
      " 21  zcr                   11102 non-null  float64       \n",
      " 22  flatness              11102 non-null  float64       \n",
      " 23  spectral_complexity   11102 non-null  float64       \n",
      " 24  pitch                 11102 non-null  float64       \n",
      " 25  loudness              11102 non-null  float64       \n",
      " 26  album_release_date    11061 non-null  datetime64[ns]\n",
      " 27  album_type            11088 non-null  category      \n",
      " 28  disc_number           11088 non-null  Int64         \n",
      " 29  track_number          11088 non-null  Int64         \n",
      " 30  duration_ms           11088 non-null  float64       \n",
      " 31  explicit              11166 non-null  bool          \n",
      " 32  popularity            11137 non-null  Int64         \n",
      " 33  album_image           11088 non-null  string        \n",
      " 34  id_album              11161 non-null  object        \n",
      " 35  lyrics                11163 non-null  string        \n",
      " 36  modified_popularity   11166 non-null  bool          \n",
      " 37  gender                11166 non-null  category      \n",
      " 38  birth_date            8588 non-null   datetime64[ns]\n",
      " 39  birth_place           8588 non-null   category      \n",
      " 40  nationality           8557 non-null   category      \n",
      " 41  description           10028 non-null  string        \n",
      " 42  active_start          6565 non-null   datetime64[ns]\n",
      " 43  province              8467 non-null   category      \n",
      " 44  region                8024 non-null   category      \n",
      " 45  country               8467 non-null   category      \n",
      " 46  latitude              8588 non-null   float64       \n",
      " 47  longitude             8588 non-null   float64       \n",
      " 48  date                  11061 non-null  datetime64[ns]\n",
      " 49  best_mode_source      11161 non-null  object        \n",
      " 50  release_date          11150 non-null  datetime64[ns]\n",
      "dtypes: Int64(3), bool(2), category(8), datetime64[ns](5), float64(19), int64(2), object(8), string(4)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e00b3",
   "metadata": {},
   "source": [
    "Active start was remodeled and filled the first date recorded of their tracks/albums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa8a79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['active_start'] = df['active_start'].fillna(\n",
    "    df.groupby('name_artist')['release_date'].transform('min')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa9b6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = ['album_release_date', 'date', 'best_mode_source','new_release']\n",
    "df.drop(columns=col_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddaf2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11166 entries, 0 to 11165\n",
      "Data columns (total 48 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  object        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11061 non-null  category      \n",
      " 5   album                 11161 non-null  object        \n",
      " 6   swear_IT              11166 non-null  int64         \n",
      " 7   swear_EN              11166 non-null  int64         \n",
      " 8   swear_IT_words        11166 non-null  object        \n",
      " 9   swear_EN_words        11166 non-null  object        \n",
      " 10  n_sentences           11090 non-null  float64       \n",
      " 11  n_tokens              11090 non-null  float64       \n",
      " 12  tokens_per_sent       11090 non-null  float64       \n",
      " 13  char_per_tok          11090 non-null  float64       \n",
      " 14  lexical_density       11090 non-null  float64       \n",
      " 15  avg_token_per_clause  11090 non-null  float64       \n",
      " 16  bpm                   11102 non-null  float64       \n",
      " 17  centroid              11102 non-null  float64       \n",
      " 18  rolloff               11102 non-null  float64       \n",
      " 19  flux                  11102 non-null  float64       \n",
      " 20  rms                   11102 non-null  float64       \n",
      " 21  zcr                   11102 non-null  float64       \n",
      " 22  flatness              11102 non-null  float64       \n",
      " 23  spectral_complexity   11102 non-null  float64       \n",
      " 24  pitch                 11102 non-null  float64       \n",
      " 25  loudness              11102 non-null  float64       \n",
      " 26  album_type            11088 non-null  category      \n",
      " 27  disc_number           11088 non-null  Int64         \n",
      " 28  track_number          11088 non-null  Int64         \n",
      " 29  duration_ms           11088 non-null  float64       \n",
      " 30  explicit              11166 non-null  bool          \n",
      " 31  popularity            11137 non-null  Int64         \n",
      " 32  album_image           11088 non-null  string        \n",
      " 33  id_album              11161 non-null  object        \n",
      " 34  lyrics                11163 non-null  string        \n",
      " 35  modified_popularity   11166 non-null  bool          \n",
      " 36  gender                11166 non-null  category      \n",
      " 37  birth_date            8588 non-null   datetime64[ns]\n",
      " 38  birth_place           8588 non-null   category      \n",
      " 39  nationality           8557 non-null   category      \n",
      " 40  description           10028 non-null  string        \n",
      " 41  active_start          11166 non-null  datetime64[ns]\n",
      " 42  province              8467 non-null   category      \n",
      " 43  region                8024 non-null   category      \n",
      " 44  country               8467 non-null   category      \n",
      " 45  latitude              8588 non-null   float64       \n",
      " 46  longitude             8588 non-null   float64       \n",
      " 47  release_date          11150 non-null  datetime64[ns]\n",
      "dtypes: Int64(3), bool(2), category(8), datetime64[ns](3), float64(19), int64(2), object(7), string(4)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7abc8010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_start</th>\n",
       "      <th>album</th>\n",
       "      <th>active_start</th>\n",
       "      <th>name_artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [active_start, album, active_start, name_artist]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condizione= (df['active_start'] > pd.Timestamp.now()) | (df['active_start']<pd.Timestamp('1980-01-01'))\n",
    "dati_filtrati = df.loc[condizione, ['active_start', 'album','active_start','name_artist']]\n",
    "dati_filtrati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2f1e4",
   "metadata": {},
   "source": [
    "## Popularity and Modified_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1084a",
   "metadata": {},
   "source": [
    "When modified_popularity is 'true' (78 occurrences) the related popularity occurrence is not in the correct format.\n",
    "We decided to drop the column modified_popularity and set the invalid popularity records to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a72b80cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "pop=df[df['modified_popularity']==True]\n",
    "print(len(pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7633e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>modified_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>-654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>147413</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>164032</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>884794</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>-119</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>-606</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>-286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      popularity  modified_popularity\n",
       "1932        <NA>                 True\n",
       "2304        <NA>                 True\n",
       "3286        -654                 True\n",
       "3667      147413                 True\n",
       "3668      164032                 True\n",
       "...          ...                  ...\n",
       "3737        <NA>                 True\n",
       "3738      884794                 True\n",
       "3739        -119                 True\n",
       "6390        -606                 True\n",
       "9261        -286                 True\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condizione= (df['popularity']<0) | (df['popularity']>100) | (df['popularity'].isna())\n",
    "dati_filtrati = df.loc[condizione, ['popularity', 'modified_popularity']]\n",
    "dati_filtrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "622b166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['popularity']<0) | (df['popularity']>100)\n",
    "df.loc[condizione, 'popularity'] = np.nan\n",
    "df.drop(columns=['modified_popularity'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c6238",
   "metadata": {},
   "source": [
    "## Swear words control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a99c24",
   "metadata": {},
   "source": [
    "We checked if the column Swear_IT and Swear_EN are coherent with the lyrics and their related Swear_IT_words and Swear_EN_words lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "225fd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3: Confronto dei conteggi...\n",
      "3/3: Controllo completato.\n",
      "\n",
      "‚úÖ Controllo superato! Tutti i conteggi 'swear_IT' corrispondono al ricalcolo dai testi.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pandas.api.types import is_scalar # Importiamo il controllo specifico\n",
    "\n",
    "## 1. Funzione per contare le parole nel testo (Versione 3)\n",
    "\n",
    "def conta_parole_in_testo_corretto(riga):\n",
    "    lista_parole = riga['swear_IT_words']\n",
    "    testo_lyrics = riga['lyrics']\n",
    "\n",
    "    # --- CORREZIONE ---\n",
    "    # 1. Controlla il testo (che √® sempre scalare)\n",
    "    if pd.isna(testo_lyrics):\n",
    "        return 0\n",
    "\n",
    "    # 2. Controlla la lista/array\n",
    "    # Prima controlla se √® uno scalare (es. np.nan o None)\n",
    "    if is_scalar(lista_parole):\n",
    "        if pd.isna(lista_parole):\n",
    "            return 0  # √à np.nan o None, quindi 0\n",
    "        else:\n",
    "            return 0  # √à uno scalare ma non nullo (es. un numero), non va bene\n",
    "    \n",
    "    # Se siamo qui, 'lista_parole' NON √® uno scalare,\n",
    "    # quindi √® una lista o un array. Ora possiamo controllarne la lunghezza.\n",
    "    if len(lista_parole) == 0:\n",
    "        return 0\n",
    "    # --- FINE CORREZIONE ---\n",
    "\n",
    "    conteggio_totale = 0\n",
    "    testo_lower = testo_lyrics.lower()\n",
    "\n",
    "    for parola in lista_parole:\n",
    "        parola_lower = str(parola).lower()\n",
    "        pattern = r'\\b' + re.escape(parola_lower) + r'\\b'\n",
    "        occorrenze = re.findall(pattern, testo_lower)\n",
    "        conteggio_totale += len(occorrenze)\n",
    "\n",
    "    return conteggio_totale\n",
    "\n",
    "# --- ESECUZIONE DEL CONTROLLO ---\n",
    "\n",
    "# 1. Calcola il nuovo conteggio applicando la funzione CORRETTA\n",
    "print(\"1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\")\n",
    "df['conteggio_calcolato'] = df.apply(conta_parole_in_testo_corretto, axis=1)\n",
    "\n",
    "# 2. Prepara il conteggio originale\n",
    "df['conteggio_originale'] = df['swear_IT'].fillna(0).astype(int)\n",
    "\n",
    "# 3. Confronta i due conteggi\n",
    "print(\"2/3: Confronto dei conteggi...\")\n",
    "incoerenze = df[df['conteggio_originale'] != df['conteggio_calcolato']]\n",
    "\n",
    "# --- RISULTATO ---\n",
    "print(\"3/3: Controllo completato.\")\n",
    "if incoerenze.empty:\n",
    "    print(\"\\n‚úÖ Controllo superato! Tutti i conteggi 'swear_IT' corrispondono al ricalcolo dai testi.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Trovate {len(incoerenze)} righe incoerenti:\")\n",
    "    print(incoerenze[['artist', 'title', 'conteggio_originale', 'conteggio_calcolato', 'swear_IT_words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56733761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\n",
      "2/3: Confronto dei conteggi...\n",
      "3/3: Controllo completato.\n",
      "\n",
      "‚úÖ Controllo superato! Tutti i conteggi 'swear_EN' corrispondono al ricalcolo dai testi.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pandas.api.types import is_scalar # Importiamo il controllo specifico\n",
    "\n",
    "## 1. Funzione per contare le parole nel testo (Versione 3)\n",
    "\n",
    "def conta_parole_in_testo_corretto(riga):\n",
    "    lista_parole = riga['swear_EN_words']\n",
    "    testo_lyrics = riga['lyrics']\n",
    "\n",
    "    # --- CORREZIONE ---\n",
    "    # 1. Controlla il testo (che √® sempre scalare)\n",
    "    if pd.isna(testo_lyrics):\n",
    "        return 0\n",
    "\n",
    "    # 2. Controlla la lista/array\n",
    "    # Prima controlla se √® uno scalare (es. np.nan o None)\n",
    "    if is_scalar(lista_parole):\n",
    "        if pd.isna(lista_parole):\n",
    "            return 0  # √à np.nan o None, quindi 0\n",
    "        else:\n",
    "            return 0  # √à uno scalare ma non nullo (es. un numero), non va bene\n",
    "    \n",
    "    # Se siamo qui, 'lista_parole' NON √® uno scalare,\n",
    "    # quindi √® una lista o un array. Ora possiamo controllarne la lunghezza.\n",
    "    if len(lista_parole) == 0:\n",
    "        return 0\n",
    "    # --- FINE CORREZIONE ---\n",
    "\n",
    "    conteggio_totale = 0\n",
    "    testo_lower = testo_lyrics.lower()\n",
    "\n",
    "    for parola in lista_parole:\n",
    "        parola_lower = str(parola).lower()\n",
    "        pattern = r'\\b' + re.escape(parola_lower) + r'\\b'\n",
    "        occorrenze = re.findall(pattern, testo_lower)\n",
    "        conteggio_totale += len(occorrenze)\n",
    "\n",
    "    return conteggio_totale\n",
    "\n",
    "# --- ESECUZIONE DEL CONTROLLO ---\n",
    "\n",
    "# 1. Calcola il nuovo conteggio applicando la funzione CORRETTA\n",
    "print(\"1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\")\n",
    "df['conteggio_calcolato'] = df.apply(conta_parole_in_testo_corretto, axis=1)\n",
    "\n",
    "# 2. Prepara il conteggio originale\n",
    "df['conteggio_originale'] = df['swear_EN'].fillna(0).astype(int)\n",
    "\n",
    "# 3. Confronta i due conteggi\n",
    "print(\"2/3: Confronto dei conteggi...\")\n",
    "incoerenze = df[df['conteggio_originale'] != df['conteggio_calcolato']]\n",
    "\n",
    "# --- RISULTATO ---\n",
    "print(\"3/3: Controllo completato.\")\n",
    "if incoerenze.empty:\n",
    "    print(\"\\n‚úÖ Controllo superato! Tutti i conteggi 'swear_EN' corrispondono al ricalcolo dai testi.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Trovate {len(incoerenze)} righe incoerenti:\")\n",
    "    print(incoerenze[['artist', 'title', 'conteggio_originale', 'conteggio_calcolato', 'swear_EN_words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eed1eac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PAROLE UNICHE ITALIANE (173) ---\n",
      "['cazzo', 'cesso', 'coglioni', 'figa', 'merda', 'palle', 'piscio', 'porca', 'stronzo', 'culo', 'frocio', 'puttana', 'sega', 'troia', 'bastardo', 'fottere', 'pompino', 'scopare', 'figo', 'cagare', 'fica', 'gay', 'zoccola', 'coglione', 'puttano', 'fottuto', 'cazzi', 'ricchione', 'stupido', 'vagina', 'selvaggio', 'fottuti', 'incazzato', 'cretino', 'sboccare', 'zanzara', 'fortuna', 'stupida', 'incazzare', 'cagna', 'grilletto', 'bastardi', 'toro', 'pippa', 'cazzata', 'sedere', 'mazzo', 'cretini', 'vaffanculo', 'fogne', 'scazzo', 'cogliona', 'cozza', 'jug', 'puttanata', 'sfiga', 'pisciare', 'scopata', 'checca', 'fesso', 'madonna', 'scassare', 'uccello', 'sfigata', 'battona', 'fregna', 'topa', 'pisciata', 'bagascia', 'maiala', 'mignotta', 'farabutto', 'chiappa', 'minchioni', 'fico', 'travestito', 'vacca', 'gnocca', 'bischero', 'idiozia', 'handicappato', 'leccaculo', 'pene', 'fregarsene', 'paraculo', 'bocchino', 'farabutti', 'granchio', 'blowjob', 'cappella', 'piccione', 'sborrare', 'fellatio', 'pisello', 'puttaniere', 'cazzeggio', 'cacca', 'stronzata', 'cagata', 'trombare', 'arrapato', 'fogna', 'water', 'cazzone', 'finocchio', 'puttanaio', 'sborra', 'scoreggia', 'feci', 'bernarda', 'sorca', 'cretina', 'bocchinaro', 'deretano', 'scrofa', 'fregare', 'pipa', 'chiavare', 'troiaggine', 'tetta', 'bombare', 'cazzeggiare', 'pugnetta', 'culattone', 'spagnola', 'cornuto', 'sveltina', 'rompicoglioni', 'seccatore', 'smerdare', 'incazzarsi', 'inculare', 'sputtanare', 'fava', 'cazzuto', 'merdina', 'maroni', 'strafottenza', 'arrapante', 'controcoglioni', 'controcazzi', 'fottersi', 'gigolo', 'rompipalle', 'fottio', 'peluria', 'zizza', 'glutei', 'merdaiolo', 'escremento', 'missionario', 'scazzato', 'chiavata', 'raspa', 'cunnu', 'arrapare', 'cacare', 'schizzare', 'vaccata', 'nerchia', 'pecorina', 'troiaio', 'pompinara', 'merdata', 'mezzasega', 'sgualdrina', 'cacata', 'spompinare', 'rottinculo', 'minchiata', 'coglionata', 'merdaio', 'segaiolo']\n"
     ]
    }
   ],
   "source": [
    "lista_parole_it = list(df['swear_IT_words'].explode().dropna().unique())\n",
    "\n",
    "# 2. Lista Inglese (EN)\n",
    "# Stessa identica logica\n",
    "lista_parole_en = list(df['swear_EN_words'].explode().dropna().unique())\n",
    "\n",
    "# 3. Stampa i risultati\n",
    "print(f\"--- PAROLE UNICHE ITALIANE ({len(lista_parole_it)}) ---\")\n",
    "print(lista_parole_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05e95446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PAROLE UNICHE INGLESI (96) ---\n",
      "['bitch', 'fuck', 'porno', 'pussy', 'escort', 'negro', 'sex', 'sexy', 'shit', 'bastardo', 'bitches', 'cock', 'ass', 'slut', 'voyeur', 'anal', 'tranny', 'dick', 'nigga', 'threesome', 'vagina', 'porn', 'clit', 'nude', 'fucking', 'bullshit', 'motherfucker', 'xx', 'boobs', 'xxx', 'playboy', 'deepthroat', 'faggot', 'fisting', 'suck', 'cumming', 'milf', 'hardcore', 'cialis', 'fag', 'rapist', 'rape', 'dildo', 'viagra', 'rimming', 'sexo', 'hentai', 'blowjob', 'fuckin', 'gangbang', 'pedobear', 'fellatio', 'bastard', 'sexual', 'snatch', 'butt', 'cumshot', 'lolita', 'vulva', 'punany', 'panties', 'shitty', 'tits', 'topless', 'anus', 'bbw', 'bondage', 'cunt', 'scat', 'raping', 'shibari', 'poof', 'coon', 'skeet', 'domination', 'ecchi', 'sucks', 'creampie', 'cum', 'titty', 'spic', 'kinky', 'bukkake', 'cocks', 'pissing', 'asshole', 'busty', 'hooker', 'semen', 'horny', 'masturbation', 'neonazi', 'doggystyle', 'tit', 'nympho', 'nipple']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n--- PAROLE UNICHE INGLESI ({len(lista_parole_en)}) ---\")\n",
    "print(lista_parole_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0e4a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['explicit']==False) & ((df['swear_IT']>0) | (df['swear_EN']>0))\n",
    "dati_filtrati = df.loc[condizione, ['explicit', 'swear_IT', 'swear_EN']]\n",
    "dati_filtrati\n",
    "df.loc[condizione,['explicit']] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3ea2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['explicit']==True) & ((df['swear_IT']==0) & (df['swear_EN']==0))\n",
    "dati_filtrati = df.loc[condizione, ['explicit', 'swear_IT', 'swear_EN']]\n",
    "dati_filtrati\n",
    "df.loc[condizione,['explicit']] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99fca526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creazione pattern RegEx in corso...\n",
      "Pattern creati.\n",
      "Inizio ricalcolo su tutto il DataFrame (pu√≤ richiedere tempo)...\n",
      "üìä Numero di record che verranno aggiornati: 3062\n",
      "üìã Creato 'df_swear_copy' con 3062 record (lo stato *prima* della modifica).\n",
      "Ricalcolo completato! Le colonne sono state aggiornate.\n"
     ]
    }
   ],
   "source": [
    "df_swear=df\n",
    "## 1. Preparazione: Creare i Pattern RegEx\n",
    "#    Questa √® la parte pi√π importante per la velocit√†.\n",
    "#    Trasformiamo ['a', 'b', 'c'] in r'\\b(a|b|c)\\b'\n",
    "\n",
    "def crea_pattern(lista_parole):\n",
    "    # 1. Assicura che siano tutte stringhe e minuscole\n",
    "    parole_pulite = [str(p).lower() for p in lista_parole if pd.notna(p)]\n",
    "    \n",
    "    # 2. Fai l'escape di ogni parola (per gestire caratteri come 'f**k')\n",
    "    parole_escaped = [re.escape(p) for p in parole_pulite]\n",
    "    \n",
    "    # 3. Uniscile con un OR (|)\n",
    "    pattern_string = \"|\".join(parole_escaped)\n",
    "    \n",
    "    # 4. Racchiudi in \\b (word boundary) per trovare solo parole intere\n",
    "    #    e compila il pattern RegEx.\n",
    "    return re.compile(r'\\b(' + pattern_string + r')\\b', re.IGNORECASE)\n",
    "\n",
    "print(\"Creazione pattern RegEx in corso...\")\n",
    "pattern_it = crea_pattern(lista_parole_it)\n",
    "pattern_en = crea_pattern(lista_parole_en)\n",
    "print(\"Pattern creati.\")\n",
    "\n",
    "\n",
    "## 2. Definire la Funzione di Ricalcolo\n",
    "#    Questa funzione verr√† applicata a OGNI RIGA del DataFrame\n",
    "\n",
    "def ricalcola_parole(riga):\n",
    "    testo = riga['lyrics']\n",
    "    \n",
    "    # Se il testo √® mancante, restituisci valori vuoti\n",
    "    if pd.isna(testo):\n",
    "        return 0, [], 0, [] # count_it, words_it, count_en, words_en\n",
    "\n",
    "    # --- Processo Italiano ---\n",
    "    # re.findall() trova TUTTE le occorrenze (anche duplicate)\n",
    "    matches_it = pattern_it.findall(testo.lower())\n",
    "    \n",
    "    # Il conteggio √® il numero totale di occorrenze\n",
    "    conteggio_it = len(matches_it)\n",
    "    # La lista di parole √® l'insieme unico (set) delle parole trovate\n",
    "    parole_uniche_it = list(set(matches_it))\n",
    "\n",
    "    # --- Processo Inglese ---\n",
    "    matches_en = pattern_en.findall(testo.lower())\n",
    "    conteggio_en = len(matches_en)\n",
    "    parole_uniche_en = list(set(matches_en))\n",
    "\n",
    "    return conteggio_it, parole_uniche_it, conteggio_en, parole_uniche_en\n",
    "\n",
    "\n",
    "## 3. Esecuzione e Aggiornamento\n",
    "#    Applichiamo la funzione all'intero DataFrame\n",
    "\n",
    "print(\"Inizio ricalcolo su tutto il DataFrame (pu√≤ richiedere tempo)...\")\n",
    "\n",
    "# 'axis=1' applica la funzione a ogni riga\n",
    "# 'result_type='expand'' divide il risultato della funzione (i 4 valori)\n",
    "# in 4 nuove colonne\n",
    "nuovi_valori = df_swear.apply(ricalcola_parole, axis=1, result_type='expand')\n",
    "\n",
    "nuovi_valori.columns = ['swear_IT_new', 'swear_IT_words_new', 'swear_EN_new', 'swear_EN_words_new']\n",
    "\n",
    "# Confrontiamo le vecchie colonne con le nuove\n",
    "# Usiamo .fillna(0) per i conteggi e .ne() (Not Equal) per le liste\n",
    "\n",
    "# 1. Confronto Conteggi\n",
    "cond_it_count = df_swear['swear_IT'].fillna(0).ne(nuovi_valori['swear_IT_new'])\n",
    "cond_en_count = df_swear['swear_EN'].fillna(0).ne(nuovi_valori['swear_EN_new'])\n",
    "\n",
    "# 2. Confronto Liste\n",
    "# .ne() gestisce correttamente il confronto tra liste e valori NaN/None\n",
    "cond_it_words = df_swear['swear_IT_words'].ne(nuovi_valori['swear_IT_words_new'])\n",
    "cond_en_words = df_swear['swear_EN_words'].ne(nuovi_valori['swear_EN_words_new'])\n",
    "\n",
    "# 3. Condizione Totale: la riga √® cambiata se ALMENO UNO dei 4 campi √® diverso\n",
    "condizione_cambiati = (cond_it_count | cond_en_count | cond_it_words | cond_en_words)\n",
    "\n",
    "# 4. Stampa e Copia (come richiesto)\n",
    "num_cambiati = condizione_cambiati.sum()\n",
    "print(f\"üìä Numero di record che verranno aggiornati: {num_cambiati}\")\n",
    "\n",
    "# Creiamo la copia delle righe *originali* che stanno per cambiare\n",
    "df_swear_copy = df_swear[condizione_cambiati].copy()\n",
    "print(f\"üìã Creato 'df_swear_copy' con {len(df_swear_copy)} record (lo stato *prima* della modifica).\")\n",
    "\n",
    "# Assegna i nuovi valori alle colonne corrette del DataFrame originale\n",
    "# (sovrascrivendo i vecchi dati)\n",
    "df_swear[['swear_IT', 'swear_IT_words', 'swear_EN', 'swear_EN_words']] = nuovi_valori\n",
    "\n",
    "print(\"Ricalcolo completato! Le colonne sono state aggiornate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6d82b",
   "metadata": {},
   "source": [
    "We discovered that the dataset was probably created by starting with a fixed swear words list but this list doesn't include plural and leads to not accurate records both in the swear_word collection, count and the explicit column.\n",
    "We decided to just fix the explicit column according to our swear words counts and not manually insert the plural or the other word forms in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc699959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['conteggio_calcolato', 'conteggio_originale'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34bde8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11166 entries, 0 to 11165\n",
      "Data columns (total 47 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  object        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11061 non-null  category      \n",
      " 5   album                 11161 non-null  object        \n",
      " 6   swear_IT              11166 non-null  int64         \n",
      " 7   swear_EN              11166 non-null  int64         \n",
      " 8   swear_IT_words        11166 non-null  object        \n",
      " 9   swear_EN_words        11166 non-null  object        \n",
      " 10  n_sentences           11090 non-null  float64       \n",
      " 11  n_tokens              11090 non-null  float64       \n",
      " 12  tokens_per_sent       11090 non-null  float64       \n",
      " 13  char_per_tok          11090 non-null  float64       \n",
      " 14  lexical_density       11090 non-null  float64       \n",
      " 15  avg_token_per_clause  11090 non-null  float64       \n",
      " 16  bpm                   11102 non-null  float64       \n",
      " 17  centroid              11102 non-null  float64       \n",
      " 18  rolloff               11102 non-null  float64       \n",
      " 19  flux                  11102 non-null  float64       \n",
      " 20  rms                   11102 non-null  float64       \n",
      " 21  zcr                   11102 non-null  float64       \n",
      " 22  flatness              11102 non-null  float64       \n",
      " 23  spectral_complexity   11102 non-null  float64       \n",
      " 24  pitch                 11102 non-null  float64       \n",
      " 25  loudness              11102 non-null  float64       \n",
      " 26  album_type            11088 non-null  category      \n",
      " 27  disc_number           11088 non-null  Int64         \n",
      " 28  track_number          11088 non-null  Int64         \n",
      " 29  duration_ms           11088 non-null  float64       \n",
      " 30  explicit              11166 non-null  bool          \n",
      " 31  popularity            11088 non-null  Int64         \n",
      " 32  album_image           11088 non-null  string        \n",
      " 33  id_album              11161 non-null  object        \n",
      " 34  lyrics                11163 non-null  string        \n",
      " 35  gender                11166 non-null  category      \n",
      " 36  birth_date            8588 non-null   datetime64[ns]\n",
      " 37  birth_place           8588 non-null   category      \n",
      " 38  nationality           8557 non-null   category      \n",
      " 39  description           10028 non-null  string        \n",
      " 40  active_start          11166 non-null  datetime64[ns]\n",
      " 41  province              8467 non-null   category      \n",
      " 42  region                8024 non-null   category      \n",
      " 43  country               8467 non-null   category      \n",
      " 44  latitude              8588 non-null   float64       \n",
      " 45  longitude             8588 non-null   float64       \n",
      " 46  release_date          11150 non-null  datetime64[ns]\n",
      "dtypes: Int64(3), bool(1), category(8), datetime64[ns](3), float64(19), int64(2), object(7), string(4)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb99ad",
   "metadata": {},
   "source": [
    "## Stat_pageviews\n",
    "\n",
    "Drop due to 60% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb0845fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['stats_pageviews'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b27355",
   "metadata": {},
   "source": [
    "## Music parameters control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca63a5",
   "metadata": {},
   "source": [
    "Due to colineary with other features (rms, rolloff) we decided to drop loudness and zcr columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "465abd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['loudness','zcr'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5955979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>swear_IT</th>\n",
       "      <th>swear_EN</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>tokens_per_sent</th>\n",
       "      <th>char_per_tok</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>avg_token_per_clause</th>\n",
       "      <th>bpm</th>\n",
       "      <th>centroid</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>flux</th>\n",
       "      <th>rms</th>\n",
       "      <th>flatness</th>\n",
       "      <th>spectral_complexity</th>\n",
       "      <th>pitch</th>\n",
       "      <th>disc_number</th>\n",
       "      <th>track_number</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>popularity</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>active_start</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>release_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11166.000000</td>\n",
       "      <td>11166.000000</td>\n",
       "      <td>11090.000000</td>\n",
       "      <td>11090.000000</td>\n",
       "      <td>11090.000000</td>\n",
       "      <td>11090.000000</td>\n",
       "      <td>11090.000000</td>\n",
       "      <td>11090.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11102.000000</td>\n",
       "      <td>11088.0</td>\n",
       "      <td>11088.0</td>\n",
       "      <td>1.108800e+04</td>\n",
       "      <td>11088.0</td>\n",
       "      <td>8588</td>\n",
       "      <td>11166</td>\n",
       "      <td>8588.000000</td>\n",
       "      <td>8588.000000</td>\n",
       "      <td>11150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.343006</td>\n",
       "      <td>0.711177</td>\n",
       "      <td>59.399639</td>\n",
       "      <td>496.891253</td>\n",
       "      <td>8.672152</td>\n",
       "      <td>4.054416</td>\n",
       "      <td>0.514367</td>\n",
       "      <td>8.002009</td>\n",
       "      <td>114.134841</td>\n",
       "      <td>0.137474</td>\n",
       "      <td>1616.965767</td>\n",
       "      <td>1.258893</td>\n",
       "      <td>0.223965</td>\n",
       "      <td>0.859986</td>\n",
       "      <td>27.410247</td>\n",
       "      <td>2256.028782</td>\n",
       "      <td>1.016685</td>\n",
       "      <td>6.859127</td>\n",
       "      <td>2.035293e+05</td>\n",
       "      <td>32.656295</td>\n",
       "      <td>1986-04-19 16:26:26.213320960</td>\n",
       "      <td>2004-03-07 23:56:31.080064512</td>\n",
       "      <td>43.332645</td>\n",
       "      <td>11.352280</td>\n",
       "      <td>2015-10-18 09:51:22.116591872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.970000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.142600e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1967-10-07 00:00:00</td>\n",
       "      <td>1988-01-01 00:00:00</td>\n",
       "      <td>37.747452</td>\n",
       "      <td>7.525403</td>\n",
       "      <td>1992-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>3.866946</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>5.859020</td>\n",
       "      <td>91.910000</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>1222.457200</td>\n",
       "      <td>1.172025</td>\n",
       "      <td>0.186225</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>21.881750</td>\n",
       "      <td>2003.341325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.701310e+05</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1979-10-21 00:00:00</td>\n",
       "      <td>1999-01-01 00:00:00</td>\n",
       "      <td>40.996545</td>\n",
       "      <td>9.189635</td>\n",
       "      <td>2011-09-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>8.404762</td>\n",
       "      <td>4.012709</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>6.764171</td>\n",
       "      <td>106.975000</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>1551.018550</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.882450</td>\n",
       "      <td>27.330850</td>\n",
       "      <td>2241.157450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.966725e+05</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1986-12-01 00:00:00</td>\n",
       "      <td>2004-01-01 00:00:00</td>\n",
       "      <td>44.407260</td>\n",
       "      <td>10.985738</td>\n",
       "      <td>2017-02-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>615.000000</td>\n",
       "      <td>9.431527</td>\n",
       "      <td>4.168686</td>\n",
       "      <td>0.542450</td>\n",
       "      <td>8.090909</td>\n",
       "      <td>134.650000</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>1935.570650</td>\n",
       "      <td>1.346875</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.913075</td>\n",
       "      <td>32.978475</td>\n",
       "      <td>2493.492250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.271840e+05</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1993-02-11 00:00:00</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>45.464194</td>\n",
       "      <td>13.217949</td>\n",
       "      <td>2021-05-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>3089.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>738.270000</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>8635.954200</td>\n",
       "      <td>1.928500</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>61.222500</td>\n",
       "      <td>3993.020300</td>\n",
       "      <td>5.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.753057e+06</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2002-04-29 00:00:00</td>\n",
       "      <td>2022-05-27 00:00:00</td>\n",
       "      <td>45.806691</td>\n",
       "      <td>18.225226</td>\n",
       "      <td>2025-06-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.704831</td>\n",
       "      <td>2.555423</td>\n",
       "      <td>24.711996</td>\n",
       "      <td>209.187612</td>\n",
       "      <td>5.675602</td>\n",
       "      <td>0.445958</td>\n",
       "      <td>0.061583</td>\n",
       "      <td>14.577876</td>\n",
       "      <td>26.827124</td>\n",
       "      <td>0.028131</td>\n",
       "      <td>567.646409</td>\n",
       "      <td>0.137123</td>\n",
       "      <td>0.064592</td>\n",
       "      <td>0.109106</td>\n",
       "      <td>8.441590</td>\n",
       "      <td>382.134853</td>\n",
       "      <td>0.138903</td>\n",
       "      <td>5.191472</td>\n",
       "      <td>8.825641e+04</td>\n",
       "      <td>19.781473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.318645</td>\n",
       "      <td>2.672280</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           swear_IT      swear_EN   n_sentences      n_tokens  \\\n",
       "count  11166.000000  11166.000000  11090.000000  11090.000000   \n",
       "mean       2.343006      0.711177     59.399639    496.891253   \n",
       "min        0.000000      0.000000      1.000000      3.000000   \n",
       "25%        0.000000      0.000000     46.000000    372.000000   \n",
       "50%        1.000000      0.000000     58.000000    491.000000   \n",
       "75%        3.000000      0.000000     73.000000    615.000000   \n",
       "max       72.000000     72.000000    437.000000   3089.000000   \n",
       "std        3.704831      2.555423     24.711996    209.187612   \n",
       "\n",
       "       tokens_per_sent  char_per_tok  lexical_density  avg_token_per_clause  \\\n",
       "count     11090.000000  11090.000000     11090.000000          11090.000000   \n",
       "mean          8.672152      4.054416         0.514367              8.002009   \n",
       "min           1.500000      2.000000         0.000000              0.000000   \n",
       "25%           7.333333      3.866946         0.482353              5.859020   \n",
       "50%           8.404762      4.012709         0.511719              6.764171   \n",
       "75%           9.431527      4.168686         0.542450              8.090909   \n",
       "max         400.000000     12.000000         1.000000            660.000000   \n",
       "std           5.675602      0.445958         0.061583             14.577876   \n",
       "\n",
       "                bpm      centroid       rolloff          flux           rms  \\\n",
       "count  11102.000000  11102.000000  11102.000000  11102.000000  11102.000000   \n",
       "mean     114.134841      0.137474   1616.965767      1.258893      0.223965   \n",
       "min       59.970000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       91.910000      0.119200   1222.457200      1.172025      0.186225   \n",
       "50%      106.975000      0.137200   1551.018550      1.257100      0.229700   \n",
       "75%      134.650000      0.155100   1935.570650      1.346875      0.267600   \n",
       "max      738.270000      0.298200   8635.954200      1.928500      0.621900   \n",
       "std       26.827124      0.028131    567.646409      0.137123      0.064592   \n",
       "\n",
       "           flatness  spectral_complexity         pitch  disc_number  \\\n",
       "count  11102.000000         11102.000000  11102.000000      11088.0   \n",
       "mean       0.859986            27.410247   2256.028782     1.016685   \n",
       "min        0.109400             0.000000      0.000000          1.0   \n",
       "25%        0.841400            21.881750   2003.341325          1.0   \n",
       "50%        0.882450            27.330850   2241.157450          1.0   \n",
       "75%        0.913075            32.978475   2493.492250          1.0   \n",
       "max        1.000000            61.222500   3993.020300          5.0   \n",
       "std        0.109106             8.441590    382.134853     0.138903   \n",
       "\n",
       "       track_number   duration_ms  popularity                     birth_date  \\\n",
       "count       11088.0  1.108800e+04     11088.0                           8588   \n",
       "mean       6.859127  2.035293e+05   32.656295  1986-04-19 16:26:26.213320960   \n",
       "min             1.0  1.142600e+04         0.0            1967-10-07 00:00:00   \n",
       "25%             2.0  1.701310e+05        16.0            1979-10-21 00:00:00   \n",
       "50%             6.0  1.966725e+05        32.0            1986-12-01 00:00:00   \n",
       "75%            10.0  2.271840e+05        47.0            1993-02-11 00:00:00   \n",
       "max            54.0  3.753057e+06       100.0            2002-04-29 00:00:00   \n",
       "std        5.191472  8.825641e+04   19.781473                            NaN   \n",
       "\n",
       "                        active_start     latitude    longitude  \\\n",
       "count                          11166  8588.000000  8588.000000   \n",
       "mean   2004-03-07 23:56:31.080064512    43.332645    11.352280   \n",
       "min              1988-01-01 00:00:00    37.747452     7.525403   \n",
       "25%              1999-01-01 00:00:00    40.996545     9.189635   \n",
       "50%              2004-01-01 00:00:00    44.407260    10.985738   \n",
       "75%              2011-01-01 00:00:00    45.464194    13.217949   \n",
       "max              2022-05-27 00:00:00    45.806691    18.225226   \n",
       "std                              NaN     2.318645     2.672280   \n",
       "\n",
       "                        release_date  \n",
       "count                          11150  \n",
       "mean   2015-10-18 09:51:22.116591872  \n",
       "min              1992-01-01 00:00:00  \n",
       "25%              2011-09-23 00:00:00  \n",
       "50%              2017-02-10 00:00:00  \n",
       "75%              2021-05-14 00:00:00  \n",
       "max              2025-06-13 00:00:00  \n",
       "std                              NaN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b6ac8",
   "metadata": {},
   "source": [
    "BPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c62640",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75cec199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 27.799999999999983\n",
      "Limite superiore: 198.76000000000002\n",
      "Numero di valori NaN in 'bpm': 65\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'bpm'\n",
    "Q1 = df_measures['bpm'].quantile(0.25)\n",
    "Q3 = df_measures['bpm'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'bpm' √® DENTRO i limiti\n",
    "\n",
    "condizione = ((df_measures['bpm'] < limite_inferiore) | (df_measures['bpm'] > limite_superiore))\n",
    "\n",
    "#condizione = ((df_measures['bpm'] < 0.04) | (df_measures['bpm'] > 0.24))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'bpm'] = np.nan\n",
    "nan_count = df_measures['bpm'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'bpm': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a50b17",
   "metadata": {},
   "source": [
    "Centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985ccc3",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218f13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: nan\n",
      "Limite superiore: nan\n",
      "Numero di valori NaN in 'centroid': 11166\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'centroid'\n",
    "Q1 = df_measures['centroid'].quantile(0.25)\n",
    "Q3 = df_measures['centroid'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'centroid' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['centroid'] < limite_inferiore) | (df_measures['centroid'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['centroid'] < 0.04) | (df_measures['centroid'] > 0.24))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'centroid'] = np.nan\n",
    "nan_count = df_measures['centroid'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'centroid': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b492f",
   "metadata": {},
   "source": [
    "observing rolloff boxplot we see visible outliers >6000 and <152, we decided (since a value as 5000 rolloff is plausible) to prune only the impossible rolloff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9ebebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 152.78702499999986\n",
      "Limite superiore: 3005.2408250000003\n",
      "Numero di valori NaN in 'rolloff': 71\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'rolloff'\n",
    "Q1 = df_measures['rolloff'].quantile(0.25)\n",
    "Q3 = df_measures['rolloff'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'rolloff' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['rolloff'] < limite_inferiore) | (df_measures['rolloff'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['rolloff'] < limite_inferiore) | (df_measures['rolloff'] > 6000))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'rolloff'] = np.nan\n",
    "nan_count = df_measures['rolloff'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'rolloff': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eea7c7",
   "metadata": {},
   "source": [
    "Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71a904",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58278216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.9097499999999996\n",
      "Limite superiore: 1.6091500000000003\n",
      "Numero di valori NaN in 'flux': 79\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'flux'\n",
    "Q1 = df_measures['flux'].quantile(0.25)\n",
    "Q3 = df_measures['flux'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'flux' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['flux'] < limite_inferiore) | (df_measures['flux'] > limite_superiore))\n",
    "\n",
    "\n",
    "condizione = ((df_measures['flux'] < 0.75) | (df_measures['flux'] > 1.75))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'flux'] = np.nan\n",
    "nan_count = df_measures['flux'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'flux': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda1de7",
   "metadata": {},
   "source": [
    "RMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fe4c6",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "497d8c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.0641625\n",
      "Limite superiore: 0.3896625\n",
      "Numero di valori NaN in 'rms': 72\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'rms'\n",
    "Q1 = df_measures['rms'].quantile(0.25)\n",
    "Q3 = df_measures['rms'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'rms' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['rms'] < limite_inferiore) | (df_measures['rms'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['rms'] < 0.) | (df_measures['rms'] > 0.43))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'rms'] = np.nan\n",
    "nan_count = df_measures['rms'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'rms': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db9cea",
   "metadata": {},
   "source": [
    "Flatness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060b959",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f6236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.7338875\n",
      "Limite superiore: 1.0205875000000002\n",
      "\n",
      "Numero di righe originali: 11166\n",
      "Numero di righe dopo la rimozione: 11093\n",
      "Righe rimosse: 73\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'flatness'\n",
    "Q1 = df_measures['flatness'].quantile(0.25)\n",
    "Q3 = df_measures['flatness'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'flatness' √® DENTRO i limiti\n",
    "\n",
    "#condizione = ((df_measures['flatness'] < limite_inferiore) | (df_measures['flatness'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['flatness'] < 0.2) | (df_measures['flatness'] > limite_superiore))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'flatness'] = np.nan\n",
    "nan_count = df_measures['flatness'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'flatness': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914694b8",
   "metadata": {},
   "source": [
    "Spectral Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca3dc6",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "507c636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 5.236662499999998\n",
      "Limite superiore: 49.623562500000006\n",
      "Numero di valori NaN in 'spectral_complexity': 66\n"
     ]
    }
   ],
   "source": [
    "df_measures = df\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'spectral_complexity'\n",
    "Q1 = df_measures['spectral_complexity'].quantile(0.25)\n",
    "Q3 = df_measures['spectral_complexity'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'spectral_complexity' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['spectral_complexity'] < limite_inferiore) | (df_measures['spectral_complexity'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['spectral_complexity'] < 0) | (df_measures['spectral_complexity'] > 55))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'spectral_complexity'] = np.nan\n",
    "nan_count = df_measures['spectral_complexity'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'spectral_complexity': {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bebc84",
   "metadata": {},
   "source": [
    "Pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d2d37",
   "metadata": {},
   "source": [
    "observing box plot we deicided to prune the most evident and incoherent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cbd3617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 1268.1149375000005\n",
      "Limite superiore: 3228.718637499999\n",
      "Numero di valori NaN in 'pitch': 69\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'pitch'\n",
    "Q1 = df_measures['pitch'].quantile(0.25)\n",
    "Q3 = df_measures['pitch'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'pitch' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['pitch'] < limite_inferiore) | (df_measures['pitch'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['pitch'] < 1000) | (df_measures['pitch'] > 4000))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'pitch'] = np.nan\n",
    "nan_count = df_measures['pitch'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'pitch': {nan_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
