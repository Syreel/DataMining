{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c1590ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import fasttext\n",
    "import regex as re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1704c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = 'dataset/artists.csv'\n",
    "tracks = 'dataset/tracks.csv'\n",
    "\n",
    "index_col = 0\n",
    "df_artists = pd.read_csv(artists, sep=';', index_col=index_col)\n",
    "df_tracks = pd.read_csv(tracks, index_col=index_col)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca4ffe",
   "metadata": {},
   "source": [
    "## üíæ Optimizing Data Types for Efficiency\n",
    "\n",
    "Before we proceed with cleaning and analysis, it's essential to ensure our DataFrames use the most **memory-efficient and appropriate data types**. Converting low-cardinality string columns (like `gender` and `nationality`) to the **`category`** dtype significantly reduces memory usage.\n",
    "\n",
    "We'll also ensure all date columns are correctly parsed as **`datetime`** objects, and descriptive text fields are designated as the modern **`string`** dtype. For integer columns that contain `NaN` values, we use the nullable integer type **`Int64`**.\n",
    "\n",
    "This step makes subsequent operations faster and more memory-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3bab8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists['gender'] = df_artists['gender'].astype('category')\n",
    "df_artists['nationality'] = df_artists['nationality'].astype('category')\n",
    "df_artists['country'] = df_artists['country'].astype('category')\n",
    "df_artists['region'] = df_artists['region'].astype('category')\n",
    "df_artists['province'] = df_artists['province'].astype('category')\n",
    "df_artists['birth_place'] = df_artists['birth_place'].astype('category')\n",
    "df_artists['birth_date'] = pd.to_datetime(df_artists['birth_date'], errors='coerce')\n",
    "df_artists['active_start'] = pd.to_datetime(df_artists['active_start'], errors='coerce')\n",
    "df_artists['description'] = df_artists['description'].astype('string')\n",
    "df_artists['name'] = df_artists['name'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c51e9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks['id_artist'] = df_tracks['id_artist'].astype('category')\n",
    "df_tracks['id_album'] = df_tracks['id_album'].astype('category')\n",
    "df_tracks['language'] = df_tracks['language'].astype('category')\n",
    "df_tracks['album_type'] = df_tracks['album_type'].astype('category')\n",
    "df_tracks['stats_pageviews'] = df_tracks['stats_pageviews'].astype('Int64')\n",
    "df_tracks['year'] = pd.to_numeric(df_tracks['year'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['month'] = pd.to_numeric(df_tracks['month'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['day'] = pd.to_numeric(df_tracks['day'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['popularity'] = pd.to_numeric(df_tracks['popularity'], errors = 'coerce').astype('Int64')\n",
    "df_tracks['n_sentences'] = df_tracks['n_sentences'].astype('Int64')\n",
    "df_tracks['n_tokens'] = df_tracks['n_tokens'].astype('Int64')\n",
    "df_tracks['disc_number'] = df_tracks['disc_number'].astype('Int64')\n",
    "df_tracks['track_number'] = df_tracks['track_number'].astype('Int64')\n",
    "df_tracks['explicit'] = df_tracks['explicit'].astype('bool')\n",
    "df_tracks['modified_popularity'] = df_tracks['modified_popularity'].astype('bool')\n",
    "df_tracks['album_release_date'] = pd.to_datetime(df_tracks['album_release_date'], errors='coerce')\n",
    "df_tracks['name_artist'] = df_tracks['name_artist'].astype('string')\n",
    "df_tracks['full_title'] = df_tracks['full_title'].astype('string')\n",
    "df_tracks['title'] = df_tracks['title'].astype('string')\n",
    "df_tracks['featured_artists'] = df_tracks['featured_artists'].astype('string')\n",
    "df_tracks['primary_artist'] = df_tracks['primary_artist'].astype('string')\n",
    "df_tracks['album_name'] = df_tracks['album_name'].astype('string')\n",
    "df_tracks['album'] = df_tracks['album'].astype('string')\n",
    "df_tracks['album_image'] = df_tracks['album_image'].astype('string')\n",
    "df_tracks['lyrics'] = df_tracks['lyrics'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "482f10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # Import the Abstract Syntax Tree module for safe evaluation\n",
    "\n",
    "# Assuming your DataFrame is df_tracks and it's already loaded\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"\n",
    "    Safely converts a string representation of a list into a Python list.\n",
    "    Handles NaN/missing values by returning an empty list or pd.NA.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value in (None, 'NaN', ''):\n",
    "        # Return an empty list for missing values if you plan to iterate over it\n",
    "        return []\n",
    "    try:\n",
    "        # Use ast.literal_eval for safe conversion of string-to-list\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Handle cases where the string is malformed or not a list structure\n",
    "        print(f\"Warning: Could not convert value: {value}\")\n",
    "        return [] # Default to empty list on failure\n",
    "\n",
    "# Apply the conversion to both columns\n",
    "df_tracks['swear_IT_words'] = df_tracks['swear_IT_words'].apply(safe_literal_eval)\n",
    "df_tracks['swear_EN_words'] = df_tracks['swear_EN_words'].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab69d5",
   "metadata": {},
   "source": [
    "## üïµÔ∏è Data Validation: Checking and Correcting Primary Key Duplicates\n",
    "\n",
    "For the data preparation phase, we start by performing a crucial check of the primary IDs for rows in both our DataFrames to check for potential duplicates. Ensuring unique identifiers is **foundational** for reliable joins and accurate analysis later on. \n",
    "\n",
    "A formal review of the primary ID columns yielded the following observations:\n",
    "\n",
    "* **`df_tracks`**: Inspection of the track ID column revealed **73 instances of duplicated identifiers**. To guarantee that each record is uniquely identifiable and to maintain the principle of one-to-one entity mapping, these duplicated rows will be managed immediately. IDs are of the format $\\text{TR\\#\\#\\#\\#\\#\\#}$, so we generate new IDs compliant with this format to replace duplicated ones.\n",
    "* **`df_artists`**: The artist ID column was found to be **entirely sound**, presenting no instances of duplicate IDs. Consequently, no corrective action is required for this DataFrame regarding its primary keys.\n",
    "\n",
    "The code below first validates the counts, displays a sample of the duplicates, and then executes the custom logic to **generate unique, non-colliding IDs** to replace the duplicated indices in `df_tracks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "776a97e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate index for tracks: 73\n",
      "number of duplicate index for artists: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Creare un set di tutti gli ID esistenti per un controllo rapido\n",
    "existing_tracks_ids = set(df_tracks.index)\n",
    "existing_artists_ids = set(df_artists.index)\n",
    "\n",
    "# 2. Identificare le posizioni (indice booleano) degli indici duplicati.\n",
    "#    Usiamo keep='first' per segnare solo la seconda, terza, ecc. occorrenza.\n",
    "duplicate_mask_tracks = df_tracks.index.duplicated()\n",
    "duplicate_mask_artists = df_artists.index.duplicated()\n",
    "num_duplicates_tracks = duplicate_mask_tracks.sum()\n",
    "num_duplicates_artists = duplicate_mask_artists.sum()\n",
    "print(\"number of duplicate index for tracks:\", num_duplicates_tracks)\n",
    "print(\"number of duplicate index for artists:\", num_duplicates_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "94ee225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostro tutte le righe che hanno un indice duplicato, ordinate per ID:\n",
      "            id_artist      name_artist  \\\n",
      "id                                       \n",
      "TR108862  ART56320683    Bassi Maestro   \n",
      "TR108862  ART07127070      Noyz Narcos   \n",
      "TR135764  ART73965015           Ghemon   \n",
      "TR135764  ART86549066       Emis Killa   \n",
      "TR190585  ART78209349             Coez   \n",
      "TR190585  ART66932389           Piotta   \n",
      "TR192351  ART81071062        Club Dogo   \n",
      "TR192351  ART88792008    Jake La Furia   \n",
      "TR205970  ART80977821  Jack The Smoker   \n",
      "TR205970  ART08456301          Rancore   \n",
      "\n",
      "                                                 full_title  \\\n",
      "id                                                            \n",
      "TR108862                         Sushi Bar by¬†Bassi¬†Maestro   \n",
      "TR108862                    SPINE by¬†Noyz¬†Narcos (Ft.¬†Coez)   \n",
      "TR135764                   Nessuno vale quanto te by¬†Ghemon   \n",
      "TR135764          Giovani eroi by¬†Emis¬†Killa (Ft.¬†Not¬†Good)   \n",
      "TR190585     Nei treni la notte by¬†Coez (Ft.¬†Frah¬†Quintale)   \n",
      "TR190585                Serpico by¬†Piotta (Ft.¬†Tiromancino)   \n",
      "TR192351        Torner√≤ Da Re - Redrum Version by¬†Club¬†Dogo   \n",
      "TR192351                Musica Commerciale by¬†Jake¬†La Furia   \n",
      "TR205970  24.7 by¬†Jack¬†The Smoker (Ft.¬†Bassi¬†Maestro & Gu√®)   \n",
      "TR205970               Sigla Catteland by¬†Rancore & DJ¬†Myke   \n",
      "\n",
      "                                   title    featured_artists   primary_artist  \\\n",
      "id                                                                              \n",
      "TR108862                       Sushi Bar                <NA>    Bassi Maestro   \n",
      "TR108862                           SPINE                Coez      Noyz Narcos   \n",
      "TR135764          Nessuno vale quanto te                <NA>           Ghemon   \n",
      "TR135764                    Giovani eroi            Not Good       Emis Killa   \n",
      "TR190585              Nei treni la notte       Frah Quintale             Coez   \n",
      "TR190585                         Serpico         Tiromancino           Piotta   \n",
      "TR192351  Torner√≤ Da Re - Redrum Version                <NA>        Club Dogo   \n",
      "TR192351              Musica Commerciale                <NA>    Jake La Furia   \n",
      "TR205970                            24.7  Bassi Maestro, Gu√®  Jack The Smoker   \n",
      "TR205970                 Sigla Catteland                <NA>          Rancore   \n",
      "\n",
      "         language                              album  stats_pageviews  \\\n",
      "id                                                                      \n",
      "TR108862       en                           Sushi EP             <NA>   \n",
      "TR108862       cs                     VIRUS (Deluxe)            19050   \n",
      "TR135764       it                           ORCHIdee             7182   \n",
      "TR135764       it                 Keta Music, Vol. 3            13155   \n",
      "TR190585       it                 From The Rooftop 2             <NA>   \n",
      "TR190585       pl                   ‚Äôna notte infame             <NA>   \n",
      "TR192351       it     Vile Denaro - 10th Anniversary             <NA>   \n",
      "TR192351       it  Musica Commerciale Deluxe Edition            10684   \n",
      "TR205970       it                              V.Ita             <NA>   \n",
      "TR205970       it                               <NA>             <NA>   \n",
      "\n",
      "          swear_IT  swear_EN                                 swear_IT_words  \\\n",
      "id                                                                            \n",
      "TR108862         8         1          [bastardo, cesso, culo, fesso, merda]   \n",
      "TR108862         1         0                                      [scopare]   \n",
      "TR135764         0         0                                             []   \n",
      "TR135764         5         1                [bastardo, cazzo, merda, troia]   \n",
      "TR190585         0         0                                             []   \n",
      "TR190585         0         0                                             []   \n",
      "TR192351         1         0                                         [culo]   \n",
      "TR192351         8         0  [cagare, cazzo, culo, figo, incazzare, merda]   \n",
      "TR205970         4         0                                  [figa, merda]   \n",
      "TR205970         0         0                                             []   \n",
      "\n",
      "         swear_EN_words  year  month   day  n_sentences  n_tokens  \\\n",
      "id                                                                  \n",
      "TR108862     [bastardo]  2007     11    13           67       610   \n",
      "TR108862             []  1932      1    14           36       306   \n",
      "TR135764             []  2068      5    27           72       466   \n",
      "TR135764     [bastardo]  2021      7    23           52       584   \n",
      "TR190585             []  2022     10    14           44       295   \n",
      "TR190585             []  2024      3     1           69       488   \n",
      "TR192351             []  2017      5    19           86       683   \n",
      "TR192351             []  2013     10    29           36       534   \n",
      "TR205970             []  1910     10    10           48       490   \n",
      "TR205970             []  2013   <NA>  <NA>           20       148   \n",
      "\n",
      "          tokens_per_sent  char_per_tok  lexical_density  \\\n",
      "id                                                         \n",
      "TR108862         9.104478      3.707980         0.490662   \n",
      "TR108862         8.500000      3.806691         0.542751   \n",
      "TR135764         6.472222      4.305687         0.473934   \n",
      "TR135764        11.230769      3.889558         0.487952   \n",
      "TR190585         6.704545      4.256140         0.498246   \n",
      "TR190585         7.072464      4.042254         0.530516   \n",
      "TR192351         7.941860      3.748103         0.449165   \n",
      "TR192351        14.833333      3.880503         0.496855   \n",
      "TR205970        10.208333      3.733766         0.465368   \n",
      "TR205970         7.400000      4.574627         0.537313   \n",
      "\n",
      "          avg_token_per_clause     bpm  centroid    rolloff    flux     rms  \\\n",
      "id                                                                            \n",
      "TR108862              5.922330   89.98    0.1302  1262.0061  1.4183  0.1970   \n",
      "TR108862              6.120000   84.97    0.1069  1307.9852  1.1196  0.2700   \n",
      "TR135764              6.383562   92.05    0.1355  1222.7497  1.1038  0.2144   \n",
      "TR135764              7.121951  165.58    0.1434  1286.8483  1.3029  0.3015   \n",
      "TR190585              6.704545  135.20    0.1077  1099.6117  0.9851  0.1700   \n",
      "TR190585              7.176471   91.96    0.1360  1091.7072  1.1547  0.2306   \n",
      "TR192351              5.598361   83.97    0.2028  1803.5711  1.3480  0.2932   \n",
      "TR192351              5.621053  128.19    0.1353  1436.6329  1.3272  0.2598   \n",
      "TR205970              6.282051   86.06    0.1573  1764.7418  1.2966  0.2984   \n",
      "TR205970              6.727273   98.03    0.1513  1809.8772  1.1953  0.2594   \n",
      "\n",
      "             zcr  flatness  spectral_complexity      pitch  loudness  \\\n",
      "id                                                                     \n",
      "TR108862  0.0527    0.9319              22.1130  2356.4160   20.0195   \n",
      "TR108862  0.0524    0.9056              29.6680  2132.3250   30.8700   \n",
      "TR135764  0.0568    0.9251              28.2788  1896.4159   22.2053   \n",
      "TR135764  0.0564    0.9067              35.7936  2132.3474   34.1819   \n",
      "TR190585  0.0447    0.8918              17.3859  1709.2517   16.3989   \n",
      "TR190585  0.0508    0.9327              23.5366  2503.3810   24.9774   \n",
      "TR192351  0.0721    0.8205              24.8264  2876.5173   33.8417   \n",
      "TR192351  0.0574    0.8673              30.2980  2236.3089   29.5758   \n",
      "TR205970  0.0697    0.9016              40.1808  2170.3874   33.8672   \n",
      "TR205970  0.0718    0.8230              42.1591  2191.6551   28.7624   \n",
      "\n",
      "                            album_name album_release_date album_type  \\\n",
      "id                                                                     \n",
      "TR108862                    Sushi - EP         2007-11-13     single   \n",
      "TR108862                         VIRUS         2022-01-14      album   \n",
      "TR135764                      ORCHIdee         2014-05-27      album   \n",
      "TR135764            Keta Music, Vol. 3         2021-07-23      album   \n",
      "TR190585            From The Rooftop 2         2022-10-14      album   \n",
      "TR190585              'na notte infame         2024-03-01      album   \n",
      "TR192351  Vile Denaro 10th Anniversary         2007-05-17      album   \n",
      "TR192351            Musica Commerciale         2013-01-01      album   \n",
      "TR205970                         V.Ita         2009-10-10      album   \n",
      "TR205970            Musica per bambini         2018-06-01      album   \n",
      "\n",
      "          disc_number  track_number  duration_ms  explicit  popularity  \\\n",
      "id                                                                       \n",
      "TR108862            1             3     215461.0      True           6   \n",
      "TR108862            1             6     155294.0      True          44   \n",
      "TR135764            1             8     238773.0     False          26   \n",
      "TR135764            1            10     152733.0      True          35   \n",
      "TR190585            1             3     181789.0     False          43   \n",
      "TR190585            1             2     192994.0     False          21   \n",
      "TR192351            2             4     244226.0     False           9   \n",
      "TR192351            1             1     163517.0     False          39   \n",
      "TR205970            1             6     240053.0      True          13   \n",
      "TR205970            1             8     279213.0     False          42   \n",
      "\n",
      "                                                album_image   id_album  \\\n",
      "id                                                                       \n",
      "TR108862  https://i.scdn.co/image/ab67616d0000b2734311be...  ALB697589   \n",
      "TR108862  https://i.scdn.co/image/ab67616d0000b273cad459...  ALB525038   \n",
      "TR135764  https://i.scdn.co/image/ab67616d0000b273f7338f...  ALB346809   \n",
      "TR135764  https://i.scdn.co/image/ab67616d0000b27361a8db...  ALB168242   \n",
      "TR190585  https://i.scdn.co/image/ab67616d0000b273f4c5be...  ALB760031   \n",
      "TR190585  https://i.scdn.co/image/ab67616d0000b2735e9652...  ALB996374   \n",
      "TR192351  https://i.scdn.co/image/ab67616d0000b27357cfe6...  ALB390480   \n",
      "TR192351  https://i.scdn.co/image/ab67616d0000b273d49501...  ALB145179   \n",
      "TR205970  https://i.scdn.co/image/ab67616d0000b273418414...  ALB704296   \n",
      "TR205970  https://i.scdn.co/image/ab67616d0000b2736545b2...  ALB599065   \n",
      "\n",
      "                                                     lyrics  \\\n",
      "id                                                            \n",
      "TR108862  Questo mondo resta freddo anche se vivi da sta...   \n",
      "TR108862  Sei al centro del mio cuore come 'na spina\\nSe...   \n",
      "TR135764  Dicono che da un posto piccolo non pu√≤ venire ...   \n",
      "TR135764  Oh, la city √® silenziosa, in zona solo un clac...   \n",
      "TR190585  Ho fatto un giro in questa citt√†\\nEd √® come fa...   \n",
      "TR190585  Se c'avessi diciott'anni\\nCo quer fuoco che c'...   \n",
      "TR192351  Quando ritorner√≤ da te\\nIo ci ritorner√≤ da re\\...   \n",
      "TR192351  Permettete una parola che √® da un po' che ho n...   \n",
      "TR205970  Vivo questa roba, scrivo della merda che ti sv...   \n",
      "TR205970  Stai su Deejay, su Radio Deejay\\nQuesto √® un a...   \n",
      "\n",
      "          modified_popularity  \n",
      "id                             \n",
      "TR108862                False  \n",
      "TR108862                False  \n",
      "TR135764                False  \n",
      "TR135764                False  \n",
      "TR190585                False  \n",
      "TR190585                False  \n",
      "TR192351                False  \n",
      "TR192351                False  \n",
      "TR205970                False  \n",
      "TR205970                False  \n"
     ]
    }
   ],
   "source": [
    "# 1. Creare una maschera per identificare TUTTE le righe (inclusa la prima)\n",
    "#    che hanno un indice duplicato.\n",
    "all_duplicates_mask = df_tracks.index.duplicated(keep=False)\n",
    "\n",
    "# 2. Filtrare il DataFrame per ottenere solo queste righe\n",
    "df_duplicate_groups = df_tracks[all_duplicates_mask]\n",
    "\n",
    "# 3. Ordinare per indice. Questo √® fondamentale per vedere\n",
    "#    le righe con lo stesso indice una accanto all'altra.\n",
    "df_duplicate_groups_sorted = df_duplicate_groups.sort_index()\n",
    "\n",
    "# 4. Stampare i gruppi di duplicati\n",
    "if not df_duplicate_groups_sorted.empty:\n",
    "    print(\"Mostro tutte le righe che hanno un indice duplicato, ordinate per ID:\")\n",
    "    # Stampiamo le prime 30 (o modifica il numero se vuoi vederne di pi√π)\n",
    "    print(df_duplicate_groups_sorted.head(10))\n",
    "else:\n",
    "    # Questo scenario si verifica se num_duplicates (dal tuo codice) era 0\n",
    "    print(\"Nessuna riga con indice duplicato trovata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6970c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "Generating 73 random unique IDs...\n",
      "Finished generating unique IDs.\n",
      "\n",
      "Generated 73 new unique IDs.\n",
      "Example new ID: TR555758\n",
      "Check for duplicates after replacement: False\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Identify Duplicated Rows and Count ---\n",
    "# Find the boolean mask for rows where the ID (index) is duplicated,\n",
    "# keeping only the duplicates *after* the first occurrence.\n",
    "duplicated_mask = df_tracks.index.duplicated(keep='first')\n",
    "num_duplicates_to_replace = duplicated_mask.sum() # Should be 73\n",
    "print(num_duplicates_to_replace)\n",
    "\n",
    "# --- 2. Define ID Generation Helper ---\n",
    "def format_track_id(number, prefix='TR', padding=6):\n",
    "    \"\"\"Formats a number into a TRXXXXXX string.\"\"\"\n",
    "    # Uses f-string formatting to zero-pad the number to 6 digits\n",
    "    return f\"{prefix}{number:0{padding}d}\"\n",
    "\n",
    "# --- 3. Generate New Unique IDs with Collision Check ---\n",
    "\n",
    "# Convert the existing index to a set for O(1) average time complexity lookups\n",
    "existing_ids = set(df_tracks.index)\n",
    "new_track_ids = []\n",
    "\n",
    "# Range for 6-digit numbers (000000 to 999999)\n",
    "MIN_ID = 0\n",
    "MAX_ID = 999999 \n",
    "\n",
    "print(f\"Generating {num_duplicates_to_replace} random unique IDs...\")\n",
    "\n",
    "while len(new_track_ids) < num_duplicates_to_replace:\n",
    "    # Generate a random 6-digit number\n",
    "    random_num = random.randint(MIN_ID, MAX_ID)\n",
    "    \n",
    "    # Format it to the \"TRXXXXXX\" string\n",
    "    new_id = format_track_id(random_num)\n",
    "    \n",
    "    # Check for collision against all existing IDs\n",
    "    if new_id not in existing_ids:\n",
    "        new_track_ids.append(new_id)\n",
    "        # Immediately add the new ID to the existing_ids set to prevent\n",
    "        # generating the same random ID twice during this loop\n",
    "        existing_ids.add(new_id)\n",
    "\n",
    "print(\"Finished generating unique IDs.\")\n",
    "\n",
    "# --- 4. Replace Duplicated IDs in the DataFrame Index ---\n",
    "\n",
    "# Get the actual index values that need to be replaced (the index values of the duplicated rows)\n",
    "indices_to_replace = df_tracks.index[duplicated_mask]\n",
    "\n",
    "# Create a Series of the new IDs, matching the indices (positions) of the duplicated rows\n",
    "new_ids_series = pd.Series(\n",
    "    new_track_ids,\n",
    "    index=indices_to_replace\n",
    ")\n",
    "\n",
    "# Replace the duplicated index values in-place\n",
    "df_tracks.index.values[duplicated_mask] = new_ids_series.values\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"\\nGenerated {len(new_track_ids)} new unique IDs.\")\n",
    "print(f\"Example new ID: {new_track_ids[0]}\")\n",
    "print(f\"Check for duplicates after replacement: {df_tracks.index.duplicated().any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2032d",
   "metadata": {},
   "source": [
    "## üßπ Removing Redundant Artist Columns\n",
    "\n",
    "We discovered that **`name_artist`**, **`name`**, and **`primary_artist`** all highlight the same information, creating unnecessary redundancy in our dataset. To determine which columns to keep, we performed a thorough **normalization and comparison analysis**.\n",
    "\n",
    "After joining the tracks and artists DataFrames, we implemented a **helper function** to normalize all artist-related string columns. This normalization process includes:\n",
    "- Converting to lowercase\n",
    "- Removing accents (e.g., '√®' ‚Üí 'e')  \n",
    "- Stripping special characters\n",
    "- Trimming whitespace\n",
    "\n",
    "We applied this normalization to **`name`**, **`primary_artist`**, **`name_artist`**, and **`featured_artists`** to ensure a fair comparison. Our analysis revealed that **`primary_artist`** and **`name_artist`** are *identical* after normalization, while **`name`** contains the same unique values but with a slightly altered version for some artists, hence still being redundant.\n",
    "\n",
    "We also checked for **self-titled tracks** (where the track name matches the artist name) and examined edge cases like featured artists. Based on these findings, we confidently **dropped** the redundant **`name`** and **`primary_artist`** columns, retaining only **`name_artist`** as this column (equivalent to **`primary_artist`**) matches the same version of how the artist name is written in **`featured_artists`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "09fbdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tracks.join(df_artists, on='id_artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1dcf5203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11166 entries, TR934808 to TR552777\n",
      "Data columns (total 57 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  string        \n",
      " 2   full_title            11166 non-null  string        \n",
      " 3   title                 11166 non-null  string        \n",
      " 4   featured_artists      3517 non-null   string        \n",
      " 5   primary_artist        11166 non-null  string        \n",
      " 6   language              11061 non-null  category      \n",
      " 7   album                 9652 non-null   string        \n",
      " 8   stats_pageviews       4642 non-null   Int64         \n",
      " 9   swear_IT              11166 non-null  int64         \n",
      " 10  swear_EN              11166 non-null  int64         \n",
      " 11  swear_IT_words        11166 non-null  object        \n",
      " 12  swear_EN_words        11166 non-null  object        \n",
      " 13  year                  10728 non-null  Int64         \n",
      " 14  month                 9969 non-null   Int64         \n",
      " 15  day                   9843 non-null   Int64         \n",
      " 16  n_sentences           11090 non-null  Int64         \n",
      " 17  n_tokens              11090 non-null  Int64         \n",
      " 18  tokens_per_sent       11090 non-null  float64       \n",
      " 19  char_per_tok          11090 non-null  float64       \n",
      " 20  lexical_density       11090 non-null  float64       \n",
      " 21  avg_token_per_clause  11090 non-null  float64       \n",
      " 22  bpm                   11102 non-null  float64       \n",
      " 23  centroid              11102 non-null  float64       \n",
      " 24  rolloff               11102 non-null  float64       \n",
      " 25  flux                  11102 non-null  float64       \n",
      " 26  rms                   11102 non-null  float64       \n",
      " 27  zcr                   11102 non-null  float64       \n",
      " 28  flatness              11102 non-null  float64       \n",
      " 29  spectral_complexity   11102 non-null  float64       \n",
      " 30  pitch                 11102 non-null  float64       \n",
      " 31  loudness              11102 non-null  float64       \n",
      " 32  album_name            11088 non-null  string        \n",
      " 33  album_release_date    10827 non-null  datetime64[ns]\n",
      " 34  album_type            11088 non-null  category      \n",
      " 35  disc_number           11088 non-null  Int64         \n",
      " 36  track_number          11088 non-null  Int64         \n",
      " 37  duration_ms           11088 non-null  float64       \n",
      " 38  explicit              11166 non-null  bool          \n",
      " 39  popularity            11137 non-null  Int64         \n",
      " 40  album_image           11088 non-null  string        \n",
      " 41  id_album              11088 non-null  category      \n",
      " 42  lyrics                11163 non-null  string        \n",
      " 43  modified_popularity   11166 non-null  bool          \n",
      " 44  name                  11166 non-null  string        \n",
      " 45  gender                11166 non-null  category      \n",
      " 46  birth_date            8588 non-null   datetime64[ns]\n",
      " 47  birth_place           8588 non-null   category      \n",
      " 48  nationality           8557 non-null   category      \n",
      " 49  description           10028 non-null  string        \n",
      " 50  active_start          6565 non-null   datetime64[ns]\n",
      " 51  active_end            0 non-null      float64       \n",
      " 52  province              8467 non-null   category      \n",
      " 53  region                8024 non-null   category      \n",
      " 54  country               8467 non-null   category      \n",
      " 55  latitude              8588 non-null   float64       \n",
      " 56  longitude             8588 non-null   float64       \n",
      "dtypes: Int64(9), bool(2), category(9), datetime64[ns](3), float64(18), int64(2), object(3), string(11)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "91a7e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(series):\n",
    "    # CRITICAL CHANGE 1: Avoid using .astype(str) if possible, \n",
    "    # but if you must, ensure the dtype is 'string' which handles NaNs correctly.\n",
    "    # If the original series is 'string' dtype, str accessor methods preserve NA/NaN.\n",
    "    # However, since you are mixing pandas string methods with encoding (which often\n",
    "    # forces conversion back to object), let's stick to the explicit logic.\n",
    "    \n",
    "    # 1. Start with the original series, handle cleaning first\n",
    "    s = series.str.lower()\n",
    "    \n",
    "    # 2. Rimuove accenti e caratteri non-ascii (these typically propagate NaN/NA correctly)\n",
    "    s = s.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    \n",
    "    # 3. Rimuove caratteri speciali (this step can convert NA/NaN to '')\n",
    "    s = s.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    \n",
    "    # 4. Rimuove spazi extra all'inizio/fine e sostituisce multipli con uno singolo\n",
    "    s = s.str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # 5. CRITICAL STEP: Convert back all missing markers\n",
    "    #    We check for three possibilities created during cleaning:\n",
    "    #    a) The original NA/NaN value that was converted to the string 'nan' by some internal step.\n",
    "    #    b) The resulting empty string ''\n",
    "    #    c) A string containing only a single space ' ' (if strip() failed for some reason)\n",
    "    s = s.replace('nan', pd.NA)  # FIX 1: Convert the string literal \"nan\" back to NA\n",
    "    s = s.replace('', pd.NA)     # FIX 2: Convert empty strings back to NA\n",
    "    s = s.replace(' ', pd.NA)    # FIX 3: Convert single-space strings back to NA\n",
    "    \n",
    "    # Final step: Ensure the final output dtype is clean\n",
    "    return s.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1f5f1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applichiamo la normalizzazione alle tre colonne\n",
    "df['name'] = normalize_series(df['name'])\n",
    "df['primary_artist'] = normalize_series(df['primary_artist'])\n",
    "df['name_artist'] = normalize_series(df['name_artist'])\n",
    "df['featured_artists'] = normalize_series(df['featured_artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "03edf8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi: 'primary_artist' e 'name_artist' sono sempre identici dopo la normalizzazione.\n"
     ]
    }
   ],
   "source": [
    "# Controlla se le due colonne sono SEMPRE identiche\n",
    "are_artists_identical = (df['primary_artist'] == df['name_artist']).all()\n",
    "\n",
    "if are_artists_identical:\n",
    "    print(\"Analisi: 'primary_artist' e 'name_artist' sono sempre identici dopo la normalizzazione.\")\n",
    "else:\n",
    "    print(\"Analisi: 'primary_artist' e 'name_artist' NON sono sempre identici.\")\n",
    "    \n",
    "    diff_df = df[df['primary_artist'] != df['name_artist']]\n",
    "    print(diff_df[['primary_artist', 'name_artist', 'primary_artist', 'name_artist']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fd05ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name primary_artist\n",
      "id                                   \n",
      "TR317207   gue pequeno            gue\n",
      "TR446826   gue pequeno            gue\n",
      "TR228275   gue pequeno            gue\n",
      "TR697556   gue pequeno            gue\n",
      "TR391415   gue pequeno            gue\n",
      "...                ...            ...\n",
      "TR794750  samuel heron   samuel costa\n",
      "TR102539  samuel heron   samuel costa\n",
      "TR178809   joey funboy       joey ita\n",
      "TR589443   joey funboy       joey ita\n",
      "TR735987   joey funboy       joey ita\n",
      "\n",
      "[870 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cerca tracce omonime (dove il nome della traccia √® uguale al nome dell'artista)\n",
    "self_titled_tracks = df[df['name'] != df['primary_artist']]\n",
    "print(self_titled_tracks[['name', 'primary_artist']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7cf3beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['name', 'primary_artist'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fbd38",
   "metadata": {},
   "source": [
    "Active_end column is completely empty so we can drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2707cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['active_end'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f54aac",
   "metadata": {},
   "source": [
    "## üîç Analyzing `full_title`\n",
    "\n",
    "### `full_title` vs `title` Redundancy\n",
    "\n",
    "The **`full_title`** and **`title`** attributes should theoretically correspond, as both identify the track's name. However, **`full_title`** contains additional information by appending the performer with **\"by (artist_name)\"** and featuring artists with **\"Ft. (featured_artists)\"**.\n",
    "\n",
    "This explains why **`full_title`** has more unique values compared to **`title`**. However, by examining the actual title name contained in the first portion of **`full_title`**, we notice that the two columns do in fact correspond to the same underlying track name.\n",
    "\n",
    "We performed a **regex-based extraction and normalization** to verify this relationship holds across *all* records. The process involved:\n",
    "\n",
    "- **Extracting** the title portion from **`full_title`** by splitting at the last occurrence of `\" by\"`\n",
    "- **Normalizing smart quotes and apostrophes** (e.g., `'` ‚Üí `'`, `\"` ‚Üí `\"`) to handle encoding differences\n",
    "- **Standardizing whitespace** by stripping leading/trailing spaces and collapsing multiple spaces into one\n",
    "\n",
    "After these comprehensive normalization steps, we confirmed that the extracted title from **`full_title`** is *identical* to **`title`** across all rows. This allows us to confidently **discard** one of the two columns, eliminating redundancy while preserving complete information.\n",
    "\n",
    "This verification ensures data integrity and simplifies our schema for future analysis. ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "66840002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(series):\n",
    "    \"\"\"\n",
    "    Normalize a pandas Series containing titles by:\n",
    "    - Replacing smart quotes/apostrophes with straight ones\n",
    "    - Stripping leading/trailing whitespace\n",
    "    - Collapsing multiple spaces into single spaces\n",
    "    \"\"\"\n",
    "    # Normalize smart apostrophes\n",
    "    s = series.str.replace('‚Äô', \"'\", regex=False)\n",
    "    s = s.str.replace('‚Äò', \"'\", regex=False)\n",
    "\n",
    "    # Normalize smart double quotes\n",
    "    s = s.str.replace('‚Äú', '\"', regex=False)\n",
    "    s = s.str.replace('‚Äù', '\"', regex=False)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    s = s.str.strip()\n",
    "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    return s.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2af45c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the columns equal after normalization? True\n",
      "Number of rows still unequal: 0\n"
     ]
    }
   ],
   "source": [
    "df_title = df_tracks[['full_title', 'title']].copy()\n",
    "\n",
    "# Extract title portion from full_title\n",
    "split_series = df_title['full_title'].str.rsplit(' by', n=1)\n",
    "df_title['cleaned_attribute'] = split_series.str[0]\n",
    "\n",
    "# Apply normalization to both columns\n",
    "df_title['cleaned_attribute'] = normalize_title(df_title['cleaned_attribute'])\n",
    "df_title['title'] = normalize_title(df_title['title'])\n",
    "\n",
    "# Compare results\n",
    "are_columns_equal_final = (df_title['cleaned_attribute'] == df_title['title']).all()\n",
    "print(f\"Are the columns equal after normalization? {are_columns_equal_final}\")\n",
    "\n",
    "# Check the remaining mismatched rows (should now be 0)\n",
    "final_mismatched_rows = df_title[df_title['cleaned_attribute'] != df_title['title']]\n",
    "print(f\"Number of rows still unequal: {len(final_mismatched_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ec2c7",
   "metadata": {},
   "source": [
    "## üéµ Validating Artist and Featured Artists from `full_title`\n",
    "\n",
    "Having established that **`full_title`** contains redundant information about track titles, we now investigate whether the **artist** and **featured artists** information embedded in **`full_title`** matches the dedicated columns **`name_artist`** and **`featured_artists`**.\n",
    "\n",
    "The **`full_title`** follows the pattern: `\"Track Name by Artist (Ft. Featured Artists)\"` or `\"Track Name by Artist, Featured Artist 1, ... & Featured Artist N\"`. We perform a **multi-step extraction and normalization** process to validate this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d6cb61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the artist names equal? True\n",
      "Number of rows still unequal: 0\n"
     ]
    }
   ],
   "source": [
    "df_artist_and_feat = df_tracks[['full_title', 'name_artist', 'featured_artists']].copy()\n",
    "\n",
    "df_artist_and_feat['name_artist'] = normalize_title(df_artist_and_feat['name_artist'])\n",
    "df_artist_and_feat['featured_artists'] = normalize_title(df_artist_and_feat['featured_artists'])\n",
    "df_artist_and_feat['full_title'] = normalize_title(df_artist_and_feat['full_title'])\n",
    "\n",
    "# --- Step 1: Extract 'artist_and_feat' (Artist + Features) ---\n",
    "split_series_1 = df_artist_and_feat['full_title'].str.rsplit(' by', n=1)\n",
    "df_artist_and_feat['artist_and_feat'] = split_series_1.str[1]\n",
    "df_artist_and_feat.drop(columns=['full_title'], inplace=True)\n",
    "\n",
    "# --- Step 2: Separate 'cleaned_artist' from 'cleaned_feat' ---\n",
    "split_series_2 = df_artist_and_feat['artist_and_feat'].str.rsplit('(Ft.', n=1)\n",
    "df_artist_and_feat['cleaned_artist'] = split_series_2.str[0]\n",
    "df_artist_and_feat['cleaned_feat'] = split_series_2.str[1].str.replace(r'\\)$', '', regex=True)\n",
    "\n",
    "# --- NEW: Convert & to , BEFORE extracting the artist ---\n",
    "df_artist_and_feat['cleaned_artist'] = df_artist_and_feat['cleaned_artist'].str.replace('&', ',', regex=False)\n",
    "\n",
    "# --- Extract primary artist by splitting at the FIRST comma ---\n",
    "split_series_3 = df_artist_and_feat['cleaned_artist'].str.split(',', n=1)\n",
    "df_artist_and_feat['cleaned_artist'] = split_series_3.str[0]\n",
    "\n",
    "# The remaining artists after the first comma become features\n",
    "remaining_artists = split_series_3.str[1]\n",
    "\n",
    "# --- Move remaining artists to 'cleaned_feat' if '(Ft....)' was empty ---\n",
    "mask_empty_feat = df_artist_and_feat['cleaned_feat'].isna() | (df_artist_and_feat['cleaned_feat'].str.strip() == '')\n",
    "\n",
    "df_artist_and_feat['cleaned_feat'] = df_artist_and_feat['cleaned_feat'].mask(\n",
    "    mask_empty_feat,\n",
    "    remaining_artists.fillna('').str.strip()\n",
    ")\n",
    "\n",
    "# --- Strip whitespace from cleaned_artist ---\n",
    "df_artist_and_feat['cleaned_artist'] = df_artist_and_feat['cleaned_artist'].str.strip()\n",
    "\n",
    "# --- Final Comparison ---\n",
    "are_names_equal_final = (df_artist_and_feat['cleaned_artist'] == df_artist_and_feat['name_artist']).all()\n",
    "print(f\"Are the artist names equal? {are_names_equal_final}\")\n",
    "\n",
    "# Identify and print the remaining mismatched rows\n",
    "final_mismatched_rows = df_artist_and_feat[df_artist_and_feat['cleaned_artist'] != df_artist_and_feat['name_artist']]\n",
    "print(f\"Number of rows still unequal: {len(final_mismatched_rows)}\")\n",
    "\n",
    "# if len(final_mismatched_rows) > 0:\n",
    "#     print(\"\\nSample of remaining mismatched rows:\")\n",
    "#     rows_to_display = final_mismatched_rows.head(10)\n",
    "#     print(rows_to_display[['name_artist', 'cleaned_artist']])\n",
    "    \n",
    "#     print(\"\\nFirst Mismatched Row Details:\")\n",
    "#     first_id = rows_to_display.index[0]\n",
    "#     print(f\"name_artist: '{df_artist_and_feat['name_artist'].loc[first_id]}'\")\n",
    "#     print(f\"cleaned_artist: '{df_artist_and_feat['cleaned_artist'].loc[first_id]}'\")\n",
    "\n",
    "# df_artist_and_feat.drop(columns=['artist_and_feat'], inplace=True)\n",
    "# print(df_artist_and_feat[['name_artist', 'cleaned_artist']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee95500",
   "metadata": {},
   "source": [
    "After applying comprehensive normalization to compare **`cleaned_feat`** (extracted from **`full_title`**) with the original **`featured_artists`** column, we identified **413 mismatched rows**.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "The mismatches reveal a systematic pattern: in many cases, the original **`featured_artists`** column is *empty* while **`cleaned_feat`** contains valid artist names extracted from **`full_title`**. This indicates that **`full_title`** actually contains *more complete* information about featured artists than the dedicated **`featured_artists`** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "51dfbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_artists(series):\n",
    "    \"\"\"\n",
    "    Normalize and sort a pandas Series containing artist names by:\n",
    "    - Replacing '&' with ',' for consistent delimiter\n",
    "    - Splitting by comma into individual artists\n",
    "    - Stripping whitespace from each artist name\n",
    "    - Sorting artists alphabetically\n",
    "    - Rejoining into a single comma-separated string\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        A pandas Series containing artist names (can be comma or ampersand-separated)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        A normalized Series with alphabetically sorted, comma-separated artist names\n",
    "    \"\"\"\n",
    "    # Replace & with , for consistent delimiter\n",
    "    s = series.str.replace('&', ',', regex=False)\n",
    "    \n",
    "    # Split by comma, strip whitespace, and filter out empty strings\n",
    "    list_artists = s.str.split(',').apply(\n",
    "        lambda x: [item.strip() for item in x if item.strip()] if isinstance(x, list) else []\n",
    "    )\n",
    "    \n",
    "    # Sort alphabetically\n",
    "    sorted_artists = list_artists.apply(lambda x: sorted(x))\n",
    "    \n",
    "    # Rejoin into comma-separated string\n",
    "    normalized_series = sorted_artists.apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    return normalized_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "cdcf3554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows still unequal: 413\n",
      "\n",
      "Sample of remaining mismatched rows:\n",
      "         featured_artists                                       cleaned_feat\n",
      "id                                                                          \n",
      "TR266736                   Friman (ITA), Mehdi (ITA), Mothz, Spender, The...\n",
      "TR281032                                                           Manu Chao\n",
      "TR811171                                                         Mara Sattei\n",
      "TR822203                                                         Mara Sattei\n",
      "TR397308                                                       Tiziano Ferro\n",
      "TR212338                                                         Mara Sattei\n",
      "TR372774                                                         Mara Sattei\n",
      "TR993112                                                         Mara Sattei\n",
      "TR444969                                                         Mara Sattei\n",
      "TR479694                                                         Mara Sattei\n",
      "\n",
      "Cleaned Series for First Mismatched Row (After Aggressive Strip):\n",
      "featured_artists (normalized): ''\n",
      "cleaned_feat (normalized): 'Friman (ITA), Mehdi (ITA), Mothz, Spender, Thelonious B., Zyrtck'\n"
     ]
    }
   ],
   "source": [
    "df_artist_and_feat['featured_artists'] = sort_artists(df_artist_and_feat['featured_artists'])\n",
    "df_artist_and_feat['cleaned_feat'] = sort_artists(df_artist_and_feat['cleaned_feat'])\n",
    "\n",
    "# Identify and print the remaining mismatched rows using the normalized series\n",
    "final_mismatched_rows = df_artist_and_feat[df_artist_and_feat['cleaned_feat'] != df_artist_and_feat['featured_artists']]\n",
    "print(f\"Number of rows still unequal: {len(final_mismatched_rows)}\")\n",
    "\n",
    "# Print the remaining mismatched rows for inspection\n",
    "if len(final_mismatched_rows) > 0:\n",
    "    print(\"\\nSample of remaining mismatched rows:\")\n",
    "    # We display the original columns and the two normalized versions for true inspection\n",
    "    rows_to_display = final_mismatched_rows.head(10)\n",
    "    print(rows_to_display[['featured_artists', 'cleaned_feat']])\n",
    "\n",
    "    print(\"\\nCleaned Series for First Mismatched Row (After Aggressive Strip):\")\n",
    "    first_id = rows_to_display.index[0]\n",
    "    # Use the normalized series for the clearest inspection\n",
    "    print(f\"featured_artists (normalized): '{df_artist_and_feat['featured_artists'].loc[first_id]}'\")\n",
    "    print(f\"cleaned_feat (normalized): '{df_artist_and_feat['cleaned_feat'].loc[first_id]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "55dd5e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "TR934808             [ernia, gue]\n",
      "TR760029           [thelonious b]\n",
      "TR916821    [mambolosco, radical]\n",
      "TR480968                 [taxi b]\n",
      "TR585039                  [rkomi]\n",
      "Name: featured_artists, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['featured_artists'] = df_artist_and_feat['cleaned_feat']\n",
    "\n",
    "df['featured_artists'] = df['featured_artists'].apply(\n",
    "    lambda x: [] if pd.isna(x) else [s.strip() for s in x.split(',')]\n",
    ")\n",
    "\n",
    "df['featured_artists'] = df['featured_artists'].apply(\n",
    "    lambda lst: normalize_series(pd.Series(lst)).tolist() if lst != [] else []\n",
    ")\n",
    "\n",
    "print(df['featured_artists'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "aced0a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "TR934808             [ernia, gue]\n",
      "TR760029           [thelonious b]\n",
      "TR916821    [mambolosco, radical]\n",
      "TR480968                 [taxi b]\n",
      "TR585039                  [rkomi]\n",
      "TR550335                   [<NA>]\n",
      "TR170793                   [<NA>]\n",
      "TR627195              [dani faiv]\n",
      "TR628871                   [<NA>]\n",
      "TR700756                   [<NA>]\n",
      "TR570242                   [<NA>]\n",
      "TR483404                   [<NA>]\n",
      "TR497808                   [<NA>]\n",
      "TR355495                   [<NA>]\n",
      "TR736423                   [<NA>]\n",
      "TR718221                   [<NA>]\n",
      "TR181361                  [wayne]\n",
      "TR950471                   [boro]\n",
      "TR562453                   [<NA>]\n",
      "TR851752                   [<NA>]\n",
      "TR387150                   [<NA>]\n",
      "TR968851                   [<NA>]\n",
      "TR634516                   [<NA>]\n",
      "TR292151                   [<NA>]\n",
      "TR709513                   [<NA>]\n",
      "TR804827                   [<NA>]\n",
      "TR889630                   [<NA>]\n",
      "TR701720                   [<NA>]\n",
      "TR933222                   [<NA>]\n",
      "TR242610                   [<NA>]\n",
      "Name: featured_artists, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['featured_artists'].head(30)) ##CAPISCI PERCH√® NA DEL CAZZO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccbd6a",
   "metadata": {},
   "source": [
    "Now full title column is redundant: the featured artist has been extracted and the title column is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6ffdbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['full_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f6ea7",
   "metadata": {},
   "source": [
    "## Language attribute\n",
    "Most present language for main lyrics are italian. english and polish. We checked most of these languages and they don't seem to respect the main language of the lyrics.\n",
    "\n",
    "So we decided to run a SOTA language model to detect based on the tokens of he lyrics colmn the language of the track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "74805d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "TR934808    Opl√†, ah\\nBdope, chiama due b\\n\\nMi candiderei...\n",
      "TR760029    Greg Willen, non dormire\\nBrrpoh\\n\\nTTTroppi c...\n",
      "TR916821    Mothz\\nYeah, yeah, yeahyeah\\nBdope, chiama due...\n",
      "TR480968    Designer sui vestiti penso di essere un outlet...\n",
      "TR585039    Bdope Yeah\\n\\nVuole solo me, non fare la gelos...\n",
      "Name: lyrics_normalized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_language = df_tracks[['language', 'lyrics', 'n_sentences']].copy()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    # Normalize smart quotes to straight quotes\n",
    "    text = re.sub(r'[‚Äò‚Äô]', \"'\", str(text))\n",
    "    text = re.sub(r'[‚Äú‚Äù]', '\"', text)\n",
    "    # Aggressively remove characters that might be noise or confuse the model (e.g., emojis, non-standard symbols)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\'\\\"]', '', text, flags=re.UNICODE)\n",
    "    return text\n",
    "\n",
    "df_language['lyrics_normalized'] = df_language['lyrics'].apply(normalize_text)\n",
    "\n",
    "print(df_language['lyrics_normalized'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "872851d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         language most_probable_language  confidence  n_sentences\n",
      "id                                                               \n",
      "TR934808       pl                   __it    0.964671          102\n",
      "TR760029       en                   __it    0.923832           56\n",
      "TR916821       en                   __it    0.953723           88\n",
      "TR480968       it                   __it    0.963273           37\n",
      "TR585039       en                   __it    0.983940           48\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'lid.176.bin'\n",
    "\n",
    "model = fasttext.load_model(MODEL_PATH)\n",
    "\n",
    "def detect_language_safe(text, model):\n",
    "    \"\"\"\n",
    "    Safely detects the language and confidence using FastText.\n",
    "    Fortified to handle DataFrame edge cases (NaN, None, short strings).\n",
    "    Returns a tuple (language_code, confidence_score) or (None, 0.0).\n",
    "    \"\"\"\n",
    "    # 1. Explicitly check for NaN/None and ensure string conversion\n",
    "    if pd.isna(text):\n",
    "        return None, 0.0\n",
    "    \n",
    "    # Ensure it's a string and strip whitespace\n",
    "    text_str = str(text).strip()\n",
    "    \n",
    "    # FIX: Remove newline and carriage return characters, as FastText requires a single line\n",
    "    text_str = text_str.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # FastText needs a minimum amount of text (let's keep the minimum length check)\n",
    "    if len(text_str) < 20: \n",
    "        # Optionally log which records were too short\n",
    "        # print(f\"Skipping record due to short length: {text_str[:10]}...\")\n",
    "        return None, 0.0\n",
    "    \n",
    "    try:\n",
    "        # k=1 asks for the single best prediction\n",
    "        predictions = model.predict(text_str, k=1) \n",
    "        \n",
    "        # predictions[0] is the label list: ['label__it']\n",
    "        # predictions[1] is the probability list: [0.99]\n",
    "        label = predictions[0][0].replace('__label', '')\n",
    "        confidence = predictions[1][0]\n",
    "        \n",
    "        return label, confidence\n",
    "    except Exception as e:\n",
    "        # If an exception is still caught, print a detailed message \n",
    "        # to help diagnose the specific content causing the crash.\n",
    "        print(f\"FastText Prediction failed for input starting: '{text_str[:50]}...'\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return None, 0.0\n",
    "\n",
    "results = df_language['lyrics_normalized'].apply(\n",
    "    lambda x: detect_language_safe(x, model)\n",
    ")\n",
    "\n",
    "# Unpack the Series of tuples into the two new columns\n",
    "\n",
    "# The first element of the tuple is the language code\n",
    "df_language['most_probable_language'] = results.apply(lambda x: x[0])\n",
    "\n",
    "# The second element of the tuple is the confidence score\n",
    "df_language['confidence'] = results.apply(lambda x: x[1])\n",
    "\n",
    "# Displaying the new columns (optional)\n",
    "print(df_language[['language', 'most_probable_language', 'confidence', 'n_sentences']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d044cc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "__it    10876\n",
      "__en      206\n",
      "__es       52\n",
      "__fr       11\n",
      "__de        5\n",
      "__pt        3\n",
      "__ro        3\n",
      "__mt        1\n",
      "__ja        1\n",
      "__ru        1\n",
      "__bg        1\n",
      "__ar        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "collapse_map = {\n",
    "    '__it': '__it',\n",
    "    '__en': '__en',\n",
    "    '__es': '__es',\n",
    "    '__fr': '__fr',\n",
    "    '__de': '__de',\n",
    "    '__pt': '__pt',\n",
    "    '__ro': '__ro',\n",
    "    '__ru': '__ru',\n",
    "    '__bg': '__bg',\n",
    "    '__ar': '__ar',\n",
    "    '__ja': '__ja',\n",
    "    '__mt': '__mt',\n",
    "\n",
    "    # Map regional languages to parent languages:\n",
    "    '__nap': '__it',   # Neapolitan ‚Üí Italian\n",
    "    '__lmo': '__it',   # Lombard ‚Üí Italian\n",
    "    '__ca': '__es',    # Catalan ‚Üí Spanish (if you prefer otherwise, tell me)\n",
    "    '__li': '__de',    # Limburgish ‚Üí German family (closest parent)\n",
    "}\n",
    "\n",
    "df_language['language_main'] = df_language['most_probable_language'].map(collapse_map)\n",
    "df['language'] = df_language['language_main']\n",
    "\n",
    "df['language'].hist(\n",
    "    bins=20, # Number of bins (intervals) for the histogram\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Distribution of FastText predicted languages')\n",
    "plt.xlabel('Predicted Language')\n",
    "plt.ylabel('Number of Records (Frequency)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(df['language'].value_counts())\n",
    "# df['language'] = df_language['most_probable_language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9a54d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows/values that contain 1 or fewer sentences is: 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11663/3646911508.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df_1_sentence, linguistic_columns] = pd.NA\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the boolean mask\n",
    "df_1_sentence = ((df['n_sentences'] < 2) | (df['n_sentences'] > 300))\n",
    "\n",
    "# 2. Count the rows being affected (Correct use of sum())\n",
    "rows_to_nullify = df_1_sentence.sum()\n",
    "print(f\"The number of rows/values that contain 1 or fewer sentences is: {rows_to_nullify}\")\n",
    "\n",
    "# 3. Define the columns to nullify\n",
    "linguistic_columns = ['language', 'swear_IT', 'swear_EN', \n",
    "                      'swear_IT_words', 'swear_EN_words', 'n_sentences', \n",
    "                      'n_tokens', 'tokens_per_sent', 'avg_token_per_clause', \n",
    "                      'explicit', 'lyrics']\n",
    "\n",
    "# 4. Apply the mask to the DataFrame df (This is the correct operation)\n",
    "df.loc[df_1_sentence, linguistic_columns] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dacee0b",
   "metadata": {},
   "source": [
    "## album, album_name, album id\n",
    "\n",
    "While the column album seems more reasonable and coherent, it contains multiple null values.\n",
    "Some album in \"album_name\" appear truncated and incomplete.\n",
    "\n",
    "We decided to keep the normalization for better readability and to have normalized occurrences.\n",
    "\n",
    "To create a new correct version of the column showing the album relative to every tracks we decided to do 3 major choices:\n",
    "\n",
    "    #Choice 1 (for null 'album'): Use 'album_name_norm',\n",
    "    \n",
    "    #Choice 2 (for Mismatch): Use 'album_norm',\n",
    "    \n",
    "    #Choice 3 (for Match): mantain 'album_norm' (the same with 'album_name_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "13bb281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creazione di 'correct_album' ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creazione di 'correct_album' ---\")\n",
    "\n",
    "# Applica la normalizzazione alle due colonne originali\n",
    "df['album_norm'] = normalize_series(df['album'])\n",
    "df['album_name_norm'] = normalize_series(df['album_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1ed02cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonna 'correct_album' iniziale creata with robust NaN handling.\n"
     ]
    }
   ],
   "source": [
    "# Define conditions for the 'correct_album' selection\n",
    "conditions = [\n",
    "    # 1. Handle cases where both are missing (P1: If 'album_name_norm' is missing)\n",
    "    (df['album_name_norm'].isna()), \n",
    "    \n",
    "    # 2. Handle cases where the comparison itself involves a missing value, \n",
    "    #    but only one is missing (P2: If ONLY 'album_norm' is missing)\n",
    "    #    (If 'album_name_norm' is missing, it's caught by P1)\n",
    "    (df['album_norm'].isna()),\n",
    "    \n",
    "    # 3. Mismatch: Ensure neither is missing before comparison (P3: Mismatch)\n",
    "    (df['album_norm'].notna() & df['album_name_norm'].notna() & \\\n",
    "     (df['album_norm'] != df['album_name_norm'])), \n",
    "    \n",
    "    # 4. Match: (P4: Match)\n",
    "    (df['album_norm'].notna() & df['album_name_norm'].notna() & \\\n",
    "     (df['album_norm'] == df['album_name_norm'])) \n",
    "]\n",
    "\n",
    "# Define the corresponding choices\n",
    "# The choices must reflect the order of the new conditions.\n",
    "choices = [\n",
    "    df['album_norm'],        # C1: P1 is true ('album_name_norm' is missing). Use 'album_norm'. (This will likely be NA/NaN).\n",
    "    df['album_name_norm'],   # C2: P2 is true ('album_norm' is missing). Use 'album_name_norm'. (This will be NA/NaN).\n",
    "    df['album_name_norm'],        # C3: P3 (Mismatch) is true. Use 'album_norm' (or 'album_name_norm' for the other value).\n",
    "    df['album_norm']         # C4: P4 (Match) is true. Use either one.\n",
    "]\n",
    "\n",
    "# Create the initial, noisy 'correct_album' column\n",
    "df['correct_album'] = np.select(conditions, choices, default=pd.NA)\n",
    "print(\"Colonna 'correct_album' iniziale creata with robust NaN handling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f23cb35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Enforcing 1-to-1 Mapping (ID as Anchor) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11663/933871790.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  track_counts = df_candidates.groupby(['id_album', 'correct_album']).size().to_frame('count').reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonna 'correct_album' creata con mapping ID-first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11663/933871790.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  idx = track_counts.groupby('id_album')['count'].idxmax()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Enforcing 1-to-1 Mapping (ID as Anchor) ---\")\n",
    "\n",
    "# --- 2.1: Prepare Data and Count Occurrences ---\n",
    "# Drop rows where either the ID or the initial album name is missing\n",
    "df_candidates = df.dropna(subset=['id_album'])\n",
    "track_counts = df_candidates.groupby(['id_album', 'correct_album']).size().to_frame('count').reset_index()\n",
    "\n",
    "# --- 2.2: Find the \"Winning\" Name for Each ID ---\n",
    "# Get the index of the row that has the maximum count for each 'id_album' group\n",
    "idx = track_counts.groupby('id_album')['count'].idxmax()\n",
    "id_to_single_album_map_df = track_counts.loc[idx]\n",
    "\n",
    "# --- 2.3: Create the Final Mapping Dictionary ---\n",
    "# Resulting map: {'noisy_id_album': 'clean_correct_album_name'}\n",
    "final_id_to_album_map = id_to_single_album_map_df.set_index('id_album')['correct_album'].to_dict()\n",
    "\n",
    "# --- 2.4: Apply the Cleaned Name to the DataFrame ---\n",
    "# This overwrites the inconsistent 'correct_album' name with the cleaned one\n",
    "# based on the track's id_album.\n",
    "df['correct_album'] = df['id_album'].map(final_id_to_album_map)\n",
    "print(\"Colonna 'correct_album' creata con mapping ID-first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e7ab2c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VIOLAZIONI: Album con pi√π di 1 ID (349 album) ---\n",
      "\n",
      "Esempi di righe incriminate (Album name associated with multiple unique IDs):\n",
      "                      correct_album   id_album album_type    name_artist\n",
      "id                                                                      \n",
      "TR607705                         00  ALB155638     single      dani faiv\n",
      "TR276729                         00  ALB563210     single          shade\n",
      "TR605506                         10  ALB100356      album         madman\n",
      "TR890254                         10  ALB721969      album         ghemon\n",
      "TR269175                         17  ALB689025     single         madame\n",
      "TR533807                         17  ALB756195      album     emis killa\n",
      "TR818380                         17  ALB756195      album     emis killa\n",
      "TR373885                         17  ALB756195      album     emis killa\n",
      "TR648074                         17  ALB756195      album     emis killa\n",
      "TR594038                         17  ALB756195      album     emis killa\n",
      "TR670228                         17  ALB756195      album     emis killa\n",
      "TR803223                         17  ALB756195      album     emis killa\n",
      "TR452849                         17  ALB756195      album     emis killa\n",
      "TR673308                         17  ALB756195      album     emis killa\n",
      "TR127689                         17  ALB756195      album     emis killa\n",
      "TR186622                         17  ALB756195      album     emis killa\n",
      "TR826237                         17  ALB756195      album     emis killa\n",
      "TR529433                         17  ALB756195      album     emis killa\n",
      "TR149622                         17  ALB756195      album     emis killa\n",
      "TR729855                         17  ALB756195      album     emis killa\n",
      "TR558533                         17  ALB756195      album     emis killa\n",
      "TR744426                         17  ALB887270      album          slait\n",
      "TR385845                         17  ALB924881      album  jake la furia\n",
      "TR483659  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR361620  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR418980  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR829380  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR233508  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR910201  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR105801  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR804736  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR494032  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR439836  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR455124  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR169027  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR859944  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR822484  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR787978  1969 achille idol rebirth  ALB633145      album  achille lauro\n",
      "TR168066  1969 achille idol rebirth  ALB839561      album       bushwaka\n",
      "TR662000                       1998  ALB689284      album           coez\n",
      "TR138172                       1998  ALB689284      album           coez\n",
      "TR539456                       1998  ALB689284      album           coez\n",
      "TR808240                       1998  ALB689284      album           coez\n",
      "TR477104                       1998  ALB689284      album           coez\n",
      "TR564129                       1998  ALB689284      album           coez\n",
      "TR862991                       1998  ALB689284      album           coez\n",
      "TR102692                       1998  ALB689284      album           coez\n",
      "TR987218                       1998  ALB689284      album           coez\n",
      "TR889782                       1998  ALB689284      album           coez\n",
      "TR265024                       1998  ALB790640      album       highsnob\n",
      "TR532071                      20139  ALB460252      album          ernia\n",
      "TR873925                      20139  ALB518853      album          vacca\n",
      "TR771305                      20139  ALB518853      album          vacca\n",
      "TR896526                      20139  ALB518853      album          vacca\n",
      "TR724878                      20139  ALB518853      album          vacca\n",
      "TR397528                      20139  ALB518853      album          vacca\n",
      "TR339276                      20139  ALB518853      album          vacca\n",
      "TR718425                      20139  ALB518853      album          vacca\n",
      "TR984393             2020 freestyle  ALB887657     single          tedua\n",
      "TR269298             2020 freestyle  ALB948872     single       mss keta\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify the violating album names (where the same name maps to > 1 unique ID)\n",
    "check_albums_per_id = df.groupby('correct_album')['id_album'].nunique()\n",
    "violating_album_names = check_albums_per_id[check_albums_per_id > 1].index.tolist()\n",
    "\n",
    "print(f\"\\n--- VIOLAZIONI: Album con pi√π di 1 ID ({len(violating_album_names)} album) ---\")\n",
    "\n",
    "if violating_album_names:\n",
    "    # 2. Filter the DataFrame for tracks belonging to the violating album names\n",
    "    df_violations = df[df['correct_album'].isin(violating_album_names)]\n",
    "    \n",
    "    # 3. Print a sample of the incriminated lines\n",
    "    #    We print 50 lines to show several distinct violations, \n",
    "    #    grouped by the violating name for better context.\n",
    "    \n",
    "    # Sort by the violating name and the ID to see the different IDs clearly\n",
    "    df_violations_sorted = df_violations.sort_values(\n",
    "        by=['correct_album', 'id_album']\n",
    "    )\n",
    "\n",
    "    print(\"\\nEsempi di righe incriminate (Album name associated with multiple unique IDs):\")\n",
    "    # Display the first 50 rows of the sorted violations\n",
    "    print(df_violations_sorted[['correct_album', 'id_album', 'album_type', 'name_artist']].head(60))\n",
    "else:\n",
    "    print(\"Nessuna violazione trovata! Il tuo mapping ID-first ha avuto successo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "212d74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11663/945771332.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  id_counts = df.groupby(['correct_album', 'id_album']).size().reset_index(name='count')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflict resolution applied. Check the new column 'correct_album_final_resolved'.\n",
      "\n",
      "--- NEW VIOLATION CHECK ---\n",
      "Original violations: 349 album names\n",
      "New violations after reversion: 264 album names\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Identify the Winning ID for Each Ambiguous Album Name ---\n",
    "\n",
    "# Find the count of each ID associated with a final album name\n",
    "id_counts = df.groupby(['correct_album', 'id_album']).size().reset_index(name='count')\n",
    "\n",
    "# Find the ID with the MAX count (the 'Winner') for each album name\n",
    "idx_max = id_counts.groupby('correct_album')['count'].idxmax()\n",
    "winning_id_map = id_counts.loc[idx_max].set_index('correct_album')['id_album'].to_dict()\n",
    "\n",
    "# --- 2. Create a column indicating the 'Winning ID' for each track ---\n",
    "df['winning_id'] = df['correct_album'].map(winning_id_map)\n",
    "\n",
    "# --- 3. Define the Reversion Condition and Choice ---\n",
    "\n",
    "# Condition for Reversion:\n",
    "# 1. The track's ID is NOT the winning ID for that album name (Minority track)\n",
    "# AND\n",
    "# 2. The album type is NOT 'single'\n",
    "reversion_condition = (df['id_album'] != df['winning_id']) & \\\n",
    "                      (df['album_type'] != 'single')\n",
    "\n",
    "# Choice for Reversion:\n",
    "# If the condition is True, revert the name to the less-cleaned 'album_norm'.\n",
    "reversion_choice = df['album_norm']\n",
    "\n",
    "# Default Choice (Keep the cleaned name):\n",
    "# If the condition is False (it's the majority ID, or it's a single), keep the current 'correct_album_final'.\n",
    "default_choice = df['correct_album']\n",
    "\n",
    "\n",
    "# --- 4. Apply the Logic to Create the NEW Final Column ---\n",
    "df['correct_album_final_resolved'] = np.select(\n",
    "    [reversion_condition],\n",
    "    [reversion_choice],\n",
    "    default=default_choice\n",
    ")\n",
    "\n",
    "print(\"Conflict resolution applied. Check the new column 'correct_album_final_resolved'.\")\n",
    "\n",
    "# --- 5. Verification Check ---\n",
    "print(\"\\n--- NEW VIOLATION CHECK ---\")\n",
    "new_check = df.groupby('correct_album_final_resolved')['id_album'].nunique()\n",
    "new_violations = new_check[new_check > 1]\n",
    "\n",
    "print(f\"Original violations: 349 album names\")\n",
    "print(f\"New violations after reversion: {len(new_violations)} album names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8e3a81ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The 'id_album' column has been consistently fixed across the entire dataset.\n",
      "   - Standard albums were fixed by (Album, Artist).\n",
      "   - Compilation albums were fixed by Album name only.\n"
     ]
    }
   ],
   "source": [
    "# 1. CREATE A CONDITIONAL GROUPING KEY (The Fix for the ValueError)\n",
    "# This key determines the level of grouping for the mode calculation:\n",
    "# - If 'compilation', the key is just the album name (string).\n",
    "# - Otherwise, the key is a tuple of (album_name, artist_name).\n",
    "\n",
    "# A. Create the complex key (tuple) for ALL rows by default.\n",
    "df['grouping_key'] = list(zip(df['correct_album_final_resolved'], df['name_artist']))\n",
    "\n",
    "# B. Identify compilation rows.\n",
    "is_compilation = (df['album_type'] == 'compilation')\n",
    "\n",
    "# C. OVERWRITE the key for compilation rows with the simpler string key using .loc[]\n",
    "df.loc[is_compilation, 'grouping_key'] = df.loc[is_compilation, 'correct_album_final_resolved']\n",
    "\n",
    "# 2. DETERMINE THE MODAL (Most Frequent) id_album for each unique grouping key.\n",
    "# This calculates the correct ID to use for every group.\n",
    "modal_id_series = df.groupby('grouping_key')['id_album'].agg(\n",
    "    # Use .mode()[0] to select the most frequent ID\n",
    "    lambda x: x.mode()[0] if not x.mode().empty else pd.NA\n",
    ")\n",
    "\n",
    "# 3. MAP THE MODAL ID BACK TO THE ORIGINAL DATAFRAME\n",
    "# Convert the modal Series to a dictionary for efficient mapping.\n",
    "modal_id_map = modal_id_series.to_dict()\n",
    "\n",
    "# Map the fixed ID onto a temporary column\n",
    "df['id_album_fixed'] = df['grouping_key'].map(modal_id_map)\n",
    "\n",
    "# 4. FINALIZE (Replace and Cleanup)\n",
    "# Replace the original 'id_album' column with the new fixed column\n",
    "df['id_album'] = df['id_album_fixed']\n",
    "\n",
    "# Drop the temporary columns used for fixing\n",
    "df = df.drop(columns=['grouping_key', 'id_album_fixed'])\n",
    "\n",
    "print(\"‚úÖ The 'id_album' column has been consistently fixed across the entire dataset.\")\n",
    "print(\"   - Standard albums were fixed by (Album, Artist).\")\n",
    "print(\"   - Compilation albums were fixed by Album name only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9cba27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Album Names Associated with Multiple Unique IDs (Violations) ---\n",
      "Format: [Album Name] -> [Number of Unique IDs]\n",
      "correct_album_final_resolved\n",
      "a me mi piace feat manu chao        8\n",
      "nella fossa                         7\n",
      "blocco 181 original soundtrack      7\n",
      "fuorilegge                          5\n",
      "de puta madre 3                     5\n",
      "luga pandemic                       4\n",
      "piangere a 90                       4\n",
      "bottiglie vuote feat max pezzali    4\n",
      "mbappe                              3\n",
      "m manc con geolier sfera ebbasta    3\n",
      "Name: id_album, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Group by the final cleaned album name and count the number of unique IDs\n",
    "check_albums_per_id = df.groupby('correct_album_final_resolved')['id_album'].nunique()\n",
    "\n",
    "# 2. Filter the resulting Series to keep only counts greater than 1\n",
    "violating_album_counts = check_albums_per_id[check_albums_per_id > 1]\n",
    "\n",
    "# 3. Print the result, sorted descending by the count (the most problematic albums first)\n",
    "print(\"\\n--- Album Names Associated with Multiple Unique IDs (Violations) ---\")\n",
    "print(\"Format: [Album Name] -> [Number of Unique IDs]\")\n",
    "print(violating_album_counts.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8ddccd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['album', 'album_name', 'album_norm', 'album_name_norm', 'correct_album', 'winning_id'], inplace = True)\n",
    "df = df.rename(columns={'correct_album_final_resolved': 'album'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf2848",
   "metadata": {},
   "source": [
    "## Stats page views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d9fbf",
   "metadata": {},
   "source": [
    "As considered in data understanding phase, almost 60% of records is missing (Nan) so we decided to drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3eef90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['stats_pageviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2706b",
   "metadata": {},
   "source": [
    "## Year, Month, Day and Album Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd2e79",
   "metadata": {},
   "source": [
    "there are multiple nan occurrences in month and day column so we decided to set such records at 01 to create a proper date based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "638b9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_components = {\n",
    "    # CRITICAL CHANGE: Fill NaN in 'year' with 0. \n",
    "    # This prevents the ValueError by allowing astype('int64') to work.\n",
    "    # The 'errors='coerce'' parameter will ensure year 0 converts to NaT.\n",
    "    'year': df['year'].fillna(0).astype('Int64'),\n",
    "    \n",
    "    # These remain as before:\n",
    "    'month': df['month'].fillna(1).astype('Int64'),\n",
    "    'day': df['day'].fillna(1).astype('Int64')\n",
    "}\n",
    "\n",
    "df['date'] = pd.to_datetime(date_components, errors='coerce') \n",
    "df.drop(columns=['year', 'month', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4283c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_release = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fc8c3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Creazione di un dataset \"Lungo\" con tutte le date e le info anagrafiche ---\n",
    "# Manteniamo 'name_artist' e 'birth_date' nell'indice, poi impiliamo le date\n",
    "df_all_dates = (\n",
    "    df_release.set_index(['name_artist', 'birth_date'])[['date', 'album_release_date']]\n",
    "    .stack()                        # Impila le colonne date in un'unica serie\n",
    "    .reset_index(name='data_evento') # Trasforma in DataFrame, la colonna delle date si chiama 'data_evento'\n",
    "    .drop(columns=['level_2'])      # Rimuove i nomi vecchi delle colonne ('date', 'album_release_date') che non servono\n",
    ")\n",
    "\n",
    "# --- 2. Creazione della Soglia di Controllo (+15 anni) ---\n",
    "df_all_dates['soglia_adulto'] = df_all_dates['birth_date'] + pd.DateOffset(years=15)\n",
    "\n",
    "# --- 3. Applicazione del Filtro Intelligente ---\n",
    "# Vogliamo tenere la data SOLO se:\n",
    "# A. La data √® maggiore del 1960 (filtro rumore base)\n",
    "#    AND\n",
    "# B. (Manca la data di nascita) OR (C'√® la nascita e la data evento √® > 15 anni)\n",
    "\n",
    "condizione_validita = (df_all_dates['data_evento'] >= '1988-01-01') & \\\n",
    "                      (\n",
    "                          (df_all_dates['soglia_adulto'].isna()) | \\\n",
    "                          (df_all_dates['data_evento'] >= df_all_dates['soglia_adulto'])\n",
    "                      )\n",
    "\n",
    "# Applichiamo il filtro: questo rimuove tutte le tracce \"infantili\"\n",
    "date_valide_plausibili = df_all_dates[condizione_validita]\n",
    "\n",
    "# --- 4. Calcolo della Minima Storica (quella \"dopo\") ---\n",
    "# Ora che abbiamo buttato le date impossibili, la .min() prender√† automaticamente\n",
    "# la successiva pi√π piccola valida.\n",
    "min_storico_artista = date_valide_plausibili.groupby('name_artist')['data_evento'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e41cfea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Integrazione nel calcolo finale (Come prima) ---\n",
    "stima_storica = df_release['name_artist'].map(min_storico_artista)\n",
    "\n",
    "soglia_master = df_release['active_start'].combine_first(\n",
    "    df_release['birth_date'] + pd.DateOffset(years=15)\n",
    ").combine_first(\n",
    "    stima_storica \n",
    ")\n",
    "\n",
    "# Applicazione correzione\n",
    "condizione = (df_release['date'] > pd.Timestamp.now()) | \\\n",
    "             (df_release['date'].isna()) | \\\n",
    "             (df_release['date'] < soglia_master)\n",
    "\n",
    "df_release.loc[condizione, 'date'] = df_release.loc[condizione, 'album_release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "454076fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione = (df_release['album_release_date'] > pd.Timestamp.now()) | \\\n",
    "             (df_release['album_release_date'].isna()) | \\\n",
    "             (df_release['album_release_date'] < soglia_master)\n",
    "\n",
    "df_release.loc[condizione, 'album_release_date'] = df_release.loc[condizione, 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94ec15",
   "metadata": {},
   "source": [
    "After cleaning and filtering the dataset, the columns of album_release_date and date are redundant and both provide the same information on when did the track released.\n",
    "\n",
    "We decided to use both column to correct the missing and non valid data and selected the best mode of the tracks in the same album (not the tracks with album_type=single) between the date mode and album_release_date mode.\n",
    "\n",
    "Then we create the column release_date that has the same data for all the tracks in the same album (selected with the previous control) and for singles we decided to use the column date because of more accuracy in the \"mode test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b453a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo della moda migliore per ciascun album...\n",
      "\n",
      "--- Conteggio finale delle fonti scelte ---\n",
      "best_mode_source\n",
      "date                  1881\n",
      "album_release_date     686\n",
      "none                     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fatto! Colonne 'correct_release_date' e 'best_mode_source' aggiunte.\n",
      "\n",
      "--- Esempio di 10 righe casuali ---\n",
      "                                                  album       date  \\\n",
      "5765                                              serio 2018-02-09   \n",
      "1743                              miracolo ultimo round 2016-02-19   \n",
      "5509                       nei sogni nessuno e monogamo 2022-03-03   \n",
      "3846                   aspettando orange county mixtape 2025-04-10   \n",
      "2720                                               fame 2025-01-31   \n",
      "6298                                 la vida que vendra 2000-05-04   \n",
      "9845                                     dentro e fuori 2015-06-02   \n",
      "8042                                           lovebars 2009-10-15   \n",
      "148   non so chi ha creato il mondo ma so che era in... 2024-02-16   \n",
      "9384                                    trap fatta bene 2024-03-15   \n",
      "\n",
      "     album_release_date correct_release_date  \n",
      "5765         2018-02-09           2018-02-09  \n",
      "1743         2016-02-19           2016-02-19  \n",
      "5509         2022-03-03           2022-03-03  \n",
      "3846         2025-04-10           2015-10-15  \n",
      "2720         2025-01-31           2025-01-31  \n",
      "6298         2000-06-12           2000-06-12  \n",
      "9845         2015-06-02           2015-06-02  \n",
      "8042         2023-09-08           2023-09-08  \n",
      "148          2024-02-16           2024-02-16  \n",
      "9384         2024-04-19           2024-04-19  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11663/87836761.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  best_album_info = df_release.groupby(group_keys).apply(get_best_mode_info)\n"
     ]
    }
   ],
   "source": [
    "group_keys = ['album'] \n",
    "def get_best_mode_info(group):\n",
    "    \"\"\"\n",
    "    Calcola la moda e la frequenza per 'date' e 'album_release_date'\n",
    "    e restituisce SIA LA FONTE ('date'/'album_release_date') \n",
    "    SIA IL VALORE (la data).\n",
    "    \"\"\"\n",
    "    # Calcola mode e frequenze per 'date'\n",
    "    date_counts = group['date'].value_counts()\n",
    "    date_mode_freq = date_counts.iloc[0] if not date_counts.empty else 0\n",
    "    date_mode_value = date_counts.index[0] if not date_counts.empty else pd.NaT\n",
    "\n",
    "    # Calcola mode e frequenze per 'album_release_date'\n",
    "    release_counts = group['album_release_date'].value_counts()\n",
    "    release_mode_freq = release_counts.iloc[0] if not release_counts.empty else 0\n",
    "    release_mode_value = release_counts.index[0] if not release_counts.empty else pd.NaT\n",
    "\n",
    "    # Confronta le frequenze e definisci fonte e valore\n",
    "    if date_mode_freq > release_mode_freq:\n",
    "        source = 'date'\n",
    "        value = date_mode_value\n",
    "    elif release_mode_freq > date_mode_freq:\n",
    "        source = 'album_release_date'\n",
    "        value = release_mode_value\n",
    "    elif date_mode_freq == 0: # Nessun dato valido\n",
    "        source = 'none'\n",
    "        value = pd.NaT\n",
    "    else: \n",
    "        # Pareggio (date_mode_freq == release_mode_freq > 0)\n",
    "        source = 'date' # Scegliamo 'date' come preferenza\n",
    "        value = date_mode_value\n",
    "    \n",
    "    # Restituisci una Serie con entrambe le info\n",
    "    return pd.Series({\n",
    "        'best_mode_source': source, \n",
    "        'correct_release_date': value\n",
    "    })\n",
    "\n",
    "# 4. Calcola le info migliori per ogni gruppo (album)\n",
    "print(\"Calcolo della moda migliore per ciascun album...\")\n",
    "# best_album_info ora sar√† un DataFrame con colonne 'best_mode_source' e 'correct_release_date'\n",
    "best_album_info = df_release.groupby(group_keys).apply(get_best_mode_info)\n",
    "\n",
    "print(\"\\n--- Conteggio finale delle fonti scelte ---\")\n",
    "final_counts = best_album_info['best_mode_source'].value_counts()\n",
    "print(final_counts)\n",
    "\n",
    "# Unisci (merge) questo DataFrame al DataFrame originale\n",
    "# Pandas unir√† 'best_mode_source' e 'correct_release_date'\n",
    "df_release = df_release.merge(best_album_info, on=group_keys, how='left')\n",
    "\n",
    "print(\"\\nFatto! Colonne 'correct_release_date' e 'best_mode_source' aggiunte.\")\n",
    "\n",
    "# Controlla il risultato (mostrando tutte le colonne rilevanti)\n",
    "print(\"\\n--- Esempio di 10 righe casuali ---\")\n",
    "print(df_release.sample(n=10)[['album', 'date', 'album_release_date', 'correct_release_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0e89a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             album       date album_release_date  \\\n",
      "6144                  meglio prima 2011-08-30         2011-08-30   \n",
      "81                             444 2024-01-04         2024-01-04   \n",
      "7812                         virus 2023-04-28         2023-04-28   \n",
      "9652                   hellvisback 2016-02-05         2016-02-05   \n",
      "5472        cd bonus track version 2010-06-02         2011-04-01   \n",
      "3617                   molto calmo 2013-12-11         2013-12-11   \n",
      "2183                          caos 2016-05-13         2022-03-18   \n",
      "3880                        exuvia 2021-04-16         2021-05-06   \n",
      "8499  lauro achille idol superstar 2022-02-11         2022-02-11   \n",
      "6826                          arte 2019-09-12         2019-09-11   \n",
      "\n",
      "     correct_release_date  \n",
      "6144           2011-08-30  \n",
      "81             2024-01-04  \n",
      "7812           2022-01-14  \n",
      "9652           2016-02-05  \n",
      "5472           2011-04-01  \n",
      "3617           2013-12-11  \n",
      "2183           2022-03-18  \n",
      "3880           2021-05-06  \n",
      "8499           2022-02-11  \n",
      "6826           2019-09-11  \n"
     ]
    }
   ],
   "source": [
    "print(df_release.sample(n=10)[['album', 'date', 'album_release_date', 'correct_release_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7becc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione = (df_release['album_type'] == 'single')  ##MANCA CONDIZIONE (?)\n",
    "df_release.loc[condizione, 'correct_release_date'] = df_release.loc[condizione, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "96709bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colonne 'correct_release_date' and 'best_mode_source' aggiunte al DataFrame originale 'df'.\n",
      "\n",
      "--- Esempio nel DataFrame 'df' ---\n",
      "                                album       date album_release_date  \\\n",
      "1007   blocco 181 original soundtrack 1951-01-01         2022-05-27   \n",
      "4618                   turno notturno 2016-09-16         2016-09-26   \n",
      "9446                      paninaro 20 1936-02-21         2018-09-21   \n",
      "10526                          gotham 2015-07-14         2015-07-14   \n",
      "2589                   nuovo medioego 2021-11-12         2020-11-26   \n",
      "\n",
      "      correct_release_date    best_mode_source  \n",
      "1007            2022-05-27  album_release_date  \n",
      "4618            2016-09-26  album_release_date  \n",
      "9446            2018-09-21  album_release_date  \n",
      "10526           2015-07-14                date  \n",
      "2589            2021-11-12  album_release_date  \n"
     ]
    }
   ],
   "source": [
    "# Assuming 'best_album_info' and 'group_keys = ['album']' were calculated correctly\n",
    "# The 'best_album_info' DataFrame has 'album' as its index (from the groupby operation).\n",
    "\n",
    "# 1. Reset the index of best_album_info so 'album' becomes a column again, \n",
    "# suitable for merging.\n",
    "best_album_info_reset = best_album_info.reset_index()\n",
    "\n",
    "# 2. Merge the calculated information back into the original DataFrame (df)\n",
    "# Pandas will match on the 'album' column.\n",
    "df = df.merge(\n",
    "    best_album_info_reset, \n",
    "    on='album', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Colonne 'correct_release_date' and 'best_mode_source' aggiunte al DataFrame originale 'df'.\")\n",
    "\n",
    "# Verification check (optional)\n",
    "print(\"\\n--- Esempio nel DataFrame 'df' ---\")\n",
    "print(df.sample(n=5)[['album', 'date', 'album_release_date', 'correct_release_date', 'best_mode_source']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "79b95959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_keys = ['album'] \n",
    "\n",
    "# best_album_info_df = df.groupby(group_keys).apply(get_best_mode_info)\n",
    "\n",
    "# df = df.merge(best_album_info_df, on=group_keys, how='left')\n",
    "# df.rename(columns={'correct_release_date': 'release_date'}, inplace=True)\n",
    "\n",
    "\n",
    "# print(\"\\nDataFrame 'df' cleaned!\")\n",
    "# print(df.sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "76dfe05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11166 entries, 0 to 11165\n",
      "Data columns (total 51 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  string        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11032 non-null  object        \n",
      " 5   swear_IT              11035 non-null  float64       \n",
      " 6   swear_EN              11035 non-null  float64       \n",
      " 7   swear_IT_words        11035 non-null  object        \n",
      " 8   swear_EN_words        11035 non-null  object        \n",
      " 9   n_sentences           10959 non-null  Int64         \n",
      " 10  n_tokens              10959 non-null  Int64         \n",
      " 11  tokens_per_sent       10959 non-null  float64       \n",
      " 12  char_per_tok          11090 non-null  float64       \n",
      " 13  lexical_density       11090 non-null  float64       \n",
      " 14  avg_token_per_clause  10959 non-null  float64       \n",
      " 15  bpm                   11102 non-null  float64       \n",
      " 16  centroid              11102 non-null  float64       \n",
      " 17  rolloff               11102 non-null  float64       \n",
      " 18  flux                  11102 non-null  float64       \n",
      " 19  rms                   11102 non-null  float64       \n",
      " 20  zcr                   11102 non-null  float64       \n",
      " 21  flatness              11102 non-null  float64       \n",
      " 22  spectral_complexity   11102 non-null  float64       \n",
      " 23  pitch                 11102 non-null  float64       \n",
      " 24  loudness              11102 non-null  float64       \n",
      " 25  album_release_date    10827 non-null  datetime64[ns]\n",
      " 26  album_type            11088 non-null  category      \n",
      " 27  disc_number           11088 non-null  Int64         \n",
      " 28  track_number          11088 non-null  Int64         \n",
      " 29  duration_ms           11088 non-null  float64       \n",
      " 30  explicit              11035 non-null  object        \n",
      " 31  popularity            11137 non-null  Int64         \n",
      " 32  album_image           11088 non-null  string        \n",
      " 33  id_album              11087 non-null  object        \n",
      " 34  lyrics                11032 non-null  string        \n",
      " 35  modified_popularity   11166 non-null  bool          \n",
      " 36  gender                11166 non-null  category      \n",
      " 37  birth_date            8588 non-null   datetime64[ns]\n",
      " 38  birth_place           8588 non-null   category      \n",
      " 39  nationality           8557 non-null   category      \n",
      " 40  description           10028 non-null  string        \n",
      " 41  active_start          6565 non-null   datetime64[ns]\n",
      " 42  province              8467 non-null   category      \n",
      " 43  region                8024 non-null   category      \n",
      " 44  country               8467 non-null   category      \n",
      " 45  latitude              8588 non-null   float64       \n",
      " 46  longitude             8588 non-null   float64       \n",
      " 47  album                 10960 non-null  object        \n",
      " 48  date                  10726 non-null  datetime64[ns]\n",
      " 49  best_mode_source      10960 non-null  object        \n",
      " 50  correct_release_date  10954 non-null  datetime64[ns]\n",
      "dtypes: Int64(5), bool(1), category(7), datetime64[ns](5), float64(19), object(9), string(5)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b947b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['active_start'] = df['active_start'].fillna(\n",
    "    df.groupby('name_artist')['correct_release_date'].transform('min')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5baa5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['correct_release_date'].isna()) | (df['correct_release_date']<df['active_start'])\n",
    "df.loc[condizione, 'correct_release_date'] = pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f988167a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_release_date</th>\n",
       "      <th>album</th>\n",
       "      <th>active_start</th>\n",
       "      <th>name_artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>rosa chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>alfa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>NaT</td>\n",
       "      <td>the interview</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>thasup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>lazza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>NaT</td>\n",
       "      <td>wish you were here</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>lazza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>lazza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2014-11-28</td>\n",
       "      <td>nerone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>niky savage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>NaT</td>\n",
       "      <td>erbavoglio</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>mudimbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>capo plaza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>massimo pericolo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>massimo pericolo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>skioffi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>skioffi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>skioffi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>skioffi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>skioffi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>mahmood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>NaT</td>\n",
       "      <td>euro contanti</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>piotta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>clementino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>fabri fibra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>fabri fibra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>NaT</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>fabri fibra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     correct_release_date               album active_start       name_artist\n",
       "23                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "24                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "33                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "36                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "39                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "42                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "47                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "49                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "53                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "55                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "56                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "58                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "65                    NaT                <NA>   2015-01-01     rosa chemical\n",
       "195                   NaT                <NA>   2016-02-05              alfa\n",
       "261                   NaT                <NA>   2015-01-01            thasup\n",
       "264                   NaT                <NA>   2015-01-01            thasup\n",
       "271                   NaT                <NA>   2015-01-01            thasup\n",
       "274                   NaT                <NA>   2015-01-01            thasup\n",
       "278                   NaT                <NA>   2015-01-01            thasup\n",
       "280                   NaT                <NA>   2015-01-01            thasup\n",
       "281                   NaT                <NA>   2015-01-01            thasup\n",
       "282                   NaT                <NA>   2015-01-01            thasup\n",
       "284                   NaT                <NA>   2015-01-01            thasup\n",
       "289                   NaT       the interview   2015-01-01            thasup\n",
       "290                   NaT                <NA>   2015-01-01            thasup\n",
       "292                   NaT                <NA>   2015-01-01            thasup\n",
       "293                   NaT                <NA>   2015-01-01            thasup\n",
       "296                   NaT                <NA>   2015-01-01            thasup\n",
       "300                   NaT                <NA>   2015-01-01            thasup\n",
       "303                   NaT                <NA>   2015-01-01            thasup\n",
       "431                   NaT                <NA>   2012-01-01             lazza\n",
       "436                   NaT  wish you were here   2012-01-01             lazza\n",
       "439                   NaT                <NA>   2012-01-01             lazza\n",
       "671                   NaT                <NA>   2014-11-28            nerone\n",
       "792                   NaT                <NA>   2021-12-15       niky savage\n",
       "1142                  NaT          erbavoglio   2013-01-01           mudimbi\n",
       "1299                  NaT                <NA>   2013-02-26        capo plaza\n",
       "1352                  NaT                <NA>   2017-04-20  massimo pericolo\n",
       "1377                  NaT                <NA>   2017-04-20  massimo pericolo\n",
       "1393                  NaT                <NA>   1977-09-30           skioffi\n",
       "1399                  NaT                <NA>   1977-09-30           skioffi\n",
       "1404                  NaT                <NA>   1977-09-30           skioffi\n",
       "1458                  NaT                <NA>   1977-09-30           skioffi\n",
       "1469                  NaT                <NA>   1977-09-30           skioffi\n",
       "1526                  NaT                <NA>   2012-01-01           mahmood\n",
       "1581                  NaT       euro contanti   1994-01-01            piotta\n",
       "1657                  NaT                <NA>   2003-01-01        clementino\n",
       "2160                  NaT                <NA>   1996-01-01       fabri fibra\n",
       "2220                  NaT                <NA>   1996-01-01       fabri fibra\n",
       "2247                  NaT                <NA>   1996-01-01       fabri fibra"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condizione= (df['correct_release_date'] > pd.Timestamp.now()) | (df['correct_release_date'].isna()) | (df['correct_release_date']<df['active_start'])\n",
    "dati_filtrati = df.loc[condizione, ['correct_release_date', 'album','active_start', 'name_artist']]\n",
    "dati_filtrati.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e00b3",
   "metadata": {},
   "source": [
    "Active start was remodeled and filled the first date recorded of their tracks/albums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "cceaa832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977-09-30 00:00:00\n",
      "Rows where 'active_start' equals the minimum date:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>correct_release_date</th>\n",
       "      <th>date</th>\n",
       "      <th>album_release_date</th>\n",
       "      <th>album_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>skifidol bimbo rmx</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>fuorilegge</td>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>jolly mixtape</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>respirare</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>new boots and panties</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>1959-03-07</td>\n",
       "      <td>1977-09-30</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>nella fossa</td>\n",
       "      <td>2014-04-03</td>\n",
       "      <td>2051-05-29</td>\n",
       "      <td>2014-04-03</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>alice</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>raptus 3</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>2017-05-29</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>silent hill</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>alice</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>its snowing motherfucker</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>alice</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2084-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>alice</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>miele</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>alice</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>1902-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>raptus 3</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>parlero di te unplugged</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>1906-03-27</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>dur daha yeni basladk</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>1930-06-03</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>alice</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>1963-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>1968-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>allindennoman alter ego</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>sarah</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>ti ho sognata tutta la notte</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2064-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>1930-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>simona</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>1968-07-01</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>je tamo ancora</td>\n",
       "      <td>2024-04-03</td>\n",
       "      <td>1937-04-03</td>\n",
       "      <td>2024-04-03</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>dove ce larcobaleno</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>wojtape</td>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>2013-04-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>galassie</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>indi</td>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>2025-01-24</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>sumeria</td>\n",
       "      <td>2012-10-15</td>\n",
       "      <td>2074-04-04</td>\n",
       "      <td>2012-10-15</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             album correct_release_date       date  \\\n",
       "1422            skifidol bimbo rmx           2017-05-29 2017-05-29   \n",
       "1423                    fuorilegge           2025-02-12 2017-05-29   \n",
       "1424                 jolly mixtape           2017-05-29 2017-05-29   \n",
       "1425                     respirare           2020-05-08 2020-05-08   \n",
       "1426         new boots and panties           1977-09-30 1959-03-07   \n",
       "1427                   nella fossa           2014-04-03 2051-05-29   \n",
       "1428                         alice           2021-04-09 2021-04-09   \n",
       "1429                      raptus 3           2019-03-15 2017-05-29   \n",
       "1430                   silent hill           2021-06-11 2021-06-11   \n",
       "1431                         alice           2021-04-09 2021-04-09   \n",
       "1432      its snowing motherfucker           2020-07-17 2020-07-17   \n",
       "1433       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1434                         alice           2021-04-09 2084-04-09   \n",
       "1435       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1436                         alice           2021-04-09 2021-04-09   \n",
       "1437       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1438                         miele           2022-11-04 2022-11-04   \n",
       "1439       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1440                         alice           2021-04-09 2021-04-09   \n",
       "1441       allindennoman alter ego           2021-10-15 1902-10-15   \n",
       "1442       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1443       allindennoman alter ego           2021-10-15        NaT   \n",
       "1444       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1445       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1446                      raptus 3           2019-03-15 2018-01-15   \n",
       "1447       parlero di te unplugged           2020-03-27 1906-03-27   \n",
       "1448         dur daha yeni basladk           2011-08-30 1930-06-03   \n",
       "1449                         alice           2021-04-09 2021-04-09   \n",
       "1450                      galassie           2024-01-12 1963-01-12   \n",
       "1451                      galassie           2024-01-12 1968-01-12   \n",
       "1452       allindennoman alter ego           2021-10-15 2021-10-15   \n",
       "1453                      galassie           2024-01-12 2024-01-12   \n",
       "1454                      galassie           2024-01-12 2024-01-12   \n",
       "1455                         sarah           2025-02-21 2025-02-21   \n",
       "1456  ti ho sognata tutta la notte           2021-07-28        NaT   \n",
       "1457                      galassie           2024-01-12 2064-01-12   \n",
       "1458                          <NA>                  NaT 2020-12-02   \n",
       "1459                      galassie           2024-01-12 2024-01-12   \n",
       "1460                      galassie           2024-01-12 2024-01-12   \n",
       "1461                      galassie           2024-01-12 1930-01-12   \n",
       "1462                        simona           2022-07-01 1968-07-01   \n",
       "1463                      galassie           2024-01-12 2024-01-12   \n",
       "1464                je tamo ancora           2024-04-03 1937-04-03   \n",
       "1465           dove ce larcobaleno           2016-03-24 2016-03-24   \n",
       "1466                       wojtape           2020-11-17 2020-11-17   \n",
       "1467                      galassie           2024-01-12 2024-01-12   \n",
       "1468                      galassie           2024-01-12 2024-01-12   \n",
       "1469                          <NA>                  NaT 2023-04-24   \n",
       "1470                          indi           2020-11-24 2020-11-24   \n",
       "1471                       sumeria           2012-10-15 2074-04-04   \n",
       "\n",
       "     album_release_date album_type  \n",
       "1422         2024-03-22     single  \n",
       "1423         2025-02-12     single  \n",
       "1424         2024-02-16      album  \n",
       "1425         2020-05-08     single  \n",
       "1426         1977-09-30      album  \n",
       "1427         2014-04-03     single  \n",
       "1428         2021-04-09      album  \n",
       "1429         2019-03-15      album  \n",
       "1430         2021-06-11     single  \n",
       "1431         2021-04-09      album  \n",
       "1432         2020-07-17     single  \n",
       "1433         2021-10-15      album  \n",
       "1434         2021-04-09      album  \n",
       "1435         2021-10-15      album  \n",
       "1436         2021-04-09      album  \n",
       "1437         2021-10-15      album  \n",
       "1438         2022-11-04     single  \n",
       "1439         2021-10-15      album  \n",
       "1440         2021-04-09      album  \n",
       "1441         2021-10-15      album  \n",
       "1442         2021-10-15      album  \n",
       "1443         2021-10-15      album  \n",
       "1444         2021-10-15      album  \n",
       "1445         2021-10-15      album  \n",
       "1446         2019-03-15      album  \n",
       "1447         2020-03-27     single  \n",
       "1448         2011-08-30      album  \n",
       "1449         2021-04-09      album  \n",
       "1450         2024-01-12      album  \n",
       "1451         2024-01-12      album  \n",
       "1452         2021-10-15      album  \n",
       "1453         2024-01-12      album  \n",
       "1454         2024-01-12      album  \n",
       "1455         2025-02-21     single  \n",
       "1456         2021-07-28     single  \n",
       "1457         2024-01-12      album  \n",
       "1458         2025-04-10      album  \n",
       "1459         2024-01-12      album  \n",
       "1460         2024-01-12      album  \n",
       "1461         2024-01-12      album  \n",
       "1462         2022-07-01     single  \n",
       "1463         2024-01-12      album  \n",
       "1464         2024-04-03     single  \n",
       "1465         2024-09-13      album  \n",
       "1466         2013-04-15      album  \n",
       "1467         2024-01-12      album  \n",
       "1468         2024-01-12      album  \n",
       "1469         2025-04-10      album  \n",
       "1470         2025-01-24      album  \n",
       "1471         2012-10-15      album  "
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date = df['active_start'].min()\n",
    "print(min_date)\n",
    "\n",
    "# 2. Filter the DataFrame to find rows where 'active_start' equals the minimum date\n",
    "rows_with_min_date = df[df['active_start'] == min_date]\n",
    "\n",
    "# 3. Print the resulting rows\n",
    "print(\"Rows where 'active_start' equals the minimum date:\")\n",
    "rows_with_min_date[['album', 'correct_release_date', 'date', 'album_release_date', 'album_type']].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fa9b6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = ['album_release_date', 'date', 'best_mode_source','new_release']\n",
    "df.drop(columns=col_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ddaf2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11166 entries, 0 to 11165\n",
      "Data columns (total 48 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  string        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11032 non-null  object        \n",
      " 5   swear_IT              11035 non-null  float64       \n",
      " 6   swear_EN              11035 non-null  float64       \n",
      " 7   swear_IT_words        11035 non-null  object        \n",
      " 8   swear_EN_words        11035 non-null  object        \n",
      " 9   n_sentences           10959 non-null  Int64         \n",
      " 10  n_tokens              10959 non-null  Int64         \n",
      " 11  tokens_per_sent       10959 non-null  float64       \n",
      " 12  char_per_tok          11090 non-null  float64       \n",
      " 13  lexical_density       11090 non-null  float64       \n",
      " 14  avg_token_per_clause  10959 non-null  float64       \n",
      " 15  bpm                   11102 non-null  float64       \n",
      " 16  centroid              11102 non-null  float64       \n",
      " 17  rolloff               11102 non-null  float64       \n",
      " 18  flux                  11102 non-null  float64       \n",
      " 19  rms                   11102 non-null  float64       \n",
      " 20  zcr                   11102 non-null  float64       \n",
      " 21  flatness              11102 non-null  float64       \n",
      " 22  spectral_complexity   11102 non-null  float64       \n",
      " 23  pitch                 11102 non-null  float64       \n",
      " 24  loudness              11102 non-null  float64       \n",
      " 25  album_type            11088 non-null  category      \n",
      " 26  disc_number           11088 non-null  Int64         \n",
      " 27  track_number          11088 non-null  Int64         \n",
      " 28  duration_ms           11088 non-null  float64       \n",
      " 29  explicit              11035 non-null  object        \n",
      " 30  popularity            11137 non-null  Int64         \n",
      " 31  album_image           11088 non-null  string        \n",
      " 32  id_album              11087 non-null  object        \n",
      " 33  lyrics                11032 non-null  string        \n",
      " 34  modified_popularity   11166 non-null  bool          \n",
      " 35  gender                11166 non-null  category      \n",
      " 36  birth_date            8588 non-null   datetime64[ns]\n",
      " 37  birth_place           8588 non-null   category      \n",
      " 38  nationality           8557 non-null   category      \n",
      " 39  description           10028 non-null  string        \n",
      " 40  active_start          11166 non-null  datetime64[ns]\n",
      " 41  province              8467 non-null   category      \n",
      " 42  region                8024 non-null   category      \n",
      " 43  country               8467 non-null   category      \n",
      " 44  latitude              8588 non-null   float64       \n",
      " 45  longitude             8588 non-null   float64       \n",
      " 46  album                 10960 non-null  object        \n",
      " 47  correct_release_date  10943 non-null  datetime64[ns]\n",
      "dtypes: Int64(5), bool(1), category(7), datetime64[ns](3), float64(19), object(8), string(5)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2f1e4",
   "metadata": {},
   "source": [
    "## Popularity and Modified_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1084a",
   "metadata": {},
   "source": [
    "When modified_popularity is 'true' (78 occurrences) the related popularity occurrence is not in the correct format.\n",
    "We decided to drop the column modified_popularity and set the invalid popularity records to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a72b80cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "pop=df[df['modified_popularity']==True]\n",
    "print(len(pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a7633e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>modified_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>-654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>147413</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>164032</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>884794</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>-119</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>-606</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>-286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      popularity  modified_popularity\n",
       "1932        <NA>                 True\n",
       "2304        <NA>                 True\n",
       "3286        -654                 True\n",
       "3667      147413                 True\n",
       "3668      164032                 True\n",
       "...          ...                  ...\n",
       "3737        <NA>                 True\n",
       "3738      884794                 True\n",
       "3739        -119                 True\n",
       "6390        -606                 True\n",
       "9261        -286                 True\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condizione= (df['popularity']<0) | (df['popularity']>100) | (df['popularity'].isna())\n",
    "dati_filtrati = df.loc[condizione, ['popularity', 'modified_popularity']]\n",
    "dati_filtrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "622b166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['popularity']<0) | (df['popularity']>100)\n",
    "df.loc[condizione, 'popularity'] = np.nan\n",
    "df.drop(columns=['modified_popularity'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c6238",
   "metadata": {},
   "source": [
    "## Swear words control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a99c24",
   "metadata": {},
   "source": [
    "We checked if the column Swear_IT and Swear_EN are coherent with the lyrics and their related Swear_IT_words and Swear_EN_words lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "225fd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\n",
      "2/3: Confronto dei conteggi...\n",
      "3/3: Controllo completato.\n",
      "\n",
      "‚úÖ Controllo superato! Tutti i conteggi 'swear_IT' corrispondono al ricalcolo dai testi.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pandas.api.types import is_scalar # Importiamo il controllo specifico\n",
    "\n",
    "## 1. Funzione per contare le parole nel testo (Versione 3)\n",
    "\n",
    "def conta_parole_in_testo_corretto(riga):\n",
    "    lista_parole = riga['swear_IT_words']\n",
    "    testo_lyrics = riga['lyrics']\n",
    "\n",
    "    # --- CORREZIONE ---\n",
    "    # 1. Controlla il testo (che √® sempre scalare)\n",
    "    if pd.isna(testo_lyrics):\n",
    "        return 0\n",
    "\n",
    "    # 2. Controlla la lista/array\n",
    "    # Prima controlla se √® uno scalare (es. np.nan o None)\n",
    "    if is_scalar(lista_parole):\n",
    "        if pd.isna(lista_parole):\n",
    "            return 0  # √à np.nan o None, quindi 0\n",
    "        else:\n",
    "            return 0  # √à uno scalare ma non nullo (es. un numero), non va bene\n",
    "    \n",
    "    # Se siamo qui, 'lista_parole' NON √® uno scalare,\n",
    "    # quindi √® una lista o un array. Ora possiamo controllarne la lunghezza.\n",
    "    if len(lista_parole) == 0:\n",
    "        return 0\n",
    "    # --- FINE CORREZIONE ---\n",
    "\n",
    "    conteggio_totale = 0\n",
    "    testo_lower = testo_lyrics.lower()\n",
    "\n",
    "    for parola in lista_parole:\n",
    "        parola_lower = str(parola).lower()\n",
    "        pattern = r'\\b' + re.escape(parola_lower) + r'\\b'\n",
    "        occorrenze = re.findall(pattern, testo_lower)\n",
    "        conteggio_totale += len(occorrenze)\n",
    "\n",
    "    return conteggio_totale\n",
    "\n",
    "# --- ESECUZIONE DEL CONTROLLO ---\n",
    "\n",
    "# 1. Calcola il nuovo conteggio applicando la funzione CORRETTA\n",
    "print(\"1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\")\n",
    "df['conteggio_calcolato'] = df.apply(conta_parole_in_testo_corretto, axis=1)\n",
    "\n",
    "# 2. Prepara il conteggio originale\n",
    "df['conteggio_originale'] = df['swear_IT'].fillna(0).astype(int)\n",
    "\n",
    "# 3. Confronta i due conteggi\n",
    "print(\"2/3: Confronto dei conteggi...\")\n",
    "incoerenze = df[df['conteggio_originale'] != df['conteggio_calcolato']]\n",
    "\n",
    "# --- RISULTATO ---\n",
    "print(\"3/3: Controllo completato.\")\n",
    "if incoerenze.empty:\n",
    "    print(\"\\n‚úÖ Controllo superato! Tutti i conteggi 'swear_IT' corrispondono al ricalcolo dai testi.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Trovate {len(incoerenze)} righe incoerenti:\")\n",
    "    print(incoerenze[['artist', 'title', 'conteggio_originale', 'conteggio_calcolato', 'swear_IT_words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "56733761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\n",
      "2/3: Confronto dei conteggi...\n",
      "3/3: Controllo completato.\n",
      "\n",
      "‚úÖ Controllo superato! Tutti i conteggi 'swear_EN' corrispondono al ricalcolo dai testi.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pandas.api.types import is_scalar # Importiamo il controllo specifico\n",
    "\n",
    "## 1. Funzione per contare le parole nel testo (Versione 3)\n",
    "\n",
    "def conta_parole_in_testo_corretto(riga):\n",
    "    lista_parole = riga['swear_EN_words']\n",
    "    testo_lyrics = riga['lyrics']\n",
    "\n",
    "    # --- CORREZIONE ---\n",
    "    # 1. Controlla il testo (che √® sempre scalare)\n",
    "    if pd.isna(testo_lyrics):\n",
    "        return 0\n",
    "\n",
    "    # 2. Controlla la lista/array\n",
    "    # Prima controlla se √® uno scalare (es. np.nan o None)\n",
    "    if is_scalar(lista_parole):\n",
    "        if pd.isna(lista_parole):\n",
    "            return 0  # √à np.nan o None, quindi 0\n",
    "        else:\n",
    "            return 0  # √à uno scalare ma non nullo (es. un numero), non va bene\n",
    "    \n",
    "    # Se siamo qui, 'lista_parole' NON √® uno scalare,\n",
    "    # quindi √® una lista o un array. Ora possiamo controllarne la lunghezza.\n",
    "    if len(lista_parole) == 0:\n",
    "        return 0\n",
    "    # --- FINE CORREZIONE ---\n",
    "\n",
    "    conteggio_totale = 0\n",
    "    testo_lower = testo_lyrics.lower()\n",
    "\n",
    "    for parola in lista_parole:\n",
    "        parola_lower = str(parola).lower()\n",
    "        pattern = r'\\b' + re.escape(parola_lower) + r'\\b'\n",
    "        occorrenze = re.findall(pattern, testo_lower)\n",
    "        conteggio_totale += len(occorrenze)\n",
    "\n",
    "    return conteggio_totale\n",
    "\n",
    "# --- ESECUZIONE DEL CONTROLLO ---\n",
    "\n",
    "# 1. Calcola il nuovo conteggio applicando la funzione CORRETTA\n",
    "print(\"1/3: Riconteggio delle parole nei testi (pu√≤ richiedere tempo)...\")\n",
    "df['conteggio_calcolato'] = df.apply(conta_parole_in_testo_corretto, axis=1)\n",
    "\n",
    "# 2. Prepara il conteggio originale\n",
    "df['conteggio_originale'] = df['swear_EN'].fillna(0).astype(int)\n",
    "\n",
    "# 3. Confronta i due conteggi\n",
    "print(\"2/3: Confronto dei conteggi...\")\n",
    "incoerenze = df[df['conteggio_originale'] != df['conteggio_calcolato']]\n",
    "\n",
    "# --- RISULTATO ---\n",
    "print(\"3/3: Controllo completato.\")\n",
    "if incoerenze.empty:\n",
    "    print(\"\\n‚úÖ Controllo superato! Tutti i conteggi 'swear_EN' corrispondono al ricalcolo dai testi.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Trovate {len(incoerenze)} righe incoerenti:\")\n",
    "    print(incoerenze[['artist', 'title', 'conteggio_originale', 'conteggio_calcolato', 'swear_EN_words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "eed1eac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PAROLE UNICHE ITALIANE (173) ---\n",
      "['cazzo', 'cesso', 'coglioni', 'figa', 'merda', 'palle', 'piscio', 'porca', 'stronzo', 'culo', 'frocio', 'puttana', 'sega', 'troia', 'bastardo', 'fottere', 'pompino', 'scopare', 'figo', 'cagare', 'fica', 'gay', 'zoccola', 'coglione', 'puttano', 'fottuto', 'cazzi', 'ricchione', 'stupido', 'vagina', 'selvaggio', 'fottuti', 'incazzato', 'cretino', 'sboccare', 'zanzara', 'fortuna', 'stupida', 'incazzare', 'cagna', 'grilletto', 'bastardi', 'toro', 'pippa', 'cazzata', 'sedere', 'mazzo', 'cretini', 'vaffanculo', 'fogne', 'scazzo', 'cogliona', 'cozza', 'jug', 'puttanata', 'sfiga', 'pisciare', 'scopata', 'checca', 'fesso', 'madonna', 'scassare', 'uccello', 'sfigata', 'battona', 'fregna', 'topa', 'pisciata', 'bagascia', 'maiala', 'mignotta', 'farabutto', 'chiappa', 'minchioni', 'fico', 'travestito', 'vacca', 'gnocca', 'bischero', 'idiozia', 'handicappato', 'leccaculo', 'pene', 'fregarsene', 'paraculo', 'bocchino', 'farabutti', 'granchio', 'blowjob', 'cappella', 'piccione', 'sborrare', 'fellatio', 'pisello', 'puttaniere', 'cazzeggio', 'cacca', 'stronzata', 'cagata', 'trombare', 'arrapato', 'fogna', 'water', 'cazzone', 'finocchio', 'puttanaio', 'sborra', 'scoreggia', 'feci', 'bernarda', 'sorca', 'cretina', 'bocchinaro', 'deretano', 'scrofa', 'fregare', 'pipa', 'chiavare', 'troiaggine', 'tetta', 'bombare', 'cazzeggiare', 'pugnetta', 'culattone', 'spagnola', 'cornuto', 'sveltina', 'rompicoglioni', 'seccatore', 'smerdare', 'incazzarsi', 'inculare', 'sputtanare', 'fava', 'cazzuto', 'merdina', 'maroni', 'strafottenza', 'arrapante', 'controcoglioni', 'controcazzi', 'fottersi', 'gigolo', 'rompipalle', 'fottio', 'peluria', 'zizza', 'glutei', 'merdaiolo', 'escremento', 'missionario', 'scazzato', 'chiavata', 'raspa', 'cunnu', 'arrapare', 'cacare', 'vaccata', 'nerchia', 'pecorina', 'troiaio', 'pompinara', 'merdata', 'mezzasega', 'sgualdrina', 'cacata', 'spompinare', 'rottinculo', 'minchiata', 'coglionata', 'merdaio', 'segaiolo', 'schizzare']\n"
     ]
    }
   ],
   "source": [
    "lista_parole_it = list(df['swear_IT_words'].explode().dropna().unique())\n",
    "\n",
    "# 2. Lista Inglese (EN)\n",
    "# Stessa identica logica\n",
    "lista_parole_en = list(df['swear_EN_words'].explode().dropna().unique())\n",
    "\n",
    "# 3. Stampa i risultati\n",
    "print(f\"--- PAROLE UNICHE ITALIANE ({len(lista_parole_it)}) ---\")\n",
    "print(lista_parole_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "05e95446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PAROLE UNICHE INGLESI (96) ---\n",
      "['bitch', 'fuck', 'porno', 'pussy', 'escort', 'negro', 'sex', 'sexy', 'shit', 'bastardo', 'bitches', 'cock', 'ass', 'slut', 'voyeur', 'anal', 'tranny', 'dick', 'nigga', 'threesome', 'vagina', 'porn', 'clit', 'nude', 'fucking', 'bullshit', 'motherfucker', 'xx', 'boobs', 'xxx', 'playboy', 'deepthroat', 'faggot', 'fisting', 'suck', 'cumming', 'milf', 'hardcore', 'cialis', 'fag', 'rapist', 'rape', 'dildo', 'viagra', 'rimming', 'sexo', 'hentai', 'blowjob', 'fuckin', 'gangbang', 'pedobear', 'fellatio', 'bastard', 'sexual', 'snatch', 'butt', 'cumshot', 'lolita', 'vulva', 'punany', 'panties', 'shitty', 'tits', 'topless', 'anus', 'bbw', 'bondage', 'cunt', 'scat', 'raping', 'shibari', 'poof', 'coon', 'skeet', 'domination', 'ecchi', 'sucks', 'creampie', 'cum', 'titty', 'spic', 'kinky', 'bukkake', 'cocks', 'pissing', 'asshole', 'busty', 'hooker', 'semen', 'horny', 'masturbation', 'neonazi', 'doggystyle', 'tit', 'nympho', 'nipple']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n--- PAROLE UNICHE INGLESI ({len(lista_parole_en)}) ---\")\n",
    "print(lista_parole_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f0e4a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['explicit']==False) & ((df['swear_IT']>0) | (df['swear_EN']>0))\n",
    "dati_filtrati = df.loc[condizione, ['explicit', 'swear_IT', 'swear_EN']]\n",
    "dati_filtrati\n",
    "df.loc[condizione,['explicit']] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c3ea2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "condizione= (df['explicit']==True) & ((df['swear_IT']==0) & (df['swear_EN']==0))\n",
    "dati_filtrati = df.loc[condizione, ['explicit', 'swear_IT', 'swear_EN']]\n",
    "dati_filtrati\n",
    "df.loc[condizione,['explicit']] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "99fca526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creazione pattern RegEx in corso...\n",
      "Pattern creati.\n",
      "Inizio ricalcolo su tutto il DataFrame (pu√≤ richiedere tempo)...\n",
      "üìä Numero di record che verranno aggiornati: 3587\n",
      "üìã Creato 'df_swear_copy' con 3587 record (lo stato *prima* della modifica).\n",
      "Ricalcolo completato! Le colonne sono state aggiornate.\n"
     ]
    }
   ],
   "source": [
    "df_swear=df\n",
    "## 1. Preparazione: Creare i Pattern RegEx\n",
    "#    Questa √® la parte pi√π importante per la velocit√†.\n",
    "#    Trasformiamo ['a', 'b', 'c'] in r'\\b(a|b|c)\\b'\n",
    "\n",
    "def crea_pattern(lista_parole):\n",
    "    # 1. Assicura che siano tutte stringhe e minuscole\n",
    "    parole_pulite = [str(p).lower() for p in lista_parole if pd.notna(p)]\n",
    "    \n",
    "    # 2. Fai l'escape di ogni parola (per gestire caratteri come 'f**k')\n",
    "    parole_escaped = [re.escape(p) for p in parole_pulite]\n",
    "    \n",
    "    # 3. Uniscile con un OR (|)\n",
    "    pattern_string = \"|\".join(parole_escaped)\n",
    "    \n",
    "    # 4. Racchiudi in \\b (word boundary) per trovare solo parole intere\n",
    "    #    e compila il pattern RegEx.\n",
    "    return re.compile(r'\\b(' + pattern_string + r')\\b', re.IGNORECASE)\n",
    "\n",
    "print(\"Creazione pattern RegEx in corso...\")\n",
    "pattern_it = crea_pattern(lista_parole_it)\n",
    "pattern_en = crea_pattern(lista_parole_en)\n",
    "print(\"Pattern creati.\")\n",
    "\n",
    "\n",
    "## 2. Definire la Funzione di Ricalcolo\n",
    "#    Questa funzione verr√† applicata a OGNI RIGA del DataFrame\n",
    "\n",
    "def ricalcola_parole(riga):\n",
    "    testo = riga['lyrics']\n",
    "    \n",
    "    # Se il testo √® mancante, restituisci valori vuoti\n",
    "    if pd.isna(testo):\n",
    "        return 0, [], 0, [] # count_it, words_it, count_en, words_en\n",
    "\n",
    "    # --- Processo Italiano ---\n",
    "    # re.findall() trova TUTTE le occorrenze (anche duplicate)\n",
    "    matches_it = pattern_it.findall(testo.lower())\n",
    "    \n",
    "    # Il conteggio √® il numero totale di occorrenze\n",
    "    conteggio_it = len(matches_it)\n",
    "    # La lista di parole √® l'insieme unico (set) delle parole trovate\n",
    "    parole_uniche_it = list(set(matches_it))\n",
    "\n",
    "    # --- Processo Inglese ---\n",
    "    matches_en = pattern_en.findall(testo.lower())\n",
    "    conteggio_en = len(matches_en)\n",
    "    parole_uniche_en = list(set(matches_en))\n",
    "\n",
    "    return conteggio_it, parole_uniche_it, conteggio_en, parole_uniche_en\n",
    "\n",
    "\n",
    "## 3. Esecuzione e Aggiornamento\n",
    "#    Applichiamo la funzione all'intero DataFrame\n",
    "\n",
    "print(\"Inizio ricalcolo su tutto il DataFrame (pu√≤ richiedere tempo)...\")\n",
    "\n",
    "# 'axis=1' applica la funzione a ogni riga\n",
    "# 'result_type='expand'' divide il risultato della funzione (i 4 valori)\n",
    "# in 4 nuove colonne\n",
    "nuovi_valori = df_swear.apply(ricalcola_parole, axis=1, result_type='expand')\n",
    "\n",
    "nuovi_valori.columns = ['swear_IT_new', 'swear_IT_words_new', 'swear_EN_new', 'swear_EN_words_new']\n",
    "\n",
    "# Confrontiamo le vecchie colonne con le nuove\n",
    "# Usiamo .fillna(0) per i conteggi e .ne() (Not Equal) per le liste\n",
    "\n",
    "# 1. Confronto Conteggi\n",
    "cond_it_count = df_swear['swear_IT'].fillna(0).ne(nuovi_valori['swear_IT_new'])\n",
    "cond_en_count = df_swear['swear_EN'].fillna(0).ne(nuovi_valori['swear_EN_new'])\n",
    "\n",
    "# 2. Confronto Liste\n",
    "# .ne() gestisce correttamente il confronto tra liste e valori NaN/None\n",
    "cond_it_words = df_swear['swear_IT_words'].ne(nuovi_valori['swear_IT_words_new'])\n",
    "cond_en_words = df_swear['swear_EN_words'].ne(nuovi_valori['swear_EN_words_new'])\n",
    "\n",
    "# 3. Condizione Totale: la riga √® cambiata se ALMENO UNO dei 4 campi √® diverso\n",
    "condizione_cambiati = (cond_it_count | cond_en_count | cond_it_words | cond_en_words)\n",
    "\n",
    "# 4. Stampa e Copia (come richiesto)\n",
    "num_cambiati = condizione_cambiati.sum()\n",
    "print(f\"üìä Numero di record che verranno aggiornati: {num_cambiati}\")\n",
    "\n",
    "# Creiamo la copia delle righe *originali* che stanno per cambiare\n",
    "df_swear_copy = df_swear[condizione_cambiati].copy()\n",
    "print(f\"üìã Creato 'df_swear_copy' con {len(df_swear_copy)} record (lo stato *prima* della modifica).\")\n",
    "\n",
    "# Assegna i nuovi valori alle colonne corrette del DataFrame originale\n",
    "# (sovrascrivendo i vecchi dati)\n",
    "df_swear[['swear_IT', 'swear_IT_words', 'swear_EN', 'swear_EN_words']] = nuovi_valori\n",
    "\n",
    "print(\"Ricalcolo completato! Le colonne sono state aggiornate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6d82b",
   "metadata": {},
   "source": [
    "We discovered that the dataset was probably created by starting with a fixed swear words list but this list doesn't include plural and leads to not accurate records both in the swear_word collection, count and the explicit column.\n",
    "We decided to just fix the explicit column according to our swear words counts and not manually insert the plural or the other word forms in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bc699959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['conteggio_calcolato', 'conteggio_originale'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "34bde8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11166 entries, 0 to 11165\n",
      "Data columns (total 47 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id_artist             11166 non-null  object        \n",
      " 1   name_artist           11166 non-null  string        \n",
      " 2   title                 11166 non-null  string        \n",
      " 3   featured_artists      11166 non-null  object        \n",
      " 4   language              11032 non-null  object        \n",
      " 5   swear_IT              11166 non-null  int64         \n",
      " 6   swear_EN              11166 non-null  int64         \n",
      " 7   swear_IT_words        11166 non-null  object        \n",
      " 8   swear_EN_words        11166 non-null  object        \n",
      " 9   n_sentences           10959 non-null  Int64         \n",
      " 10  n_tokens              10959 non-null  Int64         \n",
      " 11  tokens_per_sent       10959 non-null  float64       \n",
      " 12  char_per_tok          11090 non-null  float64       \n",
      " 13  lexical_density       11090 non-null  float64       \n",
      " 14  avg_token_per_clause  10959 non-null  float64       \n",
      " 15  bpm                   11102 non-null  float64       \n",
      " 16  centroid              11102 non-null  float64       \n",
      " 17  rolloff               11102 non-null  float64       \n",
      " 18  flux                  11102 non-null  float64       \n",
      " 19  rms                   11102 non-null  float64       \n",
      " 20  zcr                   11102 non-null  float64       \n",
      " 21  flatness              11102 non-null  float64       \n",
      " 22  spectral_complexity   11102 non-null  float64       \n",
      " 23  pitch                 11102 non-null  float64       \n",
      " 24  loudness              11102 non-null  float64       \n",
      " 25  album_type            11088 non-null  category      \n",
      " 26  disc_number           11088 non-null  Int64         \n",
      " 27  track_number          11088 non-null  Int64         \n",
      " 28  duration_ms           11088 non-null  float64       \n",
      " 29  explicit              11035 non-null  object        \n",
      " 30  popularity            11088 non-null  Int64         \n",
      " 31  album_image           11088 non-null  string        \n",
      " 32  id_album              11087 non-null  object        \n",
      " 33  lyrics                11032 non-null  string        \n",
      " 34  gender                11166 non-null  category      \n",
      " 35  birth_date            8588 non-null   datetime64[ns]\n",
      " 36  birth_place           8588 non-null   category      \n",
      " 37  nationality           8557 non-null   category      \n",
      " 38  description           10028 non-null  string        \n",
      " 39  active_start          11166 non-null  datetime64[ns]\n",
      " 40  province              8467 non-null   category      \n",
      " 41  region                8024 non-null   category      \n",
      " 42  country               8467 non-null   category      \n",
      " 43  latitude              8588 non-null   float64       \n",
      " 44  longitude             8588 non-null   float64       \n",
      " 45  album                 10960 non-null  object        \n",
      " 46  correct_release_date  10943 non-null  datetime64[ns]\n",
      "dtypes: Int64(5), category(7), datetime64[ns](3), float64(17), int64(2), object(8), string(5)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a50710",
   "metadata": {},
   "source": [
    "## Music parameters control\n",
    "Due to colineary with other features (rms, rolloff) we decided to drop loudness and zcr columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "12c581a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['loudness','zcr'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "31ee4500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 27.799999999999983\n",
      "Limite superiore: 198.76000000000002\n",
      "Numero di valori NaN in 'bpm': 65\n"
     ]
    }
   ],
   "source": [
    "df_measures = df.copy()\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'bpm'\n",
    "Q1 = df_measures['bpm'].quantile(0.25)\n",
    "Q3 = df_measures['bpm'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'bpm' √® DENTRO i limiti\n",
    "\n",
    "condizione = ((df_measures['bpm'] < limite_inferiore) | (df_measures['bpm'] > limite_superiore))\n",
    "\n",
    "#condizione = ((df_measures['bpm'] < 0.04) | (df_measures['bpm'] > 0.24))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'bpm'] = np.nan\n",
    "nan_count = df_measures['bpm'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'bpm': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "55cdd685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.06535000000000002\n",
      "Limite superiore: 0.20894999999999997\n",
      "Numero di valori NaN in 'centroid': 79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'centroid'\n",
    "Q1 = df_measures['centroid'].quantile(0.25)\n",
    "Q3 = df_measures['centroid'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'centroid' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['centroid'] < limite_inferiore) | (df_measures['centroid'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['centroid'] < 0.04) | (df_measures['centroid'] > 0.24))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'centroid'] = np.nan\n",
    "nan_count = df_measures['centroid'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'centroid': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1a5d8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 152.78702499999986\n",
      "Limite superiore: 3005.2408250000003\n",
      "Numero di valori NaN in 'rolloff': 71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'rolloff'\n",
    "Q1 = df_measures['rolloff'].quantile(0.25)\n",
    "Q3 = df_measures['rolloff'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'rolloff' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['rolloff'] < limite_inferiore) | (df_measures['rolloff'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['rolloff'] < limite_inferiore) | (df_measures['rolloff'] > 6000))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'rolloff'] = np.nan\n",
    "nan_count = df_measures['rolloff'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'rolloff': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d3f2e9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.9097499999999996\n",
      "Limite superiore: 1.6091500000000003\n",
      "Numero di valori NaN in 'flux': 79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'flux'\n",
    "Q1 = df_measures['flux'].quantile(0.25)\n",
    "Q3 = df_measures['flux'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'flux' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['flux'] < limite_inferiore) | (df_measures['flux'] > limite_superiore))\n",
    "\n",
    "\n",
    "condizione = ((df_measures['flux'] < 0.75) | (df_measures['flux'] > 1.75))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'flux'] = np.nan\n",
    "nan_count = df_measures['flux'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'flux': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "974b1f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.0641625\n",
      "Limite superiore: 0.3896625\n",
      "Numero di valori NaN in 'rms': 72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'rms'\n",
    "Q1 = df_measures['rms'].quantile(0.25)\n",
    "Q3 = df_measures['rms'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'rms' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['rms'] < limite_inferiore) | (df_measures['rms'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['rms'] < 0.) | (df_measures['rms'] > 0.43))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'rms'] = np.nan\n",
    "nan_count = df_measures['rms'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'rms': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "315492c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.7338875\n",
      "Limite superiore: 1.0205875000000002\n",
      "Numero di valori NaN in 'flatness': 73\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'flatness'\n",
    "Q1 = df_measures['flatness'].quantile(0.25)\n",
    "Q3 = df_measures['flatness'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'flatness' √® DENTRO i limiti\n",
    "\n",
    "#condizione = ((df_measures['flatness'] < limite_inferiore) | (df_measures['flatness'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['flatness'] < 0.2) | (df_measures['flatness'] > limite_superiore))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'flatness'] = np.nan\n",
    "nan_count = df_measures['flatness'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'flatness': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "93a9146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 5.236662499999998\n",
      "Limite superiore: 49.623562500000006\n",
      "Numero di valori NaN in 'spectral_complexity': 66\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'spectral_complexity'\n",
    "Q1 = df_measures['spectral_complexity'].quantile(0.25)\n",
    "Q3 = df_measures['spectral_complexity'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'spectral_complexity' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['spectral_complexity'] < limite_inferiore) | (df_measures['spectral_complexity'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['spectral_complexity'] < 0) | (df_measures['spectral_complexity'] > 55))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'spectral_complexity'] = np.nan\n",
    "nan_count = df_measures['spectral_complexity'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'spectral_complexity': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "eb3a5a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 1268.1149375000005\n",
      "Limite superiore: 3228.718637499999\n",
      "Numero di valori NaN in 'pitch': 69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'pitch'\n",
    "Q1 = df_measures['pitch'].quantile(0.25)\n",
    "Q3 = df_measures['pitch'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'pitch' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['pitch'] < limite_inferiore) | (df_measures['pitch'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['pitch'] < 1000) | (df_measures['pitch'] > 4000))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'pitch'] = np.nan\n",
    "nan_count = df_measures['pitch'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'pitch': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4bd4f5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 17.0\n",
      "Limite superiore: 977.0\n",
      "Numero di valori NaN in 'n_tokens': 211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'pitch'\n",
    "Q1 = df_measures['n_tokens'].quantile(0.25)\n",
    "Q3 = df_measures['n_tokens'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'n_tokens' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['n_tokens'] < limite_inferiore) | (df_measures['n_tokens'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['n_tokens'] < 1) | (df_measures['n_tokens'] > 2000))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'n_tokens'] = np.nan\n",
    "nan_count = df_measures['n_tokens'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'n_tokens': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "95a31452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 4.256533364333873\n",
      "Limite superiore: 12.542141362473846\n",
      "Numero di valori NaN in 'tokens_per_sent': 208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Calcola Q1 e Q3 per la colonna 'pitch'\n",
    "Q1 = df_measures['tokens_per_sent'].quantile(0.25)\n",
    "Q3 = df_measures['tokens_per_sent'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'tokens_per_sent' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['tokens_per_sent'] < limite_inferiore) | (df_measures['tokens_per_sent'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['tokens_per_sent'] < 1) | (df_measures['tokens_per_sent'] > 100))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'tokens_per_sent'] = np.nan\n",
    "nan_count = df_measures['tokens_per_sent'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'tokens_per_sent': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "fcd44f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 0.39220676991058934\n",
      "Limite superiore: 0.6325965599529393\n",
      "Numero di valori NaN in 'lexical_density': 85\n"
     ]
    }
   ],
   "source": [
    "# 1. Calcola Q1 e Q3 per la colonna 'pitch'\n",
    "Q1 = df_measures['lexical_density'].quantile(0.25)\n",
    "Q3 = df_measures['lexical_density'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'lexical_density' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['lexical_density'] < limite_inferiore) | (df_measures['lexical_density'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['lexical_density'] < 0.05) | (df_measures['lexical_density'] > 0.95))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'lexical_density'] = np.nan\n",
    "nan_count = df_measures['lexical_density'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'lexical_density': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f032b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite inferiore: 2.560984109885416\n",
      "Limite superiore: 11.42476546627214\n",
      "Numero di valori NaN in 'avg_token_per_clause': 221\n"
     ]
    }
   ],
   "source": [
    "# 1. Calcola Q1 e Q3 per la colonna 'pitch'\n",
    "Q1 = df_measures['avg_token_per_clause'].quantile(0.25)\n",
    "Q3 = df_measures['avg_token_per_clause'].quantile(0.75)\n",
    "\n",
    "# 2. Calcola l'IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Definisci i limiti\n",
    "limite_superiore = Q3 + (1.5 * IQR)\n",
    "limite_inferiore = Q1 - (1.5 * IQR)\n",
    "\n",
    "print(f\"Limite inferiore: {limite_inferiore}\")\n",
    "print(f\"Limite superiore: {limite_superiore}\")\n",
    "\n",
    "# 4. Filtra il DataFrame\n",
    "# Manteniamo solo le righe dove 'avg_token_per_clause' √® DENTRO i limiti\n",
    "#condizione = ((df_measures['avg_token_per_clause'] < limite_inferiore) | (df_measures['avg_token_per_clause'] > limite_superiore))\n",
    "\n",
    "condizione = ((df_measures['avg_token_per_clause'] < 0) | (df_measures['avg_token_per_clause'] > 175))\n",
    "# 5. Controlla il risultato\n",
    "df_measures.loc[condizione, 'avg_token_per_clause'] = np.nan\n",
    "nan_count = df_measures['avg_token_per_clause'].isnull().sum()\n",
    "print(f\"Numero di valori NaN in 'avg_token_per_clause': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e51a8dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpm                      65\n",
      "centroid                 79\n",
      "rolloff                  71\n",
      "flux                     79\n",
      "rms                      72\n",
      "flatness                 73\n",
      "spectral_complexity      66\n",
      "pitch                    69\n",
      "n_tokens                211\n",
      "tokens_per_sent         208\n",
      "lexical_density          85\n",
      "avg_token_per_clause    221\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "colonne_da_sostituire = ['bpm', 'centroid', 'rolloff', 'flux', 'rms', 'flatness', 'spectral_complexity', 'pitch', 'n_tokens', 'tokens_per_sent', 'lexical_density', 'avg_token_per_clause']\n",
    "\n",
    "df[colonne_da_sostituire] = df_measures[colonne_da_sostituire]\n",
    "\n",
    "nan_count = df_measures[colonne_da_sostituire].isnull().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5f626d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verification Check After Cleaning ---\n",
      "Total number of albums with a track number/count mismatch (AFTER CLEANING): 0\n",
      "\n",
      "Cleaned Album Summary Head:\n",
      "           Max_Track_Number_Clean  Total_Track_Count_Clean\n",
      "id_album                                                  \n",
      "ALB100033                    14.0                       14\n",
      "ALB100157                     1.0                        1\n",
      "ALB100356                     1.0                        1\n",
      "ALB100713                    19.0                       19\n",
      "ALB100900                     2.0                        2\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the DataFrame to work on\n",
    "df_track_number = df.copy()\n",
    "\n",
    "# 1. Establish the correct order\n",
    "# Crucial Step: Sort by 'id_album' first, and then by the existing 'track_number'\n",
    "# This ensures that when we re-index (using cumcount), we maintain the intended order\n",
    "df_track_number.sort_values(by=['id_album', 'track_number'], inplace=True)\n",
    "\n",
    "# 2. Re-index Tracks within Each Album Group\n",
    "# groupby('id_album') isolates each album.\n",
    "# cumcount() counts the rows sequentially (0, 1, 2, ...) within each isolated group.\n",
    "# We add + 1 to make it 1-based (1, 2, 3, ...)\n",
    "df_track_number['Clean_Track_Number'] = df_track_number.groupby('id_album').cumcount() + 1\n",
    "\n",
    "# 3. Optional: Compare Old and New Track Numbers (for verification)\n",
    "# We can drop the old 'track_number' if we only need the clean one,\n",
    "# but keeping it temporarily shows the impact of the cleaning.\n",
    "# print(df_track_number[['id_album', 'track_number', 'Clean_Track_Number']].head(20))\n",
    "\n",
    "# 4. Replace the old track_number column with the new, clean column\n",
    "df_track_number.drop(columns=['track_number'], inplace=True)\n",
    "df_track_number.rename(columns={'Clean_Track_Number': 'track_number'}, inplace=True)\n",
    "\n",
    "\n",
    "# Verification Step: Re-run the count and max check to show the data is now consistent\n",
    "print(\"--- Verification Check After Cleaning ---\")\n",
    "\n",
    "# Max Track Number should now equal Total Track Count for ALL albums\n",
    "max_tracks_clean = df_track_number.groupby('id_album')['track_number'].max().rename('Max_Track_Number_Clean')\n",
    "total_tracks_clean = df_track_number.groupby('id_album')['track_number'].count().rename('Total_Track_Count_Clean')\n",
    "\n",
    "album_summary_clean = pd.concat([max_tracks_clean, total_tracks_clean], axis=1)\n",
    "\n",
    "# Check for mismatches again (should be 0 now)\n",
    "mismatch_count_clean = (album_summary_clean['Max_Track_Number_Clean'] != album_summary_clean['Total_Track_Count_Clean']).sum()\n",
    "\n",
    "print(f\"Total number of albums with a track number/count mismatch (AFTER CLEANING): {mismatch_count_clean}\")\n",
    "print(\"\\nCleaned Album Summary Head:\")\n",
    "print(album_summary_clean.head())\n",
    "\n",
    "df['track_number'] = df_track_number['track_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5457b7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_artist</th>\n",
       "      <th>name_artist</th>\n",
       "      <th>title</th>\n",
       "      <th>featured_artists</th>\n",
       "      <th>language</th>\n",
       "      <th>swear_IT</th>\n",
       "      <th>swear_EN</th>\n",
       "      <th>swear_IT_words</th>\n",
       "      <th>swear_EN_words</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>tokens_per_sent</th>\n",
       "      <th>char_per_tok</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>avg_token_per_clause</th>\n",
       "      <th>bpm</th>\n",
       "      <th>centroid</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>flux</th>\n",
       "      <th>rms</th>\n",
       "      <th>flatness</th>\n",
       "      <th>spectral_complexity</th>\n",
       "      <th>pitch</th>\n",
       "      <th>album_type</th>\n",
       "      <th>disc_number</th>\n",
       "      <th>track_number</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>popularity</th>\n",
       "      <th>album_image</th>\n",
       "      <th>id_album</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>birth_place</th>\n",
       "      <th>nationality</th>\n",
       "      <th>description</th>\n",
       "      <th>active_start</th>\n",
       "      <th>province</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>album</th>\n",
       "      <th>correct_release_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ART04205421</td>\n",
       "      <td>rosa chemical</td>\n",
       "      <td>‚Äãpolka 2 :-/</td>\n",
       "      <td>[ernia, gue]</td>\n",
       "      <td>__it</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>[stronzo, coglioni, piscio, palle, figa, porca...</td>\n",
       "      <td>[pussy, fuck, bitch, porno]</td>\n",
       "      <td>102</td>\n",
       "      <td>911</td>\n",
       "      <td>8.931373</td>\n",
       "      <td>4.170455</td>\n",
       "      <td>0.575284</td>\n",
       "      <td>8.133929</td>\n",
       "      <td>135.32</td>\n",
       "      <td>0.1858</td>\n",
       "      <td>2895.7767</td>\n",
       "      <td>1.4499</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.8202</td>\n",
       "      <td>25.7148</td>\n",
       "      <td>2311.1779</td>\n",
       "      <td>album</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>207761.0</td>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b2736d5e14...</td>\n",
       "      <td>ALB115557</td>\n",
       "      <td>Opl√†, ah\\nBdope, chiama due b‚Äî\\n\\nMi candidere...</td>\n",
       "      <td>M</td>\n",
       "      <td>1998-01-30</td>\n",
       "      <td>Grugliasco</td>\n",
       "      <td>Italia</td>\n",
       "      <td>rapper e cantautore italiano (1998-)</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Torino</td>\n",
       "      <td>Piemonte</td>\n",
       "      <td>Italia</td>\n",
       "      <td>45.068046</td>\n",
       "      <td>7.57762</td>\n",
       "      <td>forever and ever</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ART04205421</td>\n",
       "      <td>rosa chemical</td>\n",
       "      <td>POLKA</td>\n",
       "      <td>[thelonious b]</td>\n",
       "      <td>__it</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>[cazzo, puttana, culo, sega, troia, frocio]</td>\n",
       "      <td>[sex, shit, negro, escort, sexy]</td>\n",
       "      <td>56</td>\n",
       "      <td>675</td>\n",
       "      <td>12.053571</td>\n",
       "      <td>4.280851</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>129.37</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>3378.4605</td>\n",
       "      <td>1.3533</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.6739</td>\n",
       "      <td>29.1089</td>\n",
       "      <td>1892.1924</td>\n",
       "      <td>album</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>207761.0</td>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b2736d5e14...</td>\n",
       "      <td>ALB115557</td>\n",
       "      <td>Greg Willen, non dormire\\n(Brr-poh)\\n\\nT-T-Tro...</td>\n",
       "      <td>M</td>\n",
       "      <td>1998-01-30</td>\n",
       "      <td>Grugliasco</td>\n",
       "      <td>Italia</td>\n",
       "      <td>rapper e cantautore italiano (1998-)</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Torino</td>\n",
       "      <td>Piemonte</td>\n",
       "      <td>Italia</td>\n",
       "      <td>45.068046</td>\n",
       "      <td>7.57762</td>\n",
       "      <td>forever and ever</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ART04205421</td>\n",
       "      <td>rosa chemical</td>\n",
       "      <td>‚Äãbritney ;-)</td>\n",
       "      <td>[mambolosco, radical]</td>\n",
       "      <td>__it</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>[bastardo, cazzo, culo, troia, merda]</td>\n",
       "      <td>[bastardo, cock, fuck, bitch, bitches]</td>\n",
       "      <td>88</td>\n",
       "      <td>758</td>\n",
       "      <td>8.613636</td>\n",
       "      <td>4.075251</td>\n",
       "      <td>0.556856</td>\n",
       "      <td>8.422222</td>\n",
       "      <td>133.68</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>2037.4847</td>\n",
       "      <td>1.3822</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>26.9762</td>\n",
       "      <td>2484.3503</td>\n",
       "      <td>album</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193544.0</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b2736d5e14...</td>\n",
       "      <td>ALB115557</td>\n",
       "      <td>Mothz\\nYeah, yeah, yeah-yeah\\nBdope, chiama du...</td>\n",
       "      <td>M</td>\n",
       "      <td>1998-01-30</td>\n",
       "      <td>Grugliasco</td>\n",
       "      <td>Italia</td>\n",
       "      <td>rapper e cantautore italiano (1998-)</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Torino</td>\n",
       "      <td>Piemonte</td>\n",
       "      <td>Italia</td>\n",
       "      <td>45.068046</td>\n",
       "      <td>7.57762</td>\n",
       "      <td>forever and ever</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ART04205421</td>\n",
       "      <td>rosa chemical</td>\n",
       "      <td>CEO</td>\n",
       "      <td>[taxi b]</td>\n",
       "      <td>__it</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[pompino, cazzo, culo, sega, troia, fottere, m...</td>\n",
       "      <td>[shit, porno, fuck]</td>\n",
       "      <td>37</td>\n",
       "      <td>382</td>\n",
       "      <td>10.324324</td>\n",
       "      <td>4.023881</td>\n",
       "      <td>0.534328</td>\n",
       "      <td>6.701754</td>\n",
       "      <td>162.22</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>1156.3781</td>\n",
       "      <td>1.5499</td>\n",
       "      <td>0.1971</td>\n",
       "      <td>0.8764</td>\n",
       "      <td>14.2956</td>\n",
       "      <td>2984.6109</td>\n",
       "      <td>single</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>169000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b27367c03d...</td>\n",
       "      <td>ALB730959</td>\n",
       "      <td>Designer sui vestiti penso di essere un outlet...</td>\n",
       "      <td>M</td>\n",
       "      <td>1998-01-30</td>\n",
       "      <td>Grugliasco</td>\n",
       "      <td>Italia</td>\n",
       "      <td>rapper e cantautore italiano (1998-)</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Torino</td>\n",
       "      <td>Piemonte</td>\n",
       "      <td>Italia</td>\n",
       "      <td>45.068046</td>\n",
       "      <td>7.57762</td>\n",
       "      <td>okay okay 2</td>\n",
       "      <td>2025-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ART04205421</td>\n",
       "      <td>rosa chemical</td>\n",
       "      <td>LONDRA</td>\n",
       "      <td>[rkomi]</td>\n",
       "      <td>__it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[cazzo]</td>\n",
       "      <td>[]</td>\n",
       "      <td>48</td>\n",
       "      <td>429</td>\n",
       "      <td>8.937500</td>\n",
       "      <td>3.922857</td>\n",
       "      <td>0.491429</td>\n",
       "      <td>8.411765</td>\n",
       "      <td>105.87</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>1693.4542</td>\n",
       "      <td>1.0886</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>36.6951</td>\n",
       "      <td>1572.0499</td>\n",
       "      <td>album</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>194779.0</td>\n",
       "      <td>True</td>\n",
       "      <td>41</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b273fcdb60...</td>\n",
       "      <td>ALB436151</td>\n",
       "      <td>Bdope (Yeah)\\n\\nVuole solo me, non fare la gel...</td>\n",
       "      <td>M</td>\n",
       "      <td>1998-01-30</td>\n",
       "      <td>Grugliasco</td>\n",
       "      <td>Italia</td>\n",
       "      <td>rapper e cantautore italiano (1998-)</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Torino</td>\n",
       "      <td>Piemonte</td>\n",
       "      <td>Italia</td>\n",
       "      <td>45.068046</td>\n",
       "      <td>7.57762</td>\n",
       "      <td>forever</td>\n",
       "      <td>2020-05-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_artist    name_artist         title       featured_artists language  \\\n",
       "0  ART04205421  rosa chemical  ‚Äãpolka 2 :-/           [ernia, gue]     __it   \n",
       "1  ART04205421  rosa chemical         POLKA         [thelonious b]     __it   \n",
       "2  ART04205421  rosa chemical  ‚Äãbritney ;-)  [mambolosco, radical]     __it   \n",
       "3  ART04205421  rosa chemical           CEO               [taxi b]     __it   \n",
       "4  ART04205421  rosa chemical        LONDRA                [rkomi]     __it   \n",
       "\n",
       "   swear_IT  swear_EN                                     swear_IT_words  \\\n",
       "0        13         6  [stronzo, coglioni, piscio, palle, figa, porca...   \n",
       "1         9        12        [cazzo, puttana, culo, sega, troia, frocio]   \n",
       "2        16        12              [bastardo, cazzo, culo, troia, merda]   \n",
       "3         8         3  [pompino, cazzo, culo, sega, troia, fottere, m...   \n",
       "4         1         0                                            [cazzo]   \n",
       "\n",
       "                           swear_EN_words  n_sentences  n_tokens  \\\n",
       "0             [pussy, fuck, bitch, porno]          102       911   \n",
       "1        [sex, shit, negro, escort, sexy]           56       675   \n",
       "2  [bastardo, cock, fuck, bitch, bitches]           88       758   \n",
       "3                     [shit, porno, fuck]           37       382   \n",
       "4                                      []           48       429   \n",
       "\n",
       "   tokens_per_sent  char_per_tok  lexical_density  avg_token_per_clause  \\\n",
       "0         8.931373      4.170455         0.575284              8.133929   \n",
       "1        12.053571      4.280851         0.648936             12.500000   \n",
       "2         8.613636      4.075251         0.556856              8.422222   \n",
       "3        10.324324      4.023881         0.534328              6.701754   \n",
       "4         8.937500      3.922857         0.491429              8.411765   \n",
       "\n",
       "      bpm  centroid    rolloff    flux     rms  flatness  spectral_complexity  \\\n",
       "0  135.32    0.1858  2895.7767  1.4499  0.1786    0.8202              25.7148   \n",
       "1  129.37    0.2071  3378.4605  1.3533  0.2020    0.6739              29.1089   \n",
       "2  133.68    0.1833  2037.4847  1.3822  0.2552    0.7842              26.9762   \n",
       "3  162.22    0.1048  1156.3781  1.5499  0.1971    0.8764              14.2956   \n",
       "4  105.87    0.1421  1693.4542  1.0886  0.2369    0.8571              36.6951   \n",
       "\n",
       "       pitch album_type  disc_number  track_number  duration_ms explicit  \\\n",
       "0  2311.1779      album            1           3.0     207761.0     True   \n",
       "1  1892.1924      album            1           4.0     207761.0     True   \n",
       "2  2484.3503      album            1           1.0     193544.0     True   \n",
       "3  2984.6109     single            1           2.0     169000.0     True   \n",
       "4  1572.0499      album            1           8.0     194779.0     True   \n",
       "\n",
       "   popularity                                        album_image   id_album  \\\n",
       "0          46  https://i.scdn.co/image/ab67616d0000b2736d5e14...  ALB115557   \n",
       "1          46  https://i.scdn.co/image/ab67616d0000b2736d5e14...  ALB115557   \n",
       "2          39  https://i.scdn.co/image/ab67616d0000b2736d5e14...  ALB115557   \n",
       "3          47  https://i.scdn.co/image/ab67616d0000b27367c03d...  ALB730959   \n",
       "4          41  https://i.scdn.co/image/ab67616d0000b273fcdb60...  ALB436151   \n",
       "\n",
       "                                              lyrics gender birth_date  \\\n",
       "0  Opl√†, ah\\nBdope, chiama due b‚Äî\\n\\nMi candidere...      M 1998-01-30   \n",
       "1  Greg Willen, non dormire\\n(Brr-poh)\\n\\nT-T-Tro...      M 1998-01-30   \n",
       "2  Mothz\\nYeah, yeah, yeah-yeah\\nBdope, chiama du...      M 1998-01-30   \n",
       "3  Designer sui vestiti penso di essere un outlet...      M 1998-01-30   \n",
       "4  Bdope (Yeah)\\n\\nVuole solo me, non fare la gel...      M 1998-01-30   \n",
       "\n",
       "  birth_place nationality                           description active_start  \\\n",
       "0  Grugliasco      Italia  rapper e cantautore italiano (1998-)   2015-01-01   \n",
       "1  Grugliasco      Italia  rapper e cantautore italiano (1998-)   2015-01-01   \n",
       "2  Grugliasco      Italia  rapper e cantautore italiano (1998-)   2015-01-01   \n",
       "3  Grugliasco      Italia  rapper e cantautore italiano (1998-)   2015-01-01   \n",
       "4  Grugliasco      Italia  rapper e cantautore italiano (1998-)   2015-01-01   \n",
       "\n",
       "  province    region country   latitude  longitude             album  \\\n",
       "0   Torino  Piemonte  Italia  45.068046    7.57762  forever and ever   \n",
       "1   Torino  Piemonte  Italia  45.068046    7.57762  forever and ever   \n",
       "2   Torino  Piemonte  Italia  45.068046    7.57762  forever and ever   \n",
       "3   Torino  Piemonte  Italia  45.068046    7.57762       okay okay 2   \n",
       "4   Torino  Piemonte  Italia  45.068046    7.57762           forever   \n",
       "\n",
       "  correct_release_date  \n",
       "0           2021-04-09  \n",
       "1           2021-04-09  \n",
       "2           2021-04-09  \n",
       "3           2025-05-16  \n",
       "4           2020-05-28  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "82c98c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/df_cleaned.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
